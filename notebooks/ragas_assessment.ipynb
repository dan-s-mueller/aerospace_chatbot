{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta, archive_db\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "\n",
    "    if Path(file_name).exists():\n",
    "\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # return df.insert(0, column, None)\n",
    "\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        # df_out = df.copy().assign(column=\"\")\n",
    "        # df_out = df_out.reindex(columns=( [col for col in df_out.columns if col not in [column]] + [column] ))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1024,\n",
       "              'host': 'voyage-large-2-instruct-ams-2merge-none-dj30h8y.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'voyage-large-2-instruct-ams-2merge-none',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-ams-2merge-none',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection to use\n",
    "db=dbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone_client.Index(db['index_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 0 to 200\n",
      "Fetching 200 to 400\n",
      "Fetching 400 to 600\n",
      "Fetching 600 to 800\n",
      "Fetching 800 to 1000\n",
      "Fetching 1000 to 1200\n",
      "Fetching 1200 to 1400\n",
      "Fetching 1400 to 1600\n",
      "Fetching 1600 to 1800\n",
      "Fetching 1800 to 2000\n",
      "Fetching 2000 to 2200\n",
      "Fetching 2200 to 2400\n",
      "Fetching 2400 to 2600\n",
      "Fetching 2600 to 2800\n"
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "chunk_size=200  # Tune to whatever doesn't error out\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shema for accesing the data\n",
    "# data=vector_data[0]['metadata']\n",
    "# page_content=data['page_content']\n",
    "# metadata={'page':data['page'],'source':data['source']}\n",
    "# embedding=vector_data[0]['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lcdocs_pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='228 deployable hinge mechanisms. The SWOT mission has two identical reflector masts, each with two  deployable hinges. NISAR has a single mast with four deployable hinges. These masts can be seen in  Figure 1. Operationally, the deployable masts are launched in a stowed state wi th a launch restraint system  composed of separation nut devices. When commanded, the launch restraints release a pre- tensioned  spring and damper mechanism which deploys each hinge. Hinge deployment progress is monitored on the  ground using a potentiometer as well as a limit switch  on each hinge. Upon completion of the deployment,  an actuator -driven latching mechanism preloads precision alignment features on either side of the hinge  together. Figure 2 displays a n overview of  the mechanisms.  \\n\\nFigure 2. Hinge Deploy & Latching Mechanisms (NISAR configuration s hown)  \\n\\nMechanism Design and  Fabrication  \\n\\nMechanism Design   Each deployable hinge for  the SWOT and NISAR masts is outfitted with a spring, damper and potentiometer  mounted co-axially with each hinge line. The NISAR mast is  composed of 7-inch (18-cm) square composite  tubing. The SWOT mast is composed of 10-inch (25-cm) squar e composite tubing. Figure 3 displays the  spring mechanisms for each mission. The smaller, 7-inch (18-cm) mast cross- section of NISAR became  the driving factor in the design of the spring mechanism to maximize mechanical  commonality between  projects . Common mounting interfaces were design ed for both p rojects . Ultimately, this led to a cylindrical  volume allowance of 7 inches  (18 cm)  in length and 1.75 inches  (44 mm)  in diameter for the NISAR spring  mechanism.  Because of the differences between the SWOT and NISAR stowed hinge angles , as well as  differences in hinge angles at different locations on each mast , four different torsion spring configurations  were developed, each with the spring arms located at different angles relative to each other in the relaxed  position.  This can also be seen in Figure 3 when comparing both images. 229 A)     B)  \\n\\nFigure 3. View of A) NISAR and B) SWOT spring mechanisms  \\n\\nThe spring  mechanism is  required to meet JPL design requirements  for mission critical  spring design. As  such, springs are required to have a minimum no test yield factor of safety ( FS) of 1.50 and an ultimate FS  of 1.65.  Further more, JPL design principles impose a minimum mechanism torque margin of 100% in worst  case environments at end of life. These dri ving requirements meant the torsion  springs needed to  produce  a minimum deployment -direction torque of 28 inch- pounds  (3.2 N -m) at hinge closure. A standard round  wire 17-7 precipitation -hardened stainless -steel  torsion spring would not produce adequate torque in the  volume available without  violating mission critical factors of safety.  Alternative materials such as Elgiloy and  MP35N were considered, but all of the vendors considered for fabrication of these springs had a significantly  higher volume of experience working with 17- 7 stainless  steel and developmental risk was deemed higher  with these alternative materials.  Therefore, a geometric solution was developed:  a rectangular cross section  spring to maximize the moment of inertia within the available volume . \\n\\nAfter developmental fabrication test runs, the spring wire height -to-width ratio selected for the spring cross  section was 3.88 :1. This  value was determined to be the highest ratio achievable with available CNC spring  winding manufacturi ng capabilities . Spring manufacturing still included many challenges given the  propensity of the spring wire to rotate about the axis of the wire during winding and inconsistencies in spring   back resulting in non- uniform torsion spring inner and outer diameters. The CNC spring winding  configuration is shown in Figure 4 . Guide support features were added to the flight spring mandrel design  to prevent twist about the axis of the spring wire at either end of each spring.  \\n\\nFigure 4. Torsion Spring on CNC Coiling Machine \\n\\nThe rectangular cross section caused early manufacturing issues  for the flight units . The springs did not  initially meet axial length requirements. Further the wire was prone to unexpected twisting during winding.  The initial inclination of the team was to attempt to relax  the overall spring length requirement, but that  would have had significant ripple impacts into the mature design of the hinge and mast structures.  To  address length requirement non- compliance, t he initially  baselined spring with 29 coils was modified to a  baseline design of 27 coils. With this change, the spring design violated  JPL design requirements  minimum  factor of safety requirements . Reducing the number of windings increased the stress in the spring.  In  consultation with JPL materials expert s, material coupon testing for the flight lot of material was conducted', metadata={'page': '[238, 239]', 'source': \"['AMS_2020.pdf', 'AMS_2020.pdf']\"})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcdocs_pinecone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 0 to 200\n",
      "Fetching 200 to 400\n",
      "Fetching 400 to 600\n",
      "Fetching 600 to 800\n",
      "Fetching 800 to 1000\n",
      "Fetching 1000 to 1200\n",
      "Fetching 1200 to 1400\n",
      "Fetching 1400 to 1600\n",
      "Fetching 1600 to 1800\n",
      "Fetching 1800 to 2000\n",
      "Fetching 2000 to 2200\n",
      "Fetching 2200 to 2400\n",
      "Fetching 2400 to 2600\n",
      "Fetching 2600 to 2800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>metadata</th>\n",
       "      <th>document</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000f9019f67f0266ac21eb6c543745fd6b44d71</td>\n",
       "      <td>['AMS_2016.pdf', 'AMS_2016.pdf']</td>\n",
       "      <td>[293, 294]</td>\n",
       "      <td>{'page': '[293, 294]', 'source': '['AMS_2016.p...</td>\n",
       "      <td>279 James W ebb Space Telescope Deployment Bru...</td>\n",
       "      <td>[0.00720933825, 0.0139422296, 0.00683793798, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001641be8dc3700ce8a5dfb7b89ba34cf1532a7a</td>\n",
       "      <td>['AMS_2018.pdf', 'AMS_2018.pdf']</td>\n",
       "      <td>[68, 69]</td>\n",
       "      <td>{'page': '[68, 69]', 'source': '['AMS_2018.pdf...</td>\n",
       "      <td>by driving the percussor cam through the secon...</td>\n",
       "      <td>[0.0130121643, 0.0220245682, -0.00461992202, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002f2a22399b4cddc2c4fc6e64701373f164c9ca</td>\n",
       "      <td>['AMS_2020.pdf', 'AMS_2020.pdf']</td>\n",
       "      <td>[238, 239]</td>\n",
       "      <td>{'page': '[238, 239]', 'source': '['AMS_2020.p...</td>\n",
       "      <td>228 deployable hinge mechanisms. The SWOT miss...</td>\n",
       "      <td>[0.0119848633, 0.0316759646, -0.0328605771, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004393406340fc757284d5fac68a9e11b5e83035</td>\n",
       "      <td>['AMS_2006.pdf', 'AMS_2006.pdf']</td>\n",
       "      <td>[149, 150]</td>\n",
       "      <td>{'page': '[149, 150]', 'source': '['AMS_2006.p...</td>\n",
       "      <td>As part of the trade-off process an assessment...</td>\n",
       "      <td>[0.013438114, 0.0535361692, -0.000656775548, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007433204368b888003b42689fbd32dd40f2d5fc</td>\n",
       "      <td>['AMS_2018.pdf', 'AMS_2018.pdf']</td>\n",
       "      <td>[285, 287]</td>\n",
       "      <td>{'page': '[285, 287]', 'source': '['AMS_2018.p...</td>\n",
       "      <td>Application to Future Platforms  With the succ...</td>\n",
       "      <td>[0.0171286669, 0.0204658043, -0.0117366463, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>ff84a007bfa20d87454dc56a385b7e75cdc18285</td>\n",
       "      <td>['AMS_2008.pdf', 'AMS_2008.pdf']</td>\n",
       "      <td>[30, 31]</td>\n",
       "      <td>{'page': '[30, 31]', 'source': '['AMS_2008.pdf...</td>\n",
       "      <td>Figure 1.  High Resolution Rotary Actuator The...</td>\n",
       "      <td>[0.00829231, 0.0277320761, -0.0370412283, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>ff8b812b601e9adadebd534c041a594057fd5163</td>\n",
       "      <td>['AMS_2004.pdf', 'AMS_2004.pdf']</td>\n",
       "      <td>[319, 320]</td>\n",
       "      <td>{'page': '[319, 320]', 'source': '['AMS_2004.p...</td>\n",
       "      <td>mech anisms were al so performe d afte r ran d...</td>\n",
       "      <td>[-0.00865876116, 0.0209979694, -0.0138918078, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>ff8e46cc86c6a9a374df1023df243bd2cb65ff56</td>\n",
       "      <td>['AMS_2002.pdf', 'AMS_2002.pdf']</td>\n",
       "      <td>[61, 62]</td>\n",
       "      <td>{'page': '[61, 62]', 'source': '['AMS_2002.pdf...</td>\n",
       "      <td>moment---notshowninthefigure---was13Nm,whichco...</td>\n",
       "      <td>[0.0218223, 0.0741071329, -0.0443298817, 0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>ff95dfb82721e17b22018b6f3a362ec9ae36807a</td>\n",
       "      <td>['AMS_2004.pdf', 'AMS_2004.pdf']</td>\n",
       "      <td>[280, 281]</td>\n",
       "      <td>{'page': '[280, 281]', 'source': '['AMS_2004.p...</td>\n",
       "      <td>The second function of the launch restraint is...</td>\n",
       "      <td>[0.0170335546, 0.0291346312, -0.0195622407, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>ffc455e744bfcacd29fb077f23486b4a98d37102</td>\n",
       "      <td>['AMS_2020.pdf', 'AMS_2020.pdf']</td>\n",
       "      <td>[363, 364]</td>\n",
       "      <td>{'page': '[363, 364]', 'source': '['AMS_2020.p...</td>\n",
       "      <td>353 actuator system have demonstrated to be a ...</td>\n",
       "      <td>[-0.00409142, 0.0216710027, -0.0364130288, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2696 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  \\\n",
       "0     0000f9019f67f0266ac21eb6c543745fd6b44d71   \n",
       "1     001641be8dc3700ce8a5dfb7b89ba34cf1532a7a   \n",
       "2     002f2a22399b4cddc2c4fc6e64701373f164c9ca   \n",
       "3     004393406340fc757284d5fac68a9e11b5e83035   \n",
       "4     007433204368b888003b42689fbd32dd40f2d5fc   \n",
       "...                                        ...   \n",
       "2691  ff84a007bfa20d87454dc56a385b7e75cdc18285   \n",
       "2692  ff8b812b601e9adadebd534c041a594057fd5163   \n",
       "2693  ff8e46cc86c6a9a374df1023df243bd2cb65ff56   \n",
       "2694  ff95dfb82721e17b22018b6f3a362ec9ae36807a   \n",
       "2695  ffc455e744bfcacd29fb077f23486b4a98d37102   \n",
       "\n",
       "                                source        page  \\\n",
       "0     ['AMS_2016.pdf', 'AMS_2016.pdf']  [293, 294]   \n",
       "1     ['AMS_2018.pdf', 'AMS_2018.pdf']    [68, 69]   \n",
       "2     ['AMS_2020.pdf', 'AMS_2020.pdf']  [238, 239]   \n",
       "3     ['AMS_2006.pdf', 'AMS_2006.pdf']  [149, 150]   \n",
       "4     ['AMS_2018.pdf', 'AMS_2018.pdf']  [285, 287]   \n",
       "...                                ...         ...   \n",
       "2691  ['AMS_2008.pdf', 'AMS_2008.pdf']    [30, 31]   \n",
       "2692  ['AMS_2004.pdf', 'AMS_2004.pdf']  [319, 320]   \n",
       "2693  ['AMS_2002.pdf', 'AMS_2002.pdf']    [61, 62]   \n",
       "2694  ['AMS_2004.pdf', 'AMS_2004.pdf']  [280, 281]   \n",
       "2695  ['AMS_2020.pdf', 'AMS_2020.pdf']  [363, 364]   \n",
       "\n",
       "                                               metadata  \\\n",
       "0     {'page': '[293, 294]', 'source': '['AMS_2016.p...   \n",
       "1     {'page': '[68, 69]', 'source': '['AMS_2018.pdf...   \n",
       "2     {'page': '[238, 239]', 'source': '['AMS_2020.p...   \n",
       "3     {'page': '[149, 150]', 'source': '['AMS_2006.p...   \n",
       "4     {'page': '[285, 287]', 'source': '['AMS_2018.p...   \n",
       "...                                                 ...   \n",
       "2691  {'page': '[30, 31]', 'source': '['AMS_2008.pdf...   \n",
       "2692  {'page': '[319, 320]', 'source': '['AMS_2004.p...   \n",
       "2693  {'page': '[61, 62]', 'source': '['AMS_2002.pdf...   \n",
       "2694  {'page': '[280, 281]', 'source': '['AMS_2004.p...   \n",
       "2695  {'page': '[363, 364]', 'source': '['AMS_2020.p...   \n",
       "\n",
       "                                               document  \\\n",
       "0     279 James W ebb Space Telescope Deployment Bru...   \n",
       "1     by driving the percussor cam through the secon...   \n",
       "2     228 deployable hinge mechanisms. The SWOT miss...   \n",
       "3     As part of the trade-off process an assessment...   \n",
       "4     Application to Future Platforms  With the succ...   \n",
       "...                                                 ...   \n",
       "2691  Figure 1.  High Resolution Rotary Actuator The...   \n",
       "2692  mech anisms were al so performe d afte r ran d...   \n",
       "2693  moment---notshowninthefigure---was13Nm,whichco...   \n",
       "2694  The second function of the launch restraint is...   \n",
       "2695  353 actuator system have demonstrated to be a ...   \n",
       "\n",
       "                                              embedding  \n",
       "0     [0.00720933825, 0.0139422296, 0.00683793798, 0...  \n",
       "1     [0.0130121643, 0.0220245682, -0.00461992202, 0...  \n",
       "2     [0.0119848633, 0.0316759646, -0.0328605771, 0....  \n",
       "3     [0.013438114, 0.0535361692, -0.000656775548, 0...  \n",
       "4     [0.0171286669, 0.0204658043, -0.0117366463, 0....  \n",
       "...                                                 ...  \n",
       "2691  [0.00829231, 0.0277320761, -0.0370412283, 0.02...  \n",
       "2692  [-0.00865876116, 0.0209979694, -0.0138918078, ...  \n",
       "2693  [0.0218223, 0.0741071329, -0.0443298817, 0.023...  \n",
       "2694  [0.0170335546, 0.0291346312, -0.0195622407, 0....  \n",
       "2695  [-0.00409142, 0.0216710027, -0.0364130288, 0.0...  \n",
       "\n",
       "[2696 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export all collections to pickles to store them\n",
    "# for db in dbs:\n",
    "df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-ams-nomerge-400),\n",
       " Collection(name=mixedbread-ai-mxbai-embed-large-v1-ams-nomerge-400),\n",
       " Collection(name=text-embedding-3-small-ams-2merge-none),\n",
       " Collection(name=text-embedding-3-large-ams-nomerge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-ams-2merge-none)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'text-embedding-3-large-ams-2merge-none',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-ams-nomerge-400',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-small-ams-2merge-none',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-ams-nomerge-400-parent-child',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'mixedbread-ai-mxbai-embed-large-v1-ams-nomerge-400',\n",
    "     'query_model':HuggingFaceInferenceAPIEmbeddings(model_name='mixedbread-ai-mxbai-embed-large-v1',api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection to use\n",
    "db=dbs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=db['index_name'],\n",
    "                        embedding_function=db['query_model'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41206"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lcdocs_chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Figure 3 - Comparison of Gas Strut Separation and BDM Separation', metadata={'doc_id': '6faab80591d1ea3f0e2cf6b14e2af8734331c09e', 'page': 347, 'source': 'AMS_2008.pdf', 'start_index': 0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcdocs_chroma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "# for db in dbs:\n",
    "df_temp_chroma=archive_db('ChromaDB',db['index_name'],db['query_model'],export_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "embedding_model=\"text-embedding-ada-002\"\n",
    "synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "# sample_size=min(len(lcdocs),20) # 500 is the max size before you'll hit rate limits with a tier 3 openai account\n",
    "sample_size=100\n",
    "percent_total=sample_size/len(lcdocs)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if testset.csv exists\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=50\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=False,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=False)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv(f'testset_{index_name}.csv', index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv(f'testset_{index_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, f'rag_response_cache_{index_name}.txt', \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f'rag_response_cache_{index_name}.txt', \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below demonstrates using aerospace_chatbot in batch mode. It requires some basic parameter setup and a QA_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO turn this into a function/standalone notebook to show how to use the QA model in batch mode\n",
    "\n",
    "index_type='ChromaDB'\n",
    "index_name=index_name\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n",
    "\n",
    "for i, row in df_questions_answers.iterrows():\n",
    "    if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "        print(f\"Processing question {i+1}/{len(df_questions_answers)}\")\n",
    "\n",
    "        # Use the QA model to query the documents\n",
    "        qa_obj=queries.QA_Model(index_type,\n",
    "                        index_name,\n",
    "                        query_model,\n",
    "                        embedding_model,\n",
    "                        llm,\n",
    "                        **QA_model_params)\n",
    "        qa_obj.query_docs(row['question'])\n",
    "        response=qa_obj.result\n",
    "\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response['answer'].content\n",
    "\n",
    "        ids=[_stable_hash_meta(source_document.metadata)\n",
    "            for source_document in response['references']]\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "        # Save the response to cache file\n",
    "        response_dict = {\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": response['answer'].content,\n",
    "            \"source_documents\": ids,\n",
    "        }\n",
    "        write_dict_to_file(response_dict, f'rag_response_cache_{index_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the context documents content for each question\n",
    "source_documents_list = []\n",
    "for cell in df_questions_answers['source_documents']:\n",
    "    cell_list = cell.strip('[]').split(', ')\n",
    "    context=[]\n",
    "    for cell in cell_list:\n",
    "        context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "    source_documents_list.append(context)\n",
    "df_questions_answers[\"contexts\"]=source_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtionaly get embeddings for questions\n",
    "\n",
    "if not Path(f'question_embeddings_{index_name}.pickle').exists():\n",
    "    question_embeddings = [\n",
    "        synthetic_embeddings.embed_query(question)\n",
    "        for question in df_questions_answers[\"question\"]\n",
    "    ]\n",
    "    with open(f'question_embeddings_{index_name}.pickle', \"wb\") as f:\n",
    "        pickle.dump(question_embeddings, f)\n",
    "\n",
    "question_embeddings = pickle.load(open(f'question_embeddings_{index_name}.pickle', \"rb\"))\n",
    "df_questions_answers[\"embedding\"] = question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f'ragas_result_cache_{index_name}.txt', \"question\", \"answer_correctness\"\n",
    ")\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unclear why but sometimes ground_truth does not provide a response. Just filter those out.\n",
    "df_questions_answers = df_questions_answers[df_questions_answers['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for evaluation\n",
    "df_qa_eval = df_questions_answers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the answer correctness if not already done\n",
    "fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "for i, row in df_qa_eval.iterrows():\n",
    "    print(i, row[\"question\"])\n",
    "    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "        evaluation_result = evaluate(\n",
    "            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "            [answer_correctness],\n",
    "        )\n",
    "        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "            \"answer_correctness\"\n",
    "        ]\n",
    "\n",
    "        # optionally save the response to cache\n",
    "        response_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "        }\n",
    "        write_dict_to_file(response_dict, f'ragas_result_cache_{index_name}.txt')\n",
    "\n",
    "# write the answer correctness to the original dataframe\n",
    "df_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link from documents to questions, that used the document as source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section adds a column to df_documents containing the ids of the questions that used the document as source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "\n",
    "# Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "# Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "agg = (\n",
    "    df_questions_exploded.groupby(\"source_documents\")\n",
    "    .agg(\n",
    "        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "        question_ids=(\n",
    "            \"id\",\n",
    "            lambda x: list(x),\n",
    "        ),  # List of question IDs referencing the document\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"source_documents\": \"id\"})\n",
    ")\n",
    "\n",
    "# Merge the aggregated information back into df_documents\n",
    "df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "# Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "# Replace NaN values in 'num_questions' with 0\n",
    "df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_qa_eval, df_documents_agg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UMAP only using documents and apply it to the documents and questions\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "df_questions = df[~df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_questions = umap.transform(df[\"embedding\"].values.tolist())\n",
    "\n",
    "\n",
    "df_without_questions = df[df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_without_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_docs = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_all = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "# find the nearet question (by embedding) for each document\n",
    "question_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\n",
    "\n",
    "df[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "    for doc_emb in df[\"embedding\"].values\n",
    "]\n",
    "\n",
    "# write the dataframe to parquet for later use\n",
    "df.to_parquet(f'df_{index_name}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the first cell to use the downloaded dataframes if you skipped the preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "# OR Load the data from downloaded file https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/df_f1_rag_docs_and_questions_umaps_v3.parquet\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe with the question and answer in spotlight\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
