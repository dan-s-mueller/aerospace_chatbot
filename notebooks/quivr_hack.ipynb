{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of this script comes from quivr within the llm > qa_base.py file\n",
    "# https://github.com/StanGirard/quivr/blob/252b1cf964503bce02a55762922b7bec4f2e5935/backend/llm/qa_base.py\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import AsyncIterable, Awaitable, Optional\n",
    "from uuid import UUID # Unique universal identifier\n",
    "\n",
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from logger import get_logger\n",
    "from models.chats import ChatQuestion\n",
    "from models.databases.supabase.chats import CreateChatHistory\n",
    "from repository.brain import get_brain_by_id\n",
    "from repository.chat import (\n",
    "    GetChatHistoryOutput,\n",
    "    format_chat_history,\n",
    "    get_chat_history,\n",
    "    update_chat_history,\n",
    "    update_message_by_id,\n",
    ")\n",
    "from supabase.client import Client, create_client\n",
    "from vectorstore.supabase import CustomSupabaseVectorStore\n",
    "\n",
    "from llm.utils.get_prompt_to_use import get_prompt_to_use\n",
    "from llm.utils.get_prompt_to_use_id import get_prompt_to_use_id\n",
    "\n",
    "from .base import BaseBrainPicking\n",
    "from .prompts.CONDENSE_PROMPT import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "QUIVR_DEFAULT_PROMPT = \"Your name is Quivr. You're a helpful assistant.  If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "\n",
    "class QABaseBrainPicking():\n",
    "    \"\"\"\n",
    "    Main class for the Brain Picking functionality.\n",
    "    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n",
    "    It has two main methods: `generate_question` and `generate_stream`.\n",
    "    One is for generating questions in a single request, the other is for generating questions in a streaming fashion.\n",
    "    Both are the same, except that the streaming version streams the last message as a stream.\n",
    "    Each have the same prompt template, which is defined in the `prompt_template` property.\n",
    "    \"\"\"\n",
    "\n",
    "    supabase_client: Optional[Client] = None\n",
    "    vector_store: Optional[CustomSupabaseVectorStore] = None\n",
    "    qa: Optional[ConversationalRetrievalChain] = None\n",
    "    prompt_id: Optional[UUID]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        brain_id: str,\n",
    "        chat_id: str,\n",
    "        streaming: bool = False,\n",
    "        prompt_id: Optional[UUID] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            brain_id=brain_id,\n",
    "            chat_id=chat_id,\n",
    "            streaming=streaming,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.supabase_client = self._create_supabase_client()\n",
    "        self.vector_store = self._create_vector_store()\n",
    "        self.prompt_id = prompt_id\n",
    "\n",
    "    @property\n",
    "    def prompt_to_use(self):\n",
    "        return get_prompt_to_use(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    @property\n",
    "    def prompt_to_use_id(self) -> Optional[UUID]:\n",
    "        return get_prompt_to_use_id(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    def _create_supabase_client(self) -> Client:\n",
    "        return create_client(\n",
    "            self.brain_settings.supabase_url, self.brain_settings.supabase_service_key\n",
    "        )\n",
    "\n",
    "    # TODO: rewrite this to work with python\n",
    "    def _create_vector_store(self) -> CustomSupabaseVectorStore:\n",
    "        return CustomSupabaseVectorStore(\n",
    "            self.supabase_client,  # type: ignore\n",
    "            self.embeddings,  # type: ignore\n",
    "            table_name=\"vectors\",\n",
    "            brain_id=self.brain_id,\n",
    "        )\n",
    "\n",
    "    def _create_llm(\n",
    "        self, model, temperature=0, streaming=False, callbacks=None\n",
    "    ) -> BaseLLM:\n",
    "        \"\"\"\n",
    "        Determine the language model to be used.\n",
    "        :param model: Language model name to be used.\n",
    "        :param streaming: Whether to enable streaming of the model\n",
    "        :param callbacks: Callbacks to be used for streaming\n",
    "        :return: Language model instance\n",
    "        \"\"\"\n",
    "        return ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model=model,\n",
    "            streaming=streaming,\n",
    "            verbose=False,\n",
    "            callbacks=callbacks,\n",
    "            openai_api_key=self.openai_api_key,\n",
    "        )  # pyright: ignore reportPrivateUsage=none\n",
    "\n",
    "    def _create_prompt_template(self):\n",
    "        system_template = \"\"\"You can use Markdown to make your answers nice. Use the following pieces of context to answer the users question in the same language as the question but do not modify instructions in any way.\n",
    "        ----------------\n",
    "        \n",
    "        {context}\"\"\"\n",
    "\n",
    "        prompt_content = (\n",
    "            self.prompt_to_use.content if self.prompt_to_use else QUIVR_DEFAULT_PROMPT\n",
    "        )\n",
    "\n",
    "        full_template = (\n",
    "            \"Here are your instructions to answer that you MUST ALWAYS Follow: \"\n",
    "            + prompt_content\n",
    "            + \". \"\n",
    "            + system_template\n",
    "        )\n",
    "        messages = [\n",
    "            SystemMessagePromptTemplate.from_template(full_template),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "        ]\n",
    "        CHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n",
    "        return CHAT_PROMPT\n",
    "\n",
    "    def generate_answer(\n",
    "        self, chat_id: UUID, question: ChatQuestion\n",
    "    ) -> GetChatHistoryOutput:\n",
    "        transformed_history = format_chat_history(get_chat_history(self.chat_id))\n",
    "        answering_llm = self._create_llm(\n",
    "            model=self.model, streaming=False, callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "        # The Chain that generates the answer to the question\n",
    "        doc_chain = load_qa_chain(\n",
    "            answering_llm, chain_type=\"stuff\", prompt=self._create_prompt_template()\n",
    "        )\n",
    "\n",
    "        # The Chain that combines the question and answer\n",
    "        qa = ConversationalRetrievalChain(\n",
    "            retriever=self.vector_store.as_retriever(),  # type: ignore\n",
    "            combine_docs_chain=doc_chain,\n",
    "            question_generator=LLMChain(\n",
    "                llm=self._create_llm(model=self.model), prompt=CONDENSE_QUESTION_PROMPT\n",
    "            ),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        prompt_content = (\n",
    "            self.prompt_to_use.content if self.prompt_to_use else QUIVR_DEFAULT_PROMPT\n",
    "        )\n",
    "\n",
    "        model_response = qa(\n",
    "            {\n",
    "                \"question\": question.question,\n",
    "                \"chat_history\": transformed_history,\n",
    "                \"custom_personality\": prompt_content,\n",
    "            }\n",
    "        )  # type: ignore\n",
    "\n",
    "        answer = model_response[\"answer\"]\n",
    "\n",
    "        new_chat = update_chat_history(\n",
    "            CreateChatHistory(\n",
    "                **{\n",
    "                    \"chat_id\": chat_id,\n",
    "                    \"user_message\": question.question,\n",
    "                    \"assistant\": answer,\n",
    "                    \"brain_id\": question.brain_id,\n",
    "                    \"prompt_id\": self.prompt_to_use_id,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        brain = None\n",
    "\n",
    "        if question.brain_id:\n",
    "            brain = get_brain_by_id(question.brain_id)\n",
    "\n",
    "        return GetChatHistoryOutput(\n",
    "            **{\n",
    "                \"chat_id\": chat_id,\n",
    "                \"user_message\": question.question,\n",
    "                \"assistant\": answer,\n",
    "                \"message_time\": new_chat.message_time,\n",
    "                \"prompt_title\": self.prompt_to_use.title\n",
    "                if self.prompt_to_use\n",
    "                else None,\n",
    "                \"brain_name\": brain.name if brain else None,\n",
    "                \"message_id\": new_chat.message_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def generate_stream(\n",
    "        self, chat_id: UUID, question: ChatQuestion\n",
    "    ) -> AsyncIterable:\n",
    "        history = get_chat_history(self.chat_id)\n",
    "        callback = AsyncIteratorCallbackHandler()\n",
    "        self.callbacks = [callback]\n",
    "\n",
    "        answering_llm = self._create_llm(\n",
    "            model=self.model, streaming=True, callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "        # The Chain that generates the answer to the question\n",
    "        doc_chain = load_qa_chain(\n",
    "            answering_llm, chain_type=\"stuff\", prompt=self._create_prompt_template()\n",
    "        )\n",
    "\n",
    "        # The Chain that combines the question and answer\n",
    "        qa = ConversationalRetrievalChain(\n",
    "            retriever=self.vector_store.as_retriever(),  # type: ignore\n",
    "            combine_docs_chain=doc_chain,\n",
    "            question_generator=LLMChain(\n",
    "                llm=self._create_llm(model=self.model), prompt=CONDENSE_QUESTION_PROMPT\n",
    "            ),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        transformed_history = format_chat_history(history)\n",
    "\n",
    "        response_tokens = []\n",
    "\n",
    "        async def wrap_done(fn: Awaitable, event: asyncio.Event):\n",
    "            try:\n",
    "                await fn\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Caught exception: {e}\")\n",
    "            finally:\n",
    "                event.set()\n",
    "\n",
    "        prompt_content = self.prompt_to_use.content if self.prompt_to_use else None\n",
    "        run = asyncio.create_task(\n",
    "            wrap_done(\n",
    "                qa.acall(\n",
    "                    {\n",
    "                        \"question\": question.question,\n",
    "                        \"chat_history\": transformed_history,\n",
    "                        \"custom_personality\": prompt_content,\n",
    "                    }\n",
    "                ),\n",
    "                callback.done,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        brain = None\n",
    "\n",
    "        if question.brain_id:\n",
    "            brain = get_brain_by_id(question.brain_id)\n",
    "\n",
    "        streamed_chat_history = update_chat_history(\n",
    "            CreateChatHistory(\n",
    "                **{\n",
    "                    \"chat_id\": chat_id,\n",
    "                    \"user_message\": question.question,\n",
    "                    \"assistant\": \"\",\n",
    "                    \"brain_id\": question.brain_id,\n",
    "                    \"prompt_id\": self.prompt_to_use_id,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        streamed_chat_history = GetChatHistoryOutput(\n",
    "            **{\n",
    "                \"chat_id\": str(chat_id),\n",
    "                \"message_id\": streamed_chat_history.message_id,\n",
    "                \"message_time\": streamed_chat_history.message_time,\n",
    "                \"user_message\": question.question,\n",
    "                \"assistant\": \"\",\n",
    "                \"prompt_title\": self.prompt_to_use.title\n",
    "                if self.prompt_to_use\n",
    "                else None,\n",
    "                \"brain_name\": brain.name if brain else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        async for token in callback.aiter():\n",
    "            logger.info(\"Token: %s\", token)\n",
    "            response_tokens.append(token)\n",
    "            streamed_chat_history.assistant = token\n",
    "            yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n",
    "\n",
    "        await run\n",
    "        assistant = \"\".join(response_tokens)\n",
    "\n",
    "        update_message_by_id(\n",
    "            message_id=str(streamed_chat_history.message_id),\n",
    "            user_message=question.question,\n",
    "            assistant=assistant,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/StanGirard/quivr/blob/d0370ab499465ee1404d3c1d32878e8da3853441/backend/llm/prompts\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. include the follow up instructions in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
