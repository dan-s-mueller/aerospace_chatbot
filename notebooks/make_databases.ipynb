{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import admin\n",
    "import data_processing\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to split chunking from upserting\n",
    "def create_upsert(index_type,index_name,query_model,rag_type,chunker,summary_llm):\n",
    "    # Set index names for special databases\n",
    "    if rag_type == 'Parent-Child':\n",
    "        index_name = index_name + '-parent-child'\n",
    "    if rag_type == 'Summary':\n",
    "        index_name = index_name + '-' + summary_llm.model_name.replace('/', '-').replace(' ','-').lower() + '-summary' \n",
    "\n",
    "    try:\n",
    "        vectorstore = data_processing.initialize_database(index_type, \n",
    "                                            index_name, \n",
    "                                            query_model,\n",
    "                                            rag_type=rag_type,\n",
    "                                            clear=True, \n",
    "                                            local_db_path=os.getenv('LOCAL_DB_PATH'),\n",
    "                                            init_ragatouille=True,\n",
    "                                            show_progress=False)\n",
    "        print(f\"Database {index_name} created.\")\n",
    "        vectorstore, _ = data_processing.upsert_docs(index_type, \n",
    "                                        index_name,\n",
    "                                        vectorstore,\n",
    "                                        chunker,\n",
    "                                        batch_size=400,\n",
    "                                        show_progress=False,\n",
    "                                        local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database {index_name} upserted chunks.\")\n",
    "    except Exception as e:  # If there is an error, be sure to delete the database\n",
    "        data_processing.delete_index(index_type, \n",
    "                                    index_name,\n",
    "                                    rag_type,\n",
    "                                    local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database deleted: {index_name}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets, Models, Docs, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 23:20:48] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Read setup data, assign models\n",
    "json_file_path = \"databases.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "sb={}\n",
    "query_params=setup_data['query_models']\n",
    "query_models=[]\n",
    "for model in query_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    query_models.append(admin.get_query_model(sb, secrets))\n",
    "\n",
    "llm_params=setup_data['llms']\n",
    "llms=[]\n",
    "for model in llm_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    llms.append(admin.set_llm(sb, secrets))\n",
    "\n",
    "chunk_params=setup_data['chunk_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra query types that take a long time. Add to the databases.json file\n",
    "\n",
    "{\n",
    "    \"id\": \"3\",\n",
    "    \"index_type\": \"ChromaDB\",\n",
    "    \"query_model\": \"Hugging Face\",\n",
    "    \"embedding_name\": \"Dedicated Endpoint\",\n",
    "    \"embedding_hf_endpoint\": \"https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud\"\n",
    "},\n",
    "{\n",
    "    \"id\": \"4\",\n",
    "    \"index_type\": \"RAGatouille\",\n",
    "    \"embedding_name\": \"colbert-ir/colbertv2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs\n",
    "data_folder='../data/AMS'\n",
    "docs= glob.glob(os.path.join(data_folder,'*.pdf'))   # Only get the PDFs in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All query models (except RAGatouille), standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2222 chunks from 2222 pages.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2-instruct'}\n",
      "Database voyage-large-2-instruct-2merge-0 created.\n",
      "Database voyage-large-2-instruct-2merge-0 upserted chunks.\n",
      "Created 33478 chunks from 4439 pages.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2-instruct'}\n",
      "Database voyage-large-2-instruct-0merge-400 created.\n",
      "Database voyage-large-2-instruct-0merge-400 upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "for i_chunk in range(len(chunk_params)):\n",
    "    # Chunk the docs before creating and upserting into the database\n",
    "    chunker=data_processing.chunk_docs(docs,\n",
    "                rag_type=rag_type,\n",
    "                n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "                chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "                chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "                llm=summary_llm,\n",
    "                show_progress=False)\n",
    "\n",
    "    print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "    # for i_run in range(len(query_params)-1):\n",
    "    # TODO Uncomment above to loop through all query models. Updated below to rerun pinecone only.\n",
    "    i_run=1\n",
    "    \n",
    "    # Create and upsert database\n",
    "    print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "    index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "    index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "    create_upsert(query_params[i_run]['index_type'],\n",
    "                    index_name,\n",
    "                    query_models[i_run],\n",
    "                    rag_type,\n",
    "                    chunker,\n",
    "                    summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, parent-child, 400 character-recursive chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Parent-Child'\n",
    "summary_llm=None\n",
    "i_chunk=1   # 400 character-recursive setting\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 36148 chunks from 2 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-0merge-400-parent-child created.\n",
      "Database text-embedding-3-large-0merge-400-parent-child upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Summary'\n",
    "summary_llm=llms[2] # meta-llama-3-8b-instruct-dsm\n",
    "\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['summaries'])} summaries from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGatouille, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=2     # RAGatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2222 chunks from 2222 pages.\n",
      "Creating and uploading database with these params: {'id': '3', 'index_type': 'RAGatouille', 'embedding_name': 'colbert-ir/colbertv2.0'}\n",
      "Database colbert-ir-colbertv2.0-2merge-0 created.\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jul 02, 22:29:11] #> Creating directory /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0 \n",
      "\n",
      "\n",
      "[Jul 02, 22:29:13] [0] \t\t #> Encoding 15090 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 50/50 [03:25<00:00,  4.11s/it]\n",
      "100%|██████████| 50/50 [01:42<00:00,  2.06s/it]\n",
      "100%|██████████| 50/50 [01:42<00:00,  2.04s/it]\n",
      "100%|██████████| 50/50 [01:44<00:00,  2.10s/it]\n",
      "100%|██████████| 50/50 [01:45<00:00,  2.10s/it]\n",
      "100%|██████████| 50/50 [01:44<00:00,  2.10s/it]\n",
      "100%|██████████| 50/50 [01:44<00:00,  2.08s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.13s/it]\n",
      "100%|██████████| 50/50 [01:44<00:00,  2.10s/it]\n",
      "100%|██████████| 22/22 [00:46<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 22:47:25] [0] \t\t avg_doclen_est = 173.15573120117188 \t len(local_sample) = 15,090\n",
      "[Jul 02, 22:47:26] [0] \t\t Creating 16,384 partitions.\n",
      "[Jul 02, 22:47:26] [0] \t\t *Estimated* 2,612,919 embeddings.\n",
      "[Jul 02, 22:47:26] [0] \t\t #> Saving the indexing plan to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0/plan.json ..\n",
      "used 20 iterations (18.5999s) to cluster 2562920 items into 16384 clusters\n",
      "[0.037, 0.039, 0.037, 0.036, 0.037, 0.041, 0.038, 0.035, 0.036, 0.038, 0.036, 0.036, 0.038, 0.039, 0.035, 0.039, 0.035, 0.038, 0.036, 0.036, 0.036, 0.038, 0.037, 0.038, 0.036, 0.037, 0.038, 0.037, 0.038, 0.039, 0.037, 0.042, 0.04, 0.036, 0.037, 0.035, 0.04, 0.038, 0.036, 0.045, 0.039, 0.036, 0.037, 0.038, 0.036, 0.034, 0.036, 0.04, 0.039, 0.035, 0.035, 0.038, 0.039, 0.039, 0.037, 0.037, 0.044, 0.038, 0.046, 0.037, 0.036, 0.041, 0.039, 0.038, 0.038, 0.038, 0.04, 0.038, 0.036, 0.038, 0.039, 0.037, 0.036, 0.039, 0.038, 0.037, 0.039, 0.039, 0.039, 0.04, 0.039, 0.036, 0.039, 0.041, 0.035, 0.039, 0.039, 0.038, 0.035, 0.041, 0.037, 0.043, 0.036, 0.039, 0.038, 0.041, 0.041, 0.036, 0.037, 0.036, 0.037, 0.04, 0.038, 0.037, 0.043, 0.034, 0.038, 0.036, 0.038, 0.037, 0.038, 0.039, 0.039, 0.036, 0.038, 0.036, 0.039, 0.038, 0.037, 0.037, 0.037, 0.035, 0.039, 0.041, 0.035, 0.04, 0.039, 0.036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 22:47:48] [0] \t\t #> Encoding 15090 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:44<00:00,  2.09s/it]\n",
      "100%|██████████| 50/50 [01:45<00:00,  2.11s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "100%|██████████| 50/50 [01:46<00:00,  2.14s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "100%|██████████| 50/50 [01:41<00:00,  2.03s/it]\n",
      "100%|██████████| 50/50 [01:41<00:00,  2.04s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.07s/it]\n",
      "100%|██████████| 22/22 [00:44<00:00,  2.04s/it]\n",
      "1it [17:46, 1066.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 377.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 23:05:35] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 02, 23:05:35] #> Building the emb2pid mapping..\n",
      "[Jul 02, 23:05:35] len(emb2pid) = 2612920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:00<00:00, 63320.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 23:05:35] #> Saved optimized IVF to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n",
      "Database colbert-ir-colbertv2.0-2merge-0 upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
