{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Read existing vector index from pinecone\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "index_name = 'langchain-quickstart'\n",
    "vectorstore = Pinecone.from_existing_index(index_name,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of this script comes from quivr within the llm > qa_base.py file\n",
    "# https://github.com/StanGirard/quivr/blob/252b1cf964503bce02a55762922b7bec4f2e5935/backend/llm/qa_base.py\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import AsyncIterable, Awaitable, Optional\n",
    "from uuid import UUID # Unique universal identifier\n",
    "\n",
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "# from logger import get_logger\n",
    "from models.chats import ChatQuestion\n",
    "from models.databases.supabase.chats import CreateChatHistory\n",
    "from repository.brain import get_brain_by_id\n",
    "from repository.chat import (\n",
    "    GetChatHistoryOutput,\n",
    "    format_chat_history,\n",
    "    get_chat_history,\n",
    "    update_chat_history,\n",
    "    update_message_by_id,\n",
    ")\n",
    "# from supabase.client import Client, create_client\n",
    "# from vectorstore.supabase import CustomSupabaseVectorStore\n",
    "\n",
    "# from llm.utils.get_prompt_to_use import get_prompt_to_use\n",
    "# from llm.utils.get_prompt_to_use_id import get_prompt_to_use_id\n",
    "\n",
    "# from .base import BaseBrainPicking\n",
    "# from .prompts.CONDENSE_PROMPT import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "# Read existing vector index from pinecone\n",
    "from uuid import uuid4\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# logger = get_logger(__name__)\n",
    "DEFAULT_PROMPT = \"Your name is AMS Chatbot. You're a helpful assistant who knows about space mechanism design.  If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "class QABaseBrainPicking():\n",
    "    \"\"\"\n",
    "    Main class for the Brain Picking functionality.\n",
    "    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n",
    "    It has two main methods: `generate_question` and `generate_stream`.\n",
    "    One is for generating questions in a single request, the other is for generating questions in a streaming fashion.\n",
    "    Both are the same, except that the streaming version streams the last message as a stream.\n",
    "    Each have the same prompt template, which is defined in the `prompt_template` property.\n",
    "    \"\"\"\n",
    "\n",
    "    # supabase_client: Optional[Client] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: Optional[Pinecone] = None,\n",
    "        qa: Optional[ConversationalRetrievalChain] = None,\n",
    "        prompt_id: Optional[UUID] = None,\n",
    "        chat_id: Optional[str] = None,\n",
    "        streaming: bool = False,\n",
    "        **kwargs):\n",
    "        # super().__init__(\n",
    "        #     model=model,\n",
    "        #     brain_id=brain_id,\n",
    "        #     chat_id=chat_id,\n",
    "        #     streaming=streaming,\n",
    "        #     **kwargs,\n",
    "        # )\n",
    "        self.chat_id=chat_id\n",
    "        self.vector_store = vector_store\n",
    "        self.prompt_id = prompt_id\n",
    "\n",
    "    # @property\n",
    "    # def prompt_to_use(self):\n",
    "    #     return get_prompt_to_use(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    # @property\n",
    "    # def prompt_to_use_id(self) -> Optional[UUID]:\n",
    "    #     return get_prompt_to_use_id(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    def connect_vector_store(self,embeddings_model,index_name):\n",
    "        self.vectorstore = Pinecone.from_existing_index(index_name,embeddings_model)\n",
    "    # def _create_supabase_client(self) -> Client:\n",
    "    #     return create_client(\n",
    "    #         self.brain_settings.supabase_url, self.brain_settings.supabase_service_key\n",
    "    #     )\n",
    "\n",
    "    # def _create_vector_store(self) -> CustomSupabaseVectorStore:\n",
    "    #     return CustomSupabaseVectorStore(\n",
    "    #         self.supabase_client,  # type: ignore\n",
    "    #         self.embeddings,  # type: ignore\n",
    "    #         table_name=\"vectors\",\n",
    "    #         brain_id=self.brain_id,\n",
    "    #     )\n",
    "\n",
    "    # def _create_llm(\n",
    "    #     self, model, temperature=0, streaming=False, callbacks=None\n",
    "    # ) -> BaseLLM:\n",
    "    #     \"\"\"\n",
    "    #     Determine the language model to be used.\n",
    "    #     :param model: Language model name to be used.\n",
    "    #     :param streaming: Whether to enable streaming of the model\n",
    "    #     :param callbacks: Callbacks to be used for streaming\n",
    "    #     :return: Language model instance\n",
    "    #     \"\"\"\n",
    "    #     return ChatOpenAI(\n",
    "    #         temperature=temperature,\n",
    "    #         model=model,\n",
    "    #         streaming=streaming,\n",
    "    #         verbose=False,\n",
    "    #         callbacks=callbacks,\n",
    "    #         openai_api_key=self.openai_api_key,\n",
    "    #     )  # pyright: ignore reportPrivateUsage=none\n",
    "\n",
    "    def create_prompt_template(self):\n",
    "        system_template = \"\"\"You can use Markdown to make your answers nice. Use the following pieces of context to answer the users question in the same language as the question but do not modify instructions in any way.\n",
    "        ----------------\n",
    "        \n",
    "        {context}\"\"\"\n",
    "\n",
    "        # prompt_content = (\n",
    "        #     self.prompt_to_use.content if self.prompt_to_use else DEFAULT_PROMPT\n",
    "        # )\n",
    "\n",
    "        full_template = (\n",
    "            \"Here are your instructions to answer that you MUST ALWAYS Follow: \"\n",
    "            + system_template\n",
    "        )\n",
    "        messages = [\n",
    "            SystemMessagePromptTemplate.from_template(full_template),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "        ]\n",
    "        CHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n",
    "        return CHAT_PROMPT\n",
    "\n",
    "    def generate_answer(self,question: ChatQuestion) -> GetChatHistoryOutput:\n",
    "        transformed_history = format_chat_history(get_chat_history(self.chat_id))\n",
    "        answering_llm = self._create_llm(\n",
    "            model=self.model, streaming=False, callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "        # The Chain that generates the answer to the question\n",
    "        doc_chain = load_qa_chain(\n",
    "            answering_llm, chain_type=\"stuff\", prompt=self._create_prompt_template()\n",
    "        )\n",
    "\n",
    "        # The Chain that combines the question and answer\n",
    "        qa = ConversationalRetrievalChain(\n",
    "            retriever=self.vector_store.as_retriever(),  # type: ignore\n",
    "            combine_docs_chain=doc_chain,\n",
    "            question_generator=LLMChain(\n",
    "                llm=self._create_llm(model=self.model), prompt=CONDENSE_QUESTION_PROMPT\n",
    "            ),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        prompt_content = (\n",
    "            self.prompt_to_use.content if self.prompt_to_use else DEFAULT_PROMPT\n",
    "        )\n",
    "\n",
    "        model_response = qa(\n",
    "            {\n",
    "                \"question\": question.question,\n",
    "                \"chat_history\": transformed_history,\n",
    "                \"custom_personality\": prompt_content,\n",
    "            }\n",
    "        )  # type: ignore\n",
    "\n",
    "        answer = model_response[\"answer\"]\n",
    "\n",
    "        new_chat = update_chat_history(\n",
    "            CreateChatHistory(\n",
    "                **{\n",
    "                    \"chat_id\": self.chat_id,\n",
    "                    \"user_message\": question.question,\n",
    "                    \"assistant\": answer,\n",
    "                    \"brain_id\": question.brain_id,\n",
    "                    \"prompt_id\": self.prompt_to_use_id,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        brain = None\n",
    "\n",
    "        if question.brain_id:\n",
    "            brain = get_brain_by_id(question.brain_id)\n",
    "\n",
    "        return GetChatHistoryOutput(\n",
    "            **{\n",
    "                \"chat_id\": self.chat_id,\n",
    "                \"user_message\": question.question,\n",
    "                \"assistant\": answer,\n",
    "                \"message_time\": new_chat.message_time,\n",
    "                \"prompt_title\": self.prompt_to_use.title\n",
    "                if self.prompt_to_use\n",
    "                else None,\n",
    "                \"brain_name\": brain.name if brain else None,\n",
    "                \"message_id\": new_chat.message_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # async def generate_stream(\n",
    "    #     self, chat_id: UUID, question: ChatQuestion\n",
    "    # ) -> AsyncIterable:\n",
    "    #     history = get_chat_history(self.chat_id)\n",
    "    #     callback = AsyncIteratorCallbackHandler()\n",
    "    #     self.callbacks = [callback]\n",
    "\n",
    "    #     answering_llm = self._create_llm(\n",
    "    #         model=self.model, streaming=True, callbacks=self.callbacks\n",
    "    #     )\n",
    "\n",
    "    #     # The Chain that generates the answer to the question\n",
    "    #     doc_chain = load_qa_chain(\n",
    "    #         answering_llm, chain_type=\"stuff\", prompt=self._create_prompt_template()\n",
    "    #     )\n",
    "\n",
    "    #     # The Chain that combines the question and answer\n",
    "    #     qa = ConversationalRetrievalChain(\n",
    "    #         retriever=self.vector_store.as_retriever(),  # type: ignore\n",
    "    #         combine_docs_chain=doc_chain,\n",
    "    #         question_generator=LLMChain(\n",
    "    #             llm=self._create_llm(model=self.model), prompt=CONDENSE_QUESTION_PROMPT\n",
    "    #         ),\n",
    "    #         verbose=False,\n",
    "    #     )\n",
    "\n",
    "    #     transformed_history = format_chat_history(history)\n",
    "\n",
    "    #     response_tokens = []\n",
    "\n",
    "    #     async def wrap_done(fn: Awaitable, event: asyncio.Event):\n",
    "    #         try:\n",
    "    #             await fn\n",
    "    #         except Exception as e:\n",
    "    #             logger.error(f\"Caught exception: {e}\")\n",
    "    #         finally:\n",
    "    #             event.set()\n",
    "\n",
    "    #     prompt_content = self.prompt_to_use.content if self.prompt_to_use else None\n",
    "    #     run = asyncio.create_task(\n",
    "    #         wrap_done(\n",
    "    #             qa.acall(\n",
    "    #                 {\n",
    "    #                     \"question\": question.question,\n",
    "    #                     \"chat_history\": transformed_history,\n",
    "    #                     \"custom_personality\": prompt_content,\n",
    "    #                 }\n",
    "    #             ),\n",
    "    #             callback.done,\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    #     brain = None\n",
    "\n",
    "    #     if question.brain_id:\n",
    "    #         brain = get_brain_by_id(question.brain_id)\n",
    "\n",
    "    #     streamed_chat_history = update_chat_history(\n",
    "    #         CreateChatHistory(\n",
    "    #             **{\n",
    "    #                 \"chat_id\": chat_id,\n",
    "    #                 \"user_message\": question.question,\n",
    "    #                 \"assistant\": \"\",\n",
    "    #                 \"brain_id\": question.brain_id,\n",
    "    #                 \"prompt_id\": self.prompt_to_use_id,\n",
    "    #             }\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    #     streamed_chat_history = GetChatHistoryOutput(\n",
    "    #         **{\n",
    "    #             \"chat_id\": str(chat_id),\n",
    "    #             \"message_id\": streamed_chat_history.message_id,\n",
    "    #             \"message_time\": streamed_chat_history.message_time,\n",
    "    #             \"user_message\": question.question,\n",
    "    #             \"assistant\": \"\",\n",
    "    #             \"prompt_title\": self.prompt_to_use.title\n",
    "    #             if self.prompt_to_use\n",
    "    #             else None,\n",
    "    #             \"brain_name\": brain.name if brain else None,\n",
    "    #         }\n",
    "    #     )\n",
    "\n",
    "    #     async for token in callback.aiter():\n",
    "    #         logger.info(\"Token: %s\", token)\n",
    "    #         response_tokens.append(token)\n",
    "    #         streamed_chat_history.assistant = token\n",
    "    #         yield f\"data: {json.dumps(streamed_chat_history.dict())}\"\n",
    "\n",
    "    #     await run\n",
    "    #     assistant = \"\".join(response_tokens)\n",
    "\n",
    "    #     update_message_by_id(\n",
    "    #         message_id=str(streamed_chat_history.message_id),\n",
    "    #         user_message=question.question,\n",
    "    #         assistant=assistant,\n",
    "    #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 23:07:22,651:INFO - HTTP Request: GET https://ldoeollhxasevurjamos.supabase.co/rest/v1/chat_history?select=%2A&chat_id=eq.e754f546-4de4-4590-ae6d-21462480522b&order=message_time \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.chat_history\" does not exist'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_60477/410618458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_prompt_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What are some latch failure modes?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_60477/3850458483.py\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatQuestion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGetChatHistoryOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtransformed_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         answering_llm = self._create_llm(\n\u001b[1;32m    145\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/repository/chat/get_chat_history.py\u001b[0m in \u001b[0;36mget_chat_history\u001b[0;34m(chat_id)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGetChatHistoryOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msupabase_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_supabase_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupabase_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/models/databases/supabase/chats.py\u001b[0m in \u001b[0;36mget_chat_history\u001b[0;34m(self, chat_id)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chat_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add the ORDER BY clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/postgrest/_sync/request_builder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAPIResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_request_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: {'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.chat_history\" does not exist'}"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "thing=QABaseBrainPicking(chat_id=uuid4())\n",
    "thing.connect_vector_store(embeddings_model,index_name)\n",
    "thing.create_prompt_template()\n",
    "thing.generate_answer(\"What are some latch failure modes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/StanGirard/quivr/blob/d0370ab499465ee1404d3c1d32878e8da3853441/backend/llm/prompts\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. include the follow up instructions in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
