{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta, archive_db, get_docs_questions_df\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "    if Path(file_name).exists():\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_questions_answers_in, df_docs):\n",
    "    df_questions_answers_out=df_questions_answers_in.copy()\n",
    "    \n",
    "    # Generate responses using RAG with input parameters\n",
    "    for i, row in df_questions_answers_out.iterrows():\n",
    "        if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "            print(f\"Processing question {i+1}/{len(df_questions_answers_out)}\")\n",
    "\n",
    "            # Use the QA model to query the documents\n",
    "            qa_obj=queries.QA_Model(index_type,\n",
    "                            index_name,\n",
    "                            query_model,\n",
    "                            llm,\n",
    "                            **QA_model_params)\n",
    "            qa_obj.query_docs(row['question'])\n",
    "            response=qa_obj.result\n",
    "\n",
    "            df_questions_answers_out.loc[df_questions_answers_out.index[i], \"answer\"] = response['answer'].content\n",
    "\n",
    "            ids=[_stable_hash_meta(source_document.metadata)\n",
    "                for source_document in response['references']]\n",
    "            df_questions_answers_out.loc[df_questions_answers_out.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "            # Save the response to cache file\n",
    "            response_dict = {\n",
    "                \"question\": row['question'],\n",
    "                \"answer\": response['answer'].content,\n",
    "                \"source_documents\": ids,\n",
    "            }\n",
    "            write_dict_to_file(response_dict, f'rag_response_cache_{index_name}.json')\n",
    "\n",
    "    # Get the context documents content for each question\n",
    "    source_documents_list = []\n",
    "    for cell in df_questions_answers_out['source_documents']:\n",
    "        cell_list = cell.strip('[]').split(', ')\n",
    "        context=[]\n",
    "        for cell in cell_list:\n",
    "            context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "        source_documents_list.append(context)\n",
    "    df_questions_answers_out[\"contexts\"]=source_documents_list\n",
    "\n",
    "    # Addtionaly get embeddings for questions\n",
    "    if not Path(f'question_embeddings_{index_name}.pickle').exists():\n",
    "        question_embeddings = [\n",
    "            query_model.embed_query(question)\n",
    "            for question in df_questions_answers_out[\"question\"]\n",
    "        ]\n",
    "        with open(f'question_embeddings_{index_name}.pickle', \"wb\") as f:\n",
    "            pickle.dump(question_embeddings, f)\n",
    "\n",
    "    question_embeddings = pickle.load(open(f'question_embeddings_{index_name}.pickle', \"rb\"))\n",
    "    df_questions_answers_out[\"question_embedding\"] = question_embeddings\n",
    "    return df_questions_answers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rag(index_name, df_questions_answers_in):\n",
    "    df_questions_answers_out=df_questions_answers_in.copy()\n",
    "\n",
    "    # Add answer correctness column, fill in if it exists\n",
    "    df_questions_answers_out = add_cached_column_from_file(\n",
    "        df_questions_answers_out, f'ragas_result_cache_{index_name}.txt', \"question\", \"answer_correctness\"\n",
    "    )\n",
    "\n",
    "    # Unclear why but sometimes ground_truth does not provide a response. Just filter those out.\n",
    "    df_questions_answers_out = df_questions_answers_out[df_questions_answers_out['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "    df_questions_answers_out\n",
    "\n",
    "    # Prepare the dataframe for evaluation\n",
    "    df_qa_eval = df_questions_answers_out.copy()\n",
    "\n",
    "    # Evaluate the answer correctness if not already done\n",
    "    fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "    for i, row in df_qa_eval.iterrows():\n",
    "        print(i, row[\"question\"])\n",
    "        if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "            evaluation_result = evaluate(\n",
    "                Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "                [answer_correctness],\n",
    "            )\n",
    "            df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "                \"answer_correctness\"\n",
    "            ]\n",
    "\n",
    "            # optionally save the response to cache\n",
    "            response_dict = {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "            }\n",
    "            write_dict_to_file(response_dict, f'ragas_result_cache_{index_name}.txt')\n",
    "\n",
    "    # write the answer correctness to the original dataframe\n",
    "    df_questions_answers_out[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n",
    "\n",
    "    return df_qa_eval, df_questions_answers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_viz_prep(index_name,df_qa_eval,df_docs):\n",
    "    # This section adds a column to df_documents containing the ids of the questions that used the document as source. \n",
    "    # Add the infos about questions using each document to the documents dataframe.\n",
    "\n",
    "    # Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "    df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "    # Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "    agg = (\n",
    "        df_questions_exploded.groupby(\"source_documents\")\n",
    "        .agg(\n",
    "            num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "            question_ids=(\n",
    "                \"id\",\n",
    "                lambda x: list(x),\n",
    "            ),  # List of question IDs referencing the document\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"source_documents\": \"id\"})\n",
    "    )\n",
    "\n",
    "    # Merge the aggregated information back into df_documents\n",
    "    df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "    # Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "    df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    # Replace NaN values in 'num_questions' with 0\n",
    "    df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    df_visualize = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "    # Create UMAP only using documents and apply it to the documents and questions\n",
    "    df_questions = df_visualize[~df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_questions[\"question_embedding\"].values.tolist()\n",
    "    )\n",
    "\n",
    "    df_without_questions = df_visualize[df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_without_questions[\"question_embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_docs = umap.transform(df_visualize[\"question_embedding\"].values.tolist())\n",
    "    df_visualize[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_visualize[\"question_embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_all = umap.transform(df_visualize[\"question_embedding\"].values.tolist())\n",
    "    df_visualize[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "    # find the nearet question (by question_embedding) for each document\n",
    "    question_embeddings = np.array(df_visualize[df_visualize[\"question\"].notna()][\"question_embedding\"].tolist())\n",
    "\n",
    "    df_visualize[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "        np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "        for doc_emb in df_visualize[\"question_embedding\"].values\n",
    "    ]\n",
    "\n",
    "    # write the dataframe to parquet for later use\n",
    "    df_visualize.to_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "    return df_visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'text-embedding-3-large-2merge-0',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first db\n",
    "db=dbs[0]\n",
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=db['index_name'],\n",
    "                        embedding_function=db['query_model'])  \n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "for db in dbs:\n",
    "    df_temp_chroma=archive_db('ChromaDB',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>page</th>\n",
       "      <th>metadata</th>\n",
       "      <th>document</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001889abd34c56d712ba79f905d7b90159f5c354</td>\n",
       "      <td>['AMS_2001_reocr.pdf', 'AMS_2001_reocr.pdf']</td>\n",
       "      <td>[37, 38]</td>\n",
       "      <td>{'page': '[37, 38]', 'source': '['AMS_2001_reo...</td>\n",
       "      <td>Using a Ball Aerospace-developed  lubricant  f...</td>\n",
       "      <td>[0.013638148084282875, -0.023859944194555283, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001bc8af131361ed46a8999ade3e7068522572a1</td>\n",
       "      <td>['AMS_2016_reocr.pdf', 'AMS_2016_reocr.pdf']</td>\n",
       "      <td>[172, 173]</td>\n",
       "      <td>{'page': '[172, 173]', 'source': '['AMS_2016_r...</td>\n",
       "      <td>158 Design Ideology and Testing  \\n\\nEarly Des...</td>\n",
       "      <td>[0.0009454930550418794, 0.018359793350100517, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0030b47c6180d167c2d013c8e7a8b8bb0c16de44</td>\n",
       "      <td>['AMS_2001_reocr.pdf', 'AMS_2001_reocr.pdf']</td>\n",
       "      <td>[43, 44]</td>\n",
       "      <td>{'page': '[43, 44]', 'source': '['AMS_2001_reo...</td>\n",
       "      <td>The Vertrel XF treated bearing had a slight am...</td>\n",
       "      <td>[-0.00619348743930459, -0.028075991198420525, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00356e19ac534a10e86a774fa22603978622d654</td>\n",
       "      <td>['AMS_2018_reocr.pdf', 'AMS_2018_reocr.pdf']</td>\n",
       "      <td>[523, 524]</td>\n",
       "      <td>{'page': '[523, 524]', 'source': '['AMS_2018_r...</td>\n",
       "      <td>•Terminate the test at the first onset of unex...</td>\n",
       "      <td>[-0.036841981112957, -0.022101974114775658, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003edd3f9efa244ccfd87445ebd5edff5c3155b3</td>\n",
       "      <td>['AMS_2020_reocr.pdf', 'AMS_2020_reocr.pdf']</td>\n",
       "      <td>[197, 198]</td>\n",
       "      <td>{'page': '[197, 198]', 'source': '['AMS_2020_r...</td>\n",
       "      <td>7 oe ~ Of Dual ~187 a highly loaded planet, a ...</td>\n",
       "      <td>[-0.002627367153763771, -0.006074088159948587,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  001889abd34c56d712ba79f905d7b90159f5c354   \n",
       "1  001bc8af131361ed46a8999ade3e7068522572a1   \n",
       "2  0030b47c6180d167c2d013c8e7a8b8bb0c16de44   \n",
       "3  00356e19ac534a10e86a774fa22603978622d654   \n",
       "4  003edd3f9efa244ccfd87445ebd5edff5c3155b3   \n",
       "\n",
       "                                         source        page  \\\n",
       "0  ['AMS_2001_reocr.pdf', 'AMS_2001_reocr.pdf']    [37, 38]   \n",
       "1  ['AMS_2016_reocr.pdf', 'AMS_2016_reocr.pdf']  [172, 173]   \n",
       "2  ['AMS_2001_reocr.pdf', 'AMS_2001_reocr.pdf']    [43, 44]   \n",
       "3  ['AMS_2018_reocr.pdf', 'AMS_2018_reocr.pdf']  [523, 524]   \n",
       "4  ['AMS_2020_reocr.pdf', 'AMS_2020_reocr.pdf']  [197, 198]   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'page': '[37, 38]', 'source': '['AMS_2001_reo...   \n",
       "1  {'page': '[172, 173]', 'source': '['AMS_2016_r...   \n",
       "2  {'page': '[43, 44]', 'source': '['AMS_2001_reo...   \n",
       "3  {'page': '[523, 524]', 'source': '['AMS_2018_r...   \n",
       "4  {'page': '[197, 198]', 'source': '['AMS_2020_r...   \n",
       "\n",
       "                                            document  \\\n",
       "0  Using a Ball Aerospace-developed  lubricant  f...   \n",
       "1  158 Design Ideology and Testing  \\n\\nEarly Des...   \n",
       "2  The Vertrel XF treated bearing had a slight am...   \n",
       "3  •Terminate the test at the first onset of unex...   \n",
       "4  7 oe ~ Of Dual ~187 a highly loaded planet, a ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.013638148084282875, -0.023859944194555283, ...  \n",
       "1  [0.0009454930550418794, 0.018359793350100517, ...  \n",
       "2  [-0.00619348743930459, -0.028075991198420525, ...  \n",
       "3  [-0.036841981112957, -0.022101974114775658, -0...  \n",
       "4  [-0.002627367153763771, -0.006074088159948587,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "embedding_model='text-embedding-3-large'\n",
    "synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022502250225022502\n"
     ]
    }
   ],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "sample_size=50\n",
    "percent_total=sample_size/len(lcdocs_chroma)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs_chroma, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32fe354e7a341afaf33fba5c051d45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0951a9f609446bae1eed10b4723e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Microvibration', 'System-level', 'Transmission path', 'Performance requirements', 'Microvibration verification']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How is the transmission path of microvibrations investigated at the system level?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the investigation of the transmission path of microvibrations at the system level. It is clear in specifying the topic of interest (transmission path of microvibrations) and the scope (system level). However, it could benefit from more specificity regarding the type of system or the methods used for investigation. For example, specifying whether the system is mechanical, electronic, or another type, and what kind of investigative techniques are being referred to (e.g., experimental, computational, analytical) would enhance clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'System-level microvibration assessments investigate the transmission of very low vibrations by the spacecraft structure from the disturbance source to the receiver location.', 'verdict': '1'}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Microvibration', 'System-level', 'Transmission path', 'Performance requirements', 'Microvibration verification']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the challenges in verifying microvibration performances at the system level?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the challenges in verifying microvibration performances at the system level. It is clear in its intent, seeking information on specific difficulties encountered in this verification process. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What are the challenges in verifying microvibration performances at the system level?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What are the challenges faced in verifying the microvibration performances at the system level and which verification methodologies are appropriate for this purpose?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the challenges in verifying microvibration performances at the system level and the appropriate verification methodologies for this purpose. It is clear in specifying the topic of interest (microvibration performance verification) and seeks detailed information on both the challenges and methodologies. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: What challenges are there in verifying microvibration performances at the system level and which verification methods are appropriate?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The second question not only asks about the challenges in verifying microvibration performances at the system level but also inquires about the appropriate verification methods, thus requiring a broader and deeper response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The challenges in verifying microvibration performances at the system level include the difficulty in directly measuring microvibration susceptibility, the inability to validate performances on ground in a representative on-orbit environment, and the high background noise that may interfere with measurements. Appropriate verification methods include a combination of analytical predictions and hardware tests, such as Finite Element Modeling (FEM), Stochastic Energy Analysis, or a combination of both.', 'verdict': '1'}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Life in vacuum', 'Low friction', 'Moist air running', 'Sputtered MoS 2 lubricated', 'PFPE oils', 'Hybrid lubrication', 'Tribometer experimental campaign']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of the recent testing campaigns at ESTL regarding hybrid lubrication of PFPE fluids and sputtered MoS2?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the purpose of recent testing campaigns at ESTL concerning hybrid lubrication of PFPE fluids and sputtered MoS2. It is clear in specifying the topic of interest (testing campaigns, hybrid lubrication, PFPE fluids, sputtered MoS2) and seeks information on the purpose of these campaigns. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The purpose of the recent testing campaigns at ESTL regarding hybrid lubrication of PFPE fluids and sputtered MoS2 is to understand and characterize the potential of hybrid lubrication with respect to the advantages stated above, performed at both tribometer and component level.', 'verdict': '1'}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Preload', 'Torque/preload uncertainty factor', 'Bolted joint analysis', 'Preload loss', 'Relaxed preload PDF']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How is preload loss accounted for in bolted joint analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how preload loss is accounted for in bolted joint analysis. It is specific and clear in its intent, seeking information on a particular aspect of bolted joint analysis (preload loss). The question is self-contained and does not rely on external references or additional context to be understood and answered. Therefore, it meets the criteria for independence and clear intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: How is preload loss accounted for in bolted joint analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for the derivation of the total uncertainty factor (UF) for preload in bolted joint analysis. It specifies the components involved (torque/preload UF and preload loss over time), making the intent clear and the question self-contained. No additional context or external references are needed to understand or answer the question, assuming the responder has domain knowledge in bolted joint analysis.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: How is the total uncertainty factor (UF) for preload in bolted joint analysis derived, taking into account both torque/preload UF and preload loss over time?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks specifically about accounting for preload loss in bolted joint analysis, while the second question asks about deriving the total uncertainty factor (UF) for preload, which includes considerations for torque/preload UF and preload loss over time. The second question has a broader scope and requires a more detailed analysis.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'A total UF, accounting for the scatter in preload from both the initial torque-up of a bolt and relaxation over time, is derived by taking the root sum square (RSS) of the UF values for the initial preload PDF and preload loss PDF.', 'verdict': '1'}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Microvibration', 'System-level', 'Transmission path', 'Performance requirements', 'Microvibration verification', 'Intermittent duty cycle', 'Verification methodologies', 'Verification by test and analysis', 'Microvibration verification', 'Mechanism-level analysis']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the appropriate methodologies for verifying microvibration?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for appropriate methodologies for verifying microvibration. It is clear in its intent, seeking specific methodologies related to the verification of microvibration. The question is independent and does not rely on external references or additional context to be understood. However, it could benefit from specifying the context or application (e.g., in a particular industry or type of equipment) to narrow down the methodologies relevant to the query.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What are the appropriate methodologies for verifying microvibration?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the circumstances in which verification by test, analysis, or a combination of both may be more suitable. It is clear in its intent, seeking information on the conditions or scenarios that favor one verification method over the other or a combination of both. The question is self-contained and does not rely on external references or prior knowledge not included within the question itself. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: What are the different circumstances in which verification by test and analysis or a combination of both approaches may be more suitable?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for methodologies to verify microvibration, while the second question asks about the appropriateness of test and analysis verification or a combination of both in various situations. These questions differ in both constraints and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Verification by test and analysis or a combination of both approaches may be more suitable in different circumstances.', 'verdict': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Check if testset.csv exists, use, or generate the synthetic dataset.\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=5\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=False,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=False)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv(f\"testset_{db['index_name']}.csv\", index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv(f\"testset_{db['index_name']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the transmission path of microvibration...</td>\n",
       "      <td>[System-Level Aspects and Microvibration Requi...</td>\n",
       "      <td>System-level microvibration assessments invest...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page': '[213, 214]', 'source': '['AMS_2018_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the recent testing camp...</td>\n",
       "      <td>[n ee a2 GF @ Life in vacuum (millions ofrevs....</td>\n",
       "      <td>The purpose of the recent testing campaigns at...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page': '[170, 171]', 'source': '['AMS_2018_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In what situations is test and analysis verifi...</td>\n",
       "      <td>[System-Level Aspects and Microvibration Requi...</td>\n",
       "      <td>Verification by test and analysis or a combina...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'page': '[213, 214]', 'source': '['AMS_2018_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What challenges are there in verifying microvi...</td>\n",
       "      <td>[System-Level Aspects and Microvibration Requi...</td>\n",
       "      <td>The challenges in verifying microvibration per...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'page': '[213, 214]', 'source': '['AMS_2018_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How is the total UF for preload in bolted join...</td>\n",
       "      <td>[Preloddyom  (1 — %LOSSaverage  ) Prelo  adye ...</td>\n",
       "      <td>A total UF, accounting for the scatter in prel...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'page': '[227, 228]', 'source': '['AMS_2018_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How is the transmission path of microvibration...   \n",
       "1  What is the purpose of the recent testing camp...   \n",
       "2  In what situations is test and analysis verifi...   \n",
       "3  What challenges are there in verifying microvi...   \n",
       "4  How is the total UF for preload in bolted join...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [System-Level Aspects and Microvibration Requi...   \n",
       "1  [n ee a2 GF @ Life in vacuum (millions ofrevs....   \n",
       "2  [System-Level Aspects and Microvibration Requi...   \n",
       "3  [System-Level Aspects and Microvibration Requi...   \n",
       "4  [Preloddyom  (1 — %LOSSaverage  ) Prelo  adye ...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  System-level microvibration assessments invest...         simple   \n",
       "1  The purpose of the recent testing campaigns at...         simple   \n",
       "2  Verification by test and analysis or a combina...      reasoning   \n",
       "3  The challenges in verifying microvibration per...  multi_context   \n",
       "4  A total UF, accounting for the scatter in prel...      reasoning   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'page': '[213, 214]', 'source': '['AMS_2018_...          True  \n",
       "1  [{'page': '[170, 171]', 'source': '['AMS_2018_...          True  \n",
       "2  [{'page': '[213, 214]', 'source': '['AMS_2018_...          True  \n",
       "3  [{'page': '[213, 214]', 'source': '['AMS_2018_...          True  \n",
       "4  [{'page': '[227, 228]', 'source': '['AMS_2018_...          True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>question_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question 0</td>\n",
       "      <td>How is the transmission path of microvibration...</td>\n",
       "      <td>System-level microvibration assessments invest...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question 1</td>\n",
       "      <td>What is the purpose of the recent testing camp...</td>\n",
       "      <td>The purpose of the recent testing campaigns at...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question 2</td>\n",
       "      <td>In what situations is test and analysis verifi...</td>\n",
       "      <td>Verification by test and analysis or a combina...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question 3</td>\n",
       "      <td>What challenges are there in verifying microvi...</td>\n",
       "      <td>The challenges in verifying microvibration per...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 4</td>\n",
       "      <td>How is the total UF for preload in bolted join...</td>\n",
       "      <td>A total UF, accounting for the scatter in prel...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           question  \\\n",
       "0  Question 0  How is the transmission path of microvibration...   \n",
       "1  Question 1  What is the purpose of the recent testing camp...   \n",
       "2  Question 2  In what situations is test and analysis verifi...   \n",
       "3  Question 3  What challenges are there in verifying microvi...   \n",
       "4  Question 4  How is the total UF for preload in bolted join...   \n",
       "\n",
       "                                        ground_truth        question_by  \n",
       "0  System-level microvibration assessments invest...  gpt-3.5-turbo-16k  \n",
       "1  The purpose of the recent testing campaigns at...  gpt-3.5-turbo-16k  \n",
       "2  Verification by test and analysis or a combina...  gpt-3.5-turbo-16k  \n",
       "3  The challenges in verifying microvibration per...  gpt-3.5-turbo-16k  \n",
       "4  A total UF, accounting for the scatter in prel...  gpt-3.5-turbo-16k  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>question_by</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question 0</td>\n",
       "      <td>How is the transmission path of microvibration...</td>\n",
       "      <td>System-level microvibration assessments invest...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question 1</td>\n",
       "      <td>What is the purpose of the recent testing camp...</td>\n",
       "      <td>The purpose of the recent testing campaigns at...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question 2</td>\n",
       "      <td>In what situations is test and analysis verifi...</td>\n",
       "      <td>Verification by test and analysis or a combina...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question 3</td>\n",
       "      <td>What challenges are there in verifying microvi...</td>\n",
       "      <td>The challenges in verifying microvibration per...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 4</td>\n",
       "      <td>How is the total UF for preload in bolted join...</td>\n",
       "      <td>A total UF, accounting for the scatter in prel...</td>\n",
       "      <td>gpt-3.5-turbo-16k</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           question  \\\n",
       "0  Question 0  How is the transmission path of microvibration...   \n",
       "1  Question 1  What is the purpose of the recent testing camp...   \n",
       "2  Question 2  In what situations is test and analysis verifi...   \n",
       "3  Question 3  What challenges are there in verifying microvi...   \n",
       "4  Question 4  How is the total UF for preload in bolted join...   \n",
       "\n",
       "                                        ground_truth        question_by  \\\n",
       "0  System-level microvibration assessments invest...  gpt-3.5-turbo-16k   \n",
       "1  The purpose of the recent testing campaigns at...  gpt-3.5-turbo-16k   \n",
       "2  Verification by test and analysis or a combina...  gpt-3.5-turbo-16k   \n",
       "3  The challenges in verifying microvibration per...  gpt-3.5-turbo-16k   \n",
       "4  A total UF, accounting for the scatter in prel...  gpt-3.5-turbo-16k   \n",
       "\n",
       "  answer source_documents  \n",
       "0   None             None  \n",
       "1   None             None  \n",
       "2   None             None  \n",
       "3   None             None  \n",
       "4   None             None  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RAG to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type='ChromaDB'\n",
    "index_name=db['index_name']\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers_rag=rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_questions_answers, df_docs)\n",
    "df_questions_answers_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa_eval, df_questions_answers_rag = eval_rag(index_name, df_questions_answers_rag):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=data_viz_prep(index_name,df_qa_eval,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-ams-2merge-none',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection to use\n",
    "db=dbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone_client.Index(db['index_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "chunk_size=200  # Tune to whatever doesn't error out\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcdocs_pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs_pinecone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "for db in dbs:\n",
    "    df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
