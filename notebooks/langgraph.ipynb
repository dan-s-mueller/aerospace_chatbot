{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test partitioning update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.processing import DocumentProcessor\n",
    "from aerospace_chatbot.services import EmbeddingService, LLMService, DatabaseService, prompts\n",
    "from aerospace_chatbot.processing import QAModel\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "test_index={}\n",
    "# test_index['db_type']='ChromaDB'\n",
    "test_index['db_type']='Pinecone'\n",
    "test_index['embedding_service']='OpenAI'\n",
    "test_index['embedding_model']='text-embedding-3-large'\n",
    "test_index['llm_service']='OpenAI'\n",
    "test_index['llm_model']='gpt-4o'\n",
    "\n",
    "# setup_fixture={}\n",
    "# setup_fixture['chunk_method']='character_recursive'\n",
    "chunk_size=400\n",
    "chunk_overlap=0\n",
    "batch_size=50\n",
    "\n",
    "index_name = 'text-embedding-3-large-test'\n",
    "rag_type = 'Standard'\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "# Set LOCAL_DB_PATH environment variable\n",
    "# os.environ['LOCAL_DB_PATH'] = os.path.abspath('.')\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "test_prompt='How does a thermal knife function in a cable based hold down release mechanism?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize services\n",
    "embedding_service = EmbeddingService(\n",
    "    model_service=test_index['embedding_service'],\n",
    "    model=test_index['embedding_model']\n",
    ")\n",
    "\n",
    "llm_service = LLMService(\n",
    "    model_service=test_index['llm_service'],\n",
    "    model=test_index['llm_model'],\n",
    ")\n",
    "\n",
    "doc_processor = DocumentProcessor(\n",
    "    embedding_service=embedding_service,\n",
    "    rag_type=rag_type,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Initialize database service\n",
    "db_service = DatabaseService(\n",
    "    db_type=test_index['db_type'],\n",
    "    index_name=index_name,\n",
    "    rag_type=rag_type,\n",
    "    embedding_service=embedding_service,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Number of PDFs found: 2\n",
      "INFO:aerospace_chatbot.processing.documents:PDFs found: ['gs://processing-pdfs/1999_christiansen_reocr.pdf', 'gs://processing-pdfs/1999_cremers_reocr.pdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gs://processing-pdfs/1999_christiansen_reocr.pdf',\n",
       " 'gs://processing-pdfs/1999_cremers_reocr.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = 'processing-pdfs'\n",
    "docs = DocumentProcessor.list_bucket_pdfs(bucket_name)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioned_docs = doc_processor.load_and_partition_documents(docs,partition_by_api=False, upload_bucket=bucket_name)\n",
    "# partitioned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_obj, output_paths = doc_processor.chunk_documents(partitioned_docs)\n",
    "# chunk_obj.chunk_convert(destination_type=Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.services.database:Validating index text-embedding-3-large-test and RAG type Standard\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:aerospace_chatbot.services.database:Pinecone index text-embedding-3-large-test found, not creating. Will be initialized with existing index.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_service.initialize_database(clear=False)\n",
    "except ValueError as e:\n",
    "    print(f\"Database initialization failed: {str(e)}\")\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_service.index_data(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/processing/queries.py:47: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "qa_model = QAModel(\n",
    "    db_service=db_service,\n",
    "    llm_service=llm_service,\n",
    "    k=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_model.query(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.result[-1]['references'])\n",
    "# print(qa_model.sources[-1])\n",
    "# print(qa_model.scores[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above section first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, RemoveMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing_extensions import List\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationInfo\n",
    "import re\n",
    "\n",
    "from aerospace_chatbot.services.prompts import CHATBOT_SYSTEM_PROMPT, QA_PROMPT, SUMMARIZE_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_service.retriever\n",
    "llm = llm_service.get_llm()\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InLineCitationsResponse(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the response with in-line citations.\")\n",
    "    citations: List[str] = Field(description=\"List of extracted source IDs from the response.\")\n",
    "\n",
    "    # Validator to ensure citations follow the <source id=\"#\"> format\n",
    "    @field_validator('content')\n",
    "    def validate_citations(cls, v):\n",
    "        # Regex pattern to match <source id=\"1\">, <source id=\"2\">, etc.\n",
    "        pattern = r'<source id=\"(\\d+)\">'\n",
    "        matches = re.findall(pattern, v)\n",
    "        \n",
    "        # Raise error if no citations are found or formatting is incorrect\n",
    "        if not matches:\n",
    "            raise ValueError('No valid source tags found. Expected format: <source id=\"1\">')\n",
    "\n",
    "        return v\n",
    "\n",
    "    # Validator to extract and populate citations from content\n",
    "    @field_validator('citations', mode='before')\n",
    "    def extract_citations(cls, v, info: ValidationInfo):\n",
    "        # Access content field from the model\n",
    "        content = info.data.get('content', '')\n",
    "        pattern = r'<source id=\"(\\d+)\">'\n",
    "        extracted = re.findall(pattern, content)\n",
    "        \n",
    "        if not extracted:\n",
    "            raise ValueError(\"No citations found in the content. Ensure sources are cited correctly.\")\n",
    "        \n",
    "        return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\nThe actuator was tested under high pressure <source id=\"1\">. \\nMaterial properties were measured over 50 cycles <source id=\"2\">.\\nThermal resistance improved by 30% <source id=\"3\">.\\n' citations=['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "valid_response = InLineCitationsResponse(content=\"\"\"\n",
    "The actuator was tested under high pressure <source id=\"1\">. \n",
    "Material properties were measured over 50 cycles <source id=\"2\">.\n",
    "Thermal resistance improved by 30% <source id=\"3\">.\n",
    "\"\"\",\n",
    "citations=[\"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    invalid_response = InLineCitationsResponse(content=\"\"\"\n",
    "    The actuator was tested under high pressure [1]. \n",
    "    Material properties were measured under load <source id=\"x\">.\n",
    "    \"\"\",\n",
    "    citations=[\"1\", \"x\"]\n",
    "    )\n",
    "except:\n",
    "    print(f\"Validation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(test_prompt)\n",
    "\n",
    "# Add context to the prompt\n",
    "# TODO update this to use the doc.id. Do some testing to check this works.\n",
    "docs_content=\"\"\n",
    "for i, doc in enumerate(retrieved_docs[0]):\n",
    "    # Source IDs in the order they show in in the array. Indexed from 0.\n",
    "    docs_content += f\"Source ID: {i}\\n{doc.page_content}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output parser with the expected Pydantic model\n",
    "output_parser = PydanticOutputParser(pydantic_object=InLineCitationsResponse)\n",
    "\n",
    "CHATBOT_SYSTEM_PROMPT=SystemMessage(content=\n",
    "\"\"\"\n",
    "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering.\n",
    "\n",
    "Use only the **Sources and Context** from the **Reference Documents** provided to answer the **User Question**. Do not use outside knowledge, and strictly follow these rules:\n",
    "\n",
    "---\n",
    "\n",
    "### **Rules**:\n",
    "1. **Answer only based on the provided Sources and Context.**  \n",
    "   - If the information is not available in the Sources and Context, respond with:  \n",
    "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*\n",
    "\n",
    "2. **Do not make up or infer answers.**\n",
    "\n",
    "3. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
    "\n",
    "4. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
    "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
    "   - **The `source` tag must be present for every source referenced in the response.**  \n",
    "   - **Do not add, omit, or modify any part of the citation format.**  \n",
    "   \n",
    "   **Examples (Correct):**  \n",
    "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
    "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
    "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
    "\n",
    "   **Examples (Incorrect – Must Be Rejected):**  \n",
    "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
    "   > <source id=\"1\" > (Extra space after `id`)  \n",
    "   > <source id=\"a\"> (Non-numeric ID)  \n",
    "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
    "\n",
    "5. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
    "   - **Do not group multiple sources into a single tag.** Each source must have its own, clearly separated citation.  \n",
    "   - For example:  \n",
    "     > The actuator uses a reinforced composite structure <source id=\"1\">. This design was validated through multiple tests <source id=\"2\">.  \n",
    "\n",
    "6. **Validation Requirement:**  \n",
    "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
    "   - Every source used must have a corresponding citation in the response. **No source should be referenced without explicit citation.**  \n",
    "\n",
    "7. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
    "\"\"\")\n",
    "\n",
    "QA_PROMPT_TEST=PromptTemplate(\n",
    "    template=\n",
    "\"\"\"\n",
    "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering.\n",
    "\n",
    "Use only the **Sources and Context** from the **Reference Documents** provided to answer the **User Question**. Do not use outside knowledge, and strictly follow these rules:\n",
    "\n",
    "---\n",
    "\n",
    "### **Rules**:\n",
    "1. **Answer only based on the provided Sources and Context.**  \n",
    "   - If the information is not available in the Sources and Context, respond with:  \n",
    "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*\n",
    "\n",
    "2. **Do not make up or infer answers.**\n",
    "\n",
    "3. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
    "\n",
    "4. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
    "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
    "   - **The `source` tag must be present for every source referenced in the response.**  \n",
    "   - **Do not add, omit, or modify any part of the citation format.**  \n",
    "   \n",
    "   **Examples (Correct):**  \n",
    "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
    "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
    "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
    "\n",
    "   **Examples (Incorrect – Must Be Rejected):**  \n",
    "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
    "   > <source id=\"1\" > (Extra space after `id`)  \n",
    "   > <source id=\"a\"> (Non-numeric ID)  \n",
    "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
    "\n",
    "5. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
    "   - **Do not group multiple sources into a single tag.** Each source must have its own, clearly separated citation.  \n",
    "   - For example:  \n",
    "     > The actuator uses a reinforced composite structure <source id=\"1\">. This design was validated through multiple tests <source id=\"2\">.  \n",
    "\n",
    "6. **Validation Requirement:**  \n",
    "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
    "   - Every source used must have a corresponding citation in the response. **No source should be referenced without explicit citation.**  \n",
    "\n",
    "7. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
    "\n",
    "---\n",
    "**Sources and Context from Reference Documents**:\n",
    "{context}\n",
    "---\n",
    "\n",
    "---\n",
    "**User Question**:\n",
    "{question}\n",
    "---\n",
    "\n",
    "---\n",
    "{format_instructions}\n",
    "---\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "QA_PROMPT=HumanMessagePromptTemplate.from_template(\n",
    "    template=\n",
    "\"\"\"\n",
    "---\n",
    "**Sources and Context from Reference Documents**:\n",
    "{context}\n",
    "---\n",
    "\n",
    "---\n",
    "**User Question**:\n",
    "{question}\n",
    "---\n",
    "\n",
    "---\n",
    "{format_instructions}\n",
    "---\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"content\": \"The thermal knife in a cable-based hold down release mechanism functions by using a heater plate that is pushed through a wire bundle by a compression spring. This action cuts the Dyneema wire bundle, which is responsible for clamping the two parts of the Reel cable element together. Once the wire bundle is cut, the upper part of the Reel is released, allowing it to deploy along with the application. Both the prime and redundant thermal knives operate along the same center line, ensuring that their heater plates make contact at their cutting edges after cutting the wire bundle. To ensure head-on contact, each thermal knife is attached to the holddown bracket at an angle of approximately 8° <source id=\\\\\"1\\\\\"><source id=\\\\\"2\\\\\"><source id=\\\\\"4\\\\\">.\",\\n  \"citations\": [\\n    \"1\",\\n    \"2\",\\n    \"4\"\\n  ]\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 1379, 'total_tokens': 1564, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None} id='run-e8ae8e78-4fa4-48e3-aa3b-3610c98650af-0' usage_metadata={'input_tokens': 1379, 'output_tokens': 185, 'total_tokens': 1564, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "prompt = QA_PROMPT_TEST.format(context=docs_content, question=test_prompt)\n",
    "raw_output = llm(prompt)\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The thermal knife in a cable-based hold down release mechanism functions by using a heater plate that is pushed through a wire bundle by a compression spring. This action cuts the Dyneema wire bundle, which is responsible for clamping the two parts of the Reel cable element together. Once the wire bundle is cut, the upper part of the Reel is released, allowing it to deploy along with the application. Both the prime and redundant thermal knives operate along the same center line, ensuring that their heater plates make contact at their cutting edges after cutting the wire bundle. To ensure head-on contact, each thermal knife is attached to the holddown bracket at an angle of approximately 8° <source id=\"1\"><source id=\"2\"><source id=\"4\">.' citations=['1', '2', '4']\n"
     ]
    }
   ],
   "source": [
    "parsed_response = output_parser.parse(raw_output.content)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    context: List[Document]\n",
    "    cited_sources: List[str]\n",
    "    summary: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Retrieve the documents from the database.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: retrieve\")\n",
    "\n",
    "    retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n",
    "    \n",
    "    # TODO: Grow context with messages, include cited sources\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate_w_context(state: State):\n",
    "    \"\"\"\n",
    "    Call the model with the prompt with context.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: generate_w_context\")\n",
    "\n",
    "    # Get the summary\n",
    "    # TODO add conversation modes\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [CHATBOT_SYSTEM_PROMPT] + [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "        # logger.info(f\"Messages with system prompt (w/summary): {messages}\")\n",
    "    else:\n",
    "        messages = [CHATBOT_SYSTEM_PROMPT] + state[\"messages\"]\n",
    "        # logger.info(f\"Messages with system prompt (w/o summary): {messages}\")\n",
    "\n",
    "    # Add context to the prompt\n",
    "    docs_content=\"\"\n",
    "    for i, doc in enumerate(state[\"context\"]):\n",
    "        # Source IDs in the order they show in in the array. Indexed from 0.\n",
    "        docs_content += f\"Source ID: {i}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "    # Prompt with context and pydantic output parser\n",
    "    prompt_with_context = QA_PROMPT.format(\n",
    "        context=docs_content,\n",
    "        question=state[\"messages\"][-1].content, \n",
    "    )\n",
    "    # Replace the last message (user question) with the prompt with context, return LLM response\n",
    "    messages[-1] = prompt_with_context \n",
    "    response = llm.invoke(messages)\n",
    "    logger.info(f\"Response from LLM: {response}\")\n",
    "\n",
    "    # Parse the response. This will return a InLineCitationsResponse object. This object has two fields: content and citations.\n",
    "    # For now, replace the last message with the content of the parsed and validated response. \n",
    "    # Add the citations to the state, only for the last message.\n",
    "    parsed_response = output_parser.parse(response.content)\n",
    "    logger.info(f\"Parsed response: {parsed_response}\")\n",
    "    response.content = parsed_response.content\n",
    "    logger.info(f\"Response after parsing: {response}\")\n",
    "\n",
    "    # Update the state messages with the messages updated in this node.\n",
    "    state[\"messages\"] = messages\n",
    "    logger.info(f\"Messages after generate_w_context: {state['messages']}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"\n",
    "    Define the logic for determining whether to end or summarize the conversation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: should_continue\")\n",
    "\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 6:\n",
    "        logger.info(f\"Summarizing conversation\")\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise just end\n",
    "    logger.info(f\"Ending conversation\")\n",
    "    # logger.info(f\"Messages before ending: {messages}\")\n",
    "    return END\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \"\"\"\n",
    "    Summarize the conversation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: summarize_conversation\")\n",
    "\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        # If a summary already exists, extend it\n",
    "        summary_message = SUMMARIZE_TEXT.format(\n",
    "            summary=summary,\n",
    "            augment=\"Extend the summary by taking into account the new messages above.\"\n",
    "        )\n",
    "    else:\n",
    "        # If no summary exists, create one\n",
    "        summary_text=\"\"\"---\\n**Conversation Summary to Date**:\\n{summary}\\n---\"\"\"\n",
    "        summary_message = SUMMARIZE_TEXT.format(\n",
    "            summary=summary_text,\n",
    "            augment=\"Create a summary of the conversation above.\"\n",
    "        )\n",
    "\n",
    "    messages = state[\"messages\"] + [summary_message]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Prune messages. This deletes all but the last two messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the conversation node and the summarize node\n",
    "workflow.add_node(\"retrieve\", retrieve) \n",
    "workflow.add_node(\"generate_w_context\", generate_w_context)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate_w_context\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # Define the start node. We use `generate_w_context`. This means these are the edges taken after the `conversation` node is called.\n",
    "    \"generate_w_context\",\n",
    "    # Next, pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `summarize_conversation` to END. This means that after `summarize_conversation` is called, we end.\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = graph.invoke({\"question\": test_prompt}, config=config)\n",
    "\n",
    "prompt = 'My name is Dan. Please tell me about some interesting mecanism designs.'\n",
    "result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How have these mecahnisms been tested?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How old are you?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some lessons learned about these mechanisms?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some problems that have occurred?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
