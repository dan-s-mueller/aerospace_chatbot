{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the same as the other stuff I wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "https://ldoeollhxasevurjamos.supabase.co\n",
      "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imxkb2VvbGxoeGFzZXZ1cmphbW9zIiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTM4MDU2MzQsImV4cCI6MjAwOTM4MTYzNH0.g3yB5WCX2agYrmlk0zH7Sbl3XoKO2dpiV_fyA14YBEo\n"
     ]
    }
   ],
   "source": [
    "# Check api keys\n",
    "print(os.getenv('OPENAI_API_KEY'))\n",
    "print(os.getenv('SUPABASE_URL'))\n",
    "print(os.getenv('SUPABASE_SERVICE_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate OpenAI embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The quivr hack stuff starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from parsers import common, pdf\n",
    "from models.files import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63fdef46-9ec1-4047-a4fe-5cd9950776b9\n"
     ]
    }
   ],
   "source": [
    "# Create a brain\n",
    "from models import brains\n",
    "from uuid import UUID, uuid4\n",
    "\n",
    "brain_id=uuid4()\n",
    "\n",
    "brain=brains.Brain(id=brain_id,\n",
    "                    name='ams_bot',\n",
    "                    description='ams expert',\n",
    "                    status='private',\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    max_tokens=256,\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    files=[],\n",
    "                    max_brain_size=52428800)\n",
    "\n",
    "print(brain.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "from models import settings\n",
    "\n",
    "brain_settings=settings.BrainSettings(anthropic_api_key='',\n",
    "                                        openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                                        supabase_url=os.getenv('SUPABASE_URL'),\n",
    "                                        supabase_service_key=os.getenv('SUPABASE_SERVICE_KEY'))\n",
    "\n",
    "# print(settings.get_supabase_client())\n",
    "# print(settings.get_supabase_db())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brain(id=None, name='Default brain', description='This is a description', status='private', model='gpt-3.5-turbo', temperature=0.0, max_tokens=256, openai_api_key=None, files=[], max_brain_size=52428800, prompt_id=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object process_file at 0x1680b1d20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to create a file object with one of the PDFs.\n",
    "# From /Users/danmueller/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/routes/crawl_routes.py\n",
    "\n",
    "# Create a SpooledTemporaryFile from the file_path\n",
    "from fastapi import APIRouter, Depends, Query, Request, UploadFile\n",
    "from tempfile import SpooledTemporaryFile\n",
    "import shutil\n",
    "\n",
    "spooled_file = SpooledTemporaryFile()\n",
    "file_path='../../../data/'\n",
    "file_name='AMS_2000.pdf'\n",
    "with open(file_path+file_name, \"rb\") as f:\n",
    "    shutil.copyfileobj(f, spooled_file)\n",
    "\n",
    "# Pass the SpooledTemporaryFile to UploadFile\n",
    "uploadFile = UploadFile(\n",
    "    file=spooled_file,  # pyright: ignore reportPrivateUsage=none\n",
    "    filename=file_name,\n",
    ")\n",
    "file = File(file=uploadFile)\n",
    "\n",
    "pdf.process_pdf(file=file,\n",
    "                    enable_summarization=False,\n",
    "                    brain_id=brain_id,\n",
    "                    user_openai_api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 22:17:45,266:INFO - HTTP Request: POST https://ldoeollhxasevurjamos.supabase.co/rest/v1/brains_users \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_51522/2318490069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbrain_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbrain_repo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_brain_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRoleEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_default_brain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# brain_repo.create_brain(brain_db_settings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/repository/brain/create_brain_user.py\u001b[0m in \u001b[0;36mcreate_brain_user\u001b[0;34m(user_id, brain_id, rights, is_default_brain)\u001b[0m\n\u001b[1;32m      9\u001b[0m ) -> None:\n\u001b[1;32m     10\u001b[0m     \u001b[0msupabase_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_supabase_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     supabase_db.create_brain_user(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbrain_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/models/databases/supabase/brains.py\u001b[0m in \u001b[0;36mcreate_brain_user\u001b[0;34m(self, user_id, brain_id, rights, default_brain)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 }\n\u001b[1;32m    146\u001b[0m             )\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/postgrest/_sync/request_builder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAPIResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_request_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: {}"
     ]
    }
   ],
   "source": [
    "from repository import brain as brain_repo\n",
    "from models.databases.supabase import brains as brains_db\n",
    "from uuid import UUID, uuid4\n",
    "from routes.authorizations.types import RoleEnum\n",
    "\n",
    "brain_db_settings=brains_db.CreateBrainProperties(name='dans_brain',\n",
    "                                                  description='dans_brain',\n",
    "                                                  status='private',\n",
    "                                                  model='gpt-3.5-turbo',\n",
    "                                                  temperature=0,\n",
    "                                                  max_tokens=256,\n",
    "                                                  openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "user_id=uuid4()\n",
    "brain_id=uuid4()\n",
    "brain_repo.create_brain_user(brain_id=user_id,user_id=user_id,rights=RoleEnum.Owner,is_default_brain=True)\n",
    "# brain_repo.create_brain(brain_db_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is also stuff which was working before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize Pinecone client\n",
    "import pinecone\n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase.client import Client\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT') \n",
    ")\n",
    "pinecone.whoami()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the existing index, clear for new start\n",
    "index_name = \"langchain-quickstart\"\n",
    "index=pinecone.Index(index_name)\n",
    "index.delete(delete_all=True) # Clear the index first, then upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import parsers\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone database: https://app.pinecone.io/organizations/-Nam3zmbSmzuXKeH8EWl/projects/us-west1-gcp-free:32467cc/indexes/langchain-quickstart\n",
    "import glob\n",
    "\n",
    "data_folder='../data/'\n",
    "docs = glob.glob(data_folder+'*.pdf')   # Only get the PDFs in the directory\n",
    "\n",
    "for doc in docs:\n",
    "    print('Parsing: '+doc)\n",
    "    loader = PyPDFLoader(doc)\n",
    "    data = loader.load_and_split()\n",
    "    \n",
    "    # This is optional, but needed to play with the data parsing.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "\n",
    "    for text in texts:\n",
    "        text.metadata['page']=text.metadata['page']+1   # Pages are 0 based, update\n",
    "    \n",
    "    print('Uploading to pinecone index '+index_name)\n",
    "    vectorstore = Pinecone.from_documents(texts, embeddings_model, index_name=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
