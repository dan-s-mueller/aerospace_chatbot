{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import eval\n",
    "import admin\n",
    "import data_processing\n",
    "import queries\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset, generate docs+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-large-0merge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them. Uncomment if desired, takes a while.\n",
    "export=False\n",
    "\n",
    "if export:\n",
    "    for collection in collections:\n",
    "        df_temp_chroma=data_processing.archive_db('ChromaDB',collection.name,collection._embedding_function,export_pickle=True)\n",
    "\n",
    "    df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "idx=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "\n",
    "docs_vectorstore=collections[idx]\n",
    "query_model=query_models[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format docs into dataframe\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [data_processing._stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-2merge-0',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)},\n",
    "     {'index_name':'voyage-large-2-instruct-0merge-400',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "idx=0\n",
    "db=dbs[0]\n",
    "index = pinecone_client.Index(db['index_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)\n",
    "\n",
    "docs=[]\n",
    "df_docs = pd.DataFrame()\n",
    "chunk_size=200  # Tune to whatever doesn't error out, 200 won't for serverless\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)\n",
    "\n",
    "    df_doc_temp = pd.DataFrame()\n",
    "    df_doc_temp[\"id\"]= [vector_elm[\"id\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"source\"]= [vector_elm[\"metadata\"][\"source\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"page\"]= [vector_elm[\"metadata\"][\"page\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"document\"]= [vector_elm[\"metadata\"][\"page_content\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"embedding\"]= [vector_elm[\"values\"] for vector_elm in vector_data]\n",
    "    df_docs = pd.concat([df_docs, df_doc_temp])\n",
    "\n",
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))\n",
    "    \n",
    "print(len(lcdocs_pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "# for db in dbs:\n",
    "#     df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': True, 'message': ['colbert-ir-colbertv2.0-2merge-0']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes=admin.show_ragatouille_indexes(format=False)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=['colbert-ir/colbertv2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "db=dbs[idx]\n",
    "\n",
    "# TODO get this as a langchain retriever, pull docs\n",
    "query_model = RAGPretrainedModel.from_pretrained(db,index_root=os.path.join(os.getenv('LOCAL_DB_PATH'),'.ragatouille'))\n",
    "docs_vectorstore=data_processing.initialize_database('RAGatouille',\n",
    "                                                     'colbert-ir-colbertv2.0-2merge-0',\n",
    "                                                     query_model,\n",
    "                                                     'Standard',\n",
    "                                                     os.getenv('LOCAL_DB_PATH'),\n",
    "                                                     init_ragatouille=False,\n",
    "                                                     clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA/CP—2008–215252 39th Aerospace Mechanisms Symposium Compiled by  E.A. Boesiger Lockheed Martin Space Systems Company, Sunnyvale, California May 2008Proceedings of a Symposium held at the V on Braun Center, Huntsville, Alabama,  Hosted by Marshall Space Flight Center  and Lockheed Martin Space Systems Company, Organized by the Mechanisms Education Association May 7–9, 2008 The NASA STI Program…in Proﬁle  Since its founding, NASA has been dedicated   to the advancement of aeronautics and space  science. The NASA Scientiﬁc and Technical  Information (STI) Program Ofﬁce plays a key   part in helping NASA maintain this important role.  The NASA STI program operates under the  auspices of the Agency Chief Information Ofﬁcer.  It collects, organizes, provides for archiving, and  disseminates NASA’s STI.\n",
      "15090\n",
      "2222\n",
      "15090\n"
     ]
    }
   ],
   "source": [
    "docs=docs_vectorstore.model.collection  # Document chunks (chunked smaller according to token size)\n",
    "print(docs[0])\n",
    "print(len(docs))\n",
    "\n",
    "metadata=docs_vectorstore.model.docid_metadata_map  # Document metadata for original documents\n",
    "print(len(metadata))\n",
    "\n",
    "map=docs_vectorstore.model.pid_docid_map    # Map of document chunks to original document\n",
    "print(len(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't quite figure out functionality to export the encodings from RAGatouille for each document. Won't pursue unless RAGatouille has exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "n_questions=10   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}.csv\")\n",
    "lcdocs=lcdocs_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset=eval.generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-large-2merge-0'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "df_testset=df_testset.head(4)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "# setup_data['eval_models'] = setup_data['eval_models'][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_models': [{'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-0merge-400-parent-child',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Parent-Child',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'Pinecone',\n",
       "   'index_name': 'voyage-large-2-instruct-2merge-0',\n",
       "   'query_model': {'query_model': 'Voyage',\n",
       "    'embedding_name': 'voyage-large-2-instruct'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'RAGatouille',\n",
       "   'index_name': 'colbert-ir-colbertv2.0-2merge-0',\n",
       "   'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'k': 4,\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent-child not functioning properly. It is just returning the document chunks of the child, not the parent.\n",
    "\n",
    "debug_model=setup_data['eval_models'][2]\n",
    "\n",
    "index_type=debug_model['index_type']\n",
    "sb['index_type']=index_type\n",
    "index_name=debug_model['index_name']\n",
    "sb['index_name']=index_name\n",
    "# Query model and llm\n",
    "for key in debug_model['query_model']:\n",
    "    sb[key] = debug_model['query_model'][key]\n",
    "query_model=admin.get_query_model(sb, secrets)\n",
    "for key in debug_model['llm']:\n",
    "    sb[key] = debug_model['llm'][key]\n",
    "llm=admin.set_llm(sb, secrets)\n",
    "# QA model params\n",
    "qa_model_params=debug_model['qa_model_params']\n",
    "\n",
    " # Use the QA model to query the documents\n",
    "qa_obj=queries.QA_Model(index_type,\n",
    "                        index_name,\n",
    "                        query_model,\n",
    "                        llm,\n",
    "                        **qa_model_params)\n",
    "qa_obj.query_docs(\"Can you describe the failure modes of a latch?\")\n",
    "response=qa_obj.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<queries.QA_Model at 0x36ad65550>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-0merge-400-parent-child', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Parent-Child', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "Processing question 1/4\n",
      "Processing question 2/4\n",
      "Processing question 3/4\n",
      "Processing question 4/4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# QA model params\u001b[39;00m\n\u001b[1;32m     21\u001b[0m qa_model_params\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqa_model_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m df_qa_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdf_qa_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df_qa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_qa,df_qa_iter],ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# After each iteration, export a pickle of the dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/../src/aerospace_chatbot/eval.py:224\u001b[0m, in \u001b[0;36mrag_responses\u001b[0;34m(index_type, index_name, query_model, llm, QA_model_params, df_qa, df_docs, testset_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     context\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m cell_list:\n\u001b[0;32m--> 224\u001b[0m         context\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdf_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    225\u001b[0m     source_documents_list\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m    226\u001b[0m df_qa_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39msource_documents_list\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Iterate through the evaluation models and databases, dump data as you go.\n",
    "# This will cache data along the way into rag_responses, so you can pick up where you left off.\n",
    "# A pickled dataframe is also exported at the end of each iteration, but not row-by-row.\n",
    "df_qa = pd.DataFrame()\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type=model['index_type']\n",
    "    sb['index_type']=index_type\n",
    "    index_name=model['index_name']\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{index_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa = eval.eval_rag(index_name, df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=False\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
