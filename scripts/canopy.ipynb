{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document processing (a few ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "import re\n",
    "import pinecone\n",
    "import glob\n",
    "import json, jsonlines\n",
    "import uuid\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from canopy.tokenizer import Tokenizer\n",
    "from canopy.knowledge_base import KnowledgeBase\n",
    "from canopy.models.data_models import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check api keys\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# print(os.getenv('PINECONE_ENVIRONMENT'))\n",
    "# print(os.getenv('PINECONE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: ../data/AMS\\AMS_2018.pdf\n",
      "Processed: ../data/AMS\\AMS_2020.pdf\n",
      "Processed: ../data/AMS\\AMS_2022.pdf\n"
     ]
    }
   ],
   "source": [
    "# data_folder='../data/FEA/'\n",
    "data_folder='../data/AMS/'\n",
    "docs = glob.glob(data_folder+'*.pdf')   # Only get the PDFs in the directory\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "# print(docs[-1])\n",
    "# loader=PyPDFLoader(docs[-1])\n",
    "# doc_pages=loader.load_and_split(text_splitter)\n",
    "\n",
    "pages=[]\n",
    "for doc in docs[-3:]:\n",
    "    loader=PyPDFLoader(doc)\n",
    "    doc_pages=loader.load_and_split(text_splitter)\n",
    "    pages.extend(doc_pages)\n",
    "    print('Processed: '+doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy text up\n",
    "pages_dict=[]\n",
    "docs_canopy=[]\n",
    "for page in pages:\n",
    "    page.metadata['source']=os.path.basename(page.metadata['source'])   # Strip path\n",
    "    page.metadata['page']=int(page.metadata['page'])+1   # Pages are 0 based, update\n",
    "    # Merge hyphenated words\n",
    "    page.page_content=re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", page.page_content)\n",
    "    # Fix newlines in the middle of sentences\n",
    "    page.page_content = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", page.page_content.strip())\n",
    "    # Remove multiple newlines\n",
    "    page.page_content = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", page.page_content)\n",
    "\n",
    "    # Format into canopy format\n",
    "    dict_temp=dict(page)\n",
    "    dict_temp_out={'id':page.metadata['source']+\"_\"+str(page.metadata['page'])+str(uuid.uuid4()),\n",
    "                   'text':dict_temp.pop('page_content'),\n",
    "                   'source':dict_temp['metadata']['source'],\n",
    "                   'metadata':dict_temp['metadata']}\n",
    "    dict_temp_out['metadata']['page']=str(dict_temp_out['metadata']['page'])\n",
    "    pages_dict.append(dict_temp_out)\n",
    "\n",
    "\n",
    "    doc_temp=Document(id=page.metadata['source']+\"_\"+str(page.metadata['page'])+str(uuid.uuid4()),\n",
    "                        text=page.page_content,\n",
    "                        source=page.metadata['source'],\n",
    "                        metadata={'page':str(page.metadata['page'])})\n",
    "    docs_canopy.append(doc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='44th Aerospace Mechanisms SymposiumNASA/CP—2018-219887 May 2018Edward A. Boesiger, Compiler Lockheed Martin Space Systems Company, Sunnyvale, California National Aeronautics and Space Administration Glenn Research Center Cleveland, Ohio 44135Proceedings of a conference held at Hilton Cleveland Downtown Hosted by NASA Glenn Research Center and Lockheed Martin Space Systems CompanySponsored and organized by Mechanisms Education AssociationCleveland, OhioMay 16–18, 2018' metadata={'source': 'AMS_2018.pdf', 'page': '5'}\n"
     ]
    }
   ],
   "source": [
    "print(pages[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'AMS_2018.pdf_1bfb03e76-1d88-4ded-a9c1-0f891f878ba1', 'text': '44th Aerospace Mechanisms SymposiumNASA/CP—2018-219887 May 2018Edward A. Boesiger, Compiler Lockheed Martin Space Systems Company, Sunnyvale, California', 'source': 'AMS_2018.pdf', 'metadata': {'source': 'AMS_2018.pdf', 'page': '1'}}\n"
     ]
    }
   ],
   "source": [
    "print(pages_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='AMS_2018.pdf_6b85047d9-8331-4c41-a99f-d6c0122c807d' text='44th Aerospace Mechanisms SymposiumNASA/CP—2018-219887 May 2018Edward A. Boesiger, Compiler Lockheed Martin Space Systems Company, Sunnyvale, California' source='AMS_2018.pdf' metadata={'page': '6'}\n"
     ]
    }
   ],
   "source": [
    "print(docs_canopy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(data_folder+'ams_data.jsonl', mode='w') as writer:\n",
    "    writer.write_all(pages_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upserting docs for canopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command line interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can switch across to Canopy CLI (or other method) and run:\n",
    "\n",
    "```\n",
    "canopy\n",
    "canopy upsert ./ai_arxiv.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canopy library for upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer.initialize()\n",
    "\n",
    "index_name='canopy--ams'\n",
    "kb = KnowledgeBase(index_name=index_name)\n",
    "kb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [08:55<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for i in tqdm(range(0, len(pages_dict), batch_size)):\n",
    "    kb.upsert(docs_canopy[i: i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we begin chatting by first starting the Canopy Server:\n",
    "\n",
    "```\n",
    "canopy start\n",
    "```\n",
    "\n",
    "Then begin chatting with:\n",
    "\n",
    "```\n",
    "canopy chat\n",
    "```\n",
    "\n",
    "_(we can also add the `--no-rag` flag to see how our RAG vs. non-RAG results compare!)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
