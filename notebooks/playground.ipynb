{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test bug on RAGxplorer on query\n",
    "Without brackets around the query in the openai client with chroma, embeddings are created for each word, not each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ragxplorer import RAGxplorer\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "import json\n",
    "import chromadb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What are examples of lubricants which should be avoided for space mechanism applications?\"\n",
    "\n",
    "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
    "embedding_model='text-embedding-ada-002'\n",
    "\n",
    "data_path='../data/AMS/'\n",
    "pdf='AMS_2022.pdf'\n",
    "\n",
    "index_path='../db/chromadb/'\n",
    "index_name='chromadb-openai-ams'\n",
    "\n",
    "viz_data='../data/AMS/ams_data-400-0-50.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(viz_data, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "index_name=data['visualization_index_name']\n",
    "umap_params=data['umap_params']\n",
    "viz_data=pd.read_json(data['viz_data'], orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client = RAGxplorer(embedding_model=embedding_model)\n",
    "chroma_client = chromadb.PersistentClient(path='../db'+'/chromadb/')\n",
    "collection=chroma_client.get_collection(name=index_name,embedding_function=rx_client._chosen_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client._query.original_query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client.load_chroma(collection,\n",
    "                     umap_params=umap_params,\n",
    "                     initialize_projector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of these do the same thing, tested\n",
    "\n",
    "query_embedding_chroma=rx_client._chosen_embedding_model([rx_client._query.original_query])\n",
    "# query_embedding=rx_client._vectordb._embedding_function(rx_client._query.original_query)\n",
    "# query_embedding=embedding_function(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rx_client._query.original_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = rx_client.visualize_query(query,\n",
    "                                import_projection_data=viz_data,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Done correctly, see here: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "openai_embedding=client.embeddings.create(input = [query], model=embedding_model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_file = '../config/config.json'\n",
    "with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        databases = {db['name']: db for db in config['databases']}\n",
    "        llms  = {m['name']: m for m in config['llms']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_models=llms['Hugging Face']['models']\n",
    "model_names= [item['model'] for item in hf_models]\n",
    "model_endpoints= [item['endpoint'] for item in hf_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "endpoint = next(item['endpoint'] for item in hf_models if item['model'] == model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "# from langchain_community.llms import HuggingFaceTextGenInference\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#                     model_name=\"tgi\",\n",
    "#                     openai_api_key=api_key,\n",
    "#                     openai_api_base=endpoint + \"/v1/\",\n",
    "#                 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = HuggingFaceTextGenInference(\n",
    "#     inference_server_url=endpoint,\n",
    "#     # max_new_tokens=512,\n",
    "#     # top_k=10,\n",
    "#     # top_p=0.95,\n",
    "#     # typical_p=0.95,\n",
    "#     temperature=0.01,\n",
    "#     # repetition_penalty=1.03,\n",
    "# )\n",
    "# llm(\"What did foo say about bar?\")\n",
    "\n",
    "# model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name,\n",
    "                     model_kwargs={\"temperature\": 0.1, \"max_length\": 516})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are examples of lubricants which should be avoided for space mechanism applications?\n",
      "\n",
      "Sources:\n",
      "\n",
      "User Question: What are examples of lubricants which should be avoided for space mechanism applications?\n",
      "Standalone Question:\n",
      "\n",
      "What are examples of lubricants which should be avoided for space mechanism applications?\n",
      "\n",
      "Sources:\n",
      "\n",
      "User Question: What are examples of lubricants which should be avoided for space mechanism applications?\n",
      "Standalone Question:\n",
      "\n",
      "What are examples\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "---\n",
    "Your name is Aerospace Chatbot. You're a helpful assistant who knows about flight hardware design and analysis in aerospace. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Include sources from the chat history in the standalone question created.\n",
    "---\n",
    "\n",
    "Chat History:\n",
    "\n",
    "User Question: What are examples of lubricants which should be avoided for space mechanism applications?\n",
    "Standalone Question:\n",
    "\"\"\"\n",
    "\n",
    "response=llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
