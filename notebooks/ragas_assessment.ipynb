{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness, faithfulness, context_recall\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import eval\n",
    "import admin\n",
    "import data_processing\n",
    "import queries\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset, generate docs+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-small-2merge-0),\n",
       " Collection(name=text-embedding-3-large-2merge-0-parent-child-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-small-2merge-0-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0-parent-child)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_models=[VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False),\n",
    "              VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes=admin.show_ragatouille_indexes(format=False)\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbs=['colbert-ir/colbertv2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=0\n",
    "# db=dbs[idx]\n",
    "\n",
    "# # TODO get this as a langchain retriever, pull docs\n",
    "# query_model = RAGPretrainedModel.from_pretrained(db,index_root=os.path.join(os.getenv('LOCAL_DB_PATH'),'.ragatouille'))\n",
    "# docs_vectorstore=data_processing.initialize_database('RAGatouille',\n",
    "#                                                      'colbert-ir-colbertv2.0-2merge-0',\n",
    "#                                                      query_model,\n",
    "#                                                      'Standard',\n",
    "#                                                      os.getenv('LOCAL_DB_PATH'),\n",
    "#                                                      init_ragatouille=False,\n",
    "#                                                      clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=docs_vectorstore.model.collection  # Document chunks (chunked smaller according to token size)\n",
    "# print(docs[0])\n",
    "# print(len(docs))\n",
    "\n",
    "# metadata=docs_vectorstore.model.docid_metadata_map  # Document metadata for original documents\n",
    "# print(len(metadata))\n",
    "\n",
    "# map=docs_vectorstore.model.pid_docid_map    # Map of document chunks to original document\n",
    "# print(len(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't quite figure out functionality to export the encodings from RAGatouille for each document. Won't pursue unless RAGatouille has exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type=\"ChromaDB\"\n",
    "# index_type=\"Pinecone\"\n",
    "\n",
    "# Select database for generating docs\n",
    "if index_type==\"ChromaDB\":\n",
    "    idx_chroma=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "    docs_vectorstore=collections[idx_chroma]\n",
    "    query_model=query_models[idx_chroma]  \n",
    "elif index_type==\"Pinecone\":\n",
    "    idx_pinecone=0\n",
    "    docs_vectorstore=indexes[idx_pinecone]\n",
    "    query_model=query_models[idx_pinecone]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "df_docs, lcdocs = eval.lcdoc_export(index_type,docs_vectorstore,query_model,export_pickle=False)\n",
    "print(len(lcdocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "# eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "eval_size=len(lcdocs)\n",
    "n_questions=30   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset=eval.generat7e_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-small-2merge-0_full'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# In case there was missing truth, drop the question\n",
    "df_testset = df_testset.dropna(subset=['ground_truth'])\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "# df_testset=df_testset.head(2)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_models': [{'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-small-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-small'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0-parent-child',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Parent-Child',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'Pinecone',\n",
       "   'index_name': 'voyage-large-2-instruct-2merge-0',\n",
       "   'query_model': {'query_model': 'Voyage',\n",
       "    'embedding_name': 'voyage-large-2-instruct'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'RAGatouille',\n",
       "   'index_name': 'colbert-ir-colbertv2.0-2merge-0',\n",
       "   'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'k': 4,\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-small-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-small'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0-parent-child', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Parent-Child', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'Hugging Face', 'llm_model': 'meta-llama/Meta-Llama-3-8B-Instruct', 'model_options': {'temperature': 0.2, 'output_level': 1000}, 'hf_endpoint': 'https://api-inference.huggingface.co/v1'}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'Hugging Face', 'llm_model': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_options': {'temperature': 0.2, 'output_level': 1000}, 'hf_endpoint': 'https://api-inference.huggingface.co/v1'}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'Pinecone', 'index_name': 'voyage-large-2-instruct-2merge-0', 'query_model': {'query_model': 'Voyage', 'embedding_name': 'voyage-large-2-instruct'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0', 'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "[Jul 19, 11:18:12] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the evaluation models and databases, dump data as you go.\n",
    "# This will cache data along the way into rag_responses, so you can pick up where you left off.\n",
    "# A pickled dataframe is also exported at the end of each iteration, but not row-by-row.\n",
    "df_qa = pd.DataFrame()\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type=model['index_type']\n",
    "    sb['index_type']=index_type\n",
    "    index_name=model['index_name']\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'add_cached_columns_from_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m eval_criterias\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_correctness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m df_qa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_criterias\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestset_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/../src/aerospace_chatbot/eval.py:312\u001b[0m, in \u001b[0;36meval_rag\u001b[0;34m(df_qa, eval_criterias, testset_name)\u001b[0m\n\u001b[1;32m    309\u001b[0m df_qa_out\u001b[38;5;241m=\u001b[39mdf_qa\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Add answer correctness column, fill in if it exists\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m df_qa_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_cached_columns_from_file\u001b[49m(\n\u001b[1;32m    313\u001b[0m     df_qa_out, \n\u001b[1;32m    314\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mragas_result_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    316\u001b[0m     eval_criterias\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Sometimes ground_truth does not provide a response. Just filter those out.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m df_qa_out \u001b[38;5;241m=\u001b[39m df_qa_out[df_qa_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m))]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'add_cached_columns_from_file'"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "eval_criterias=[\"answer_correctness\", \"faithfulness\", \"context_recall\"]\n",
    "df_qa = eval.eval_rag(df_qa,eval_criterias,testset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=True\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
