{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "\n",
    "    if Path(file_name).exists():\n",
    "\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # return df.insert(0, column, None)\n",
    "\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        # df_out = df.copy().assign(column=\"\")\n",
    "        # df_out = df_out.reindex(columns=( [col for col in df_out.columns if col not in [column]] + [column] ))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "query_model=OpenAIEmbeddings(model='text-embedding-ada-002',openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# index_name='chromadb-openai-ams-400chunk-2000-2020'\n",
    "index_name='chromadb-openai-ams-full-2000-2020'\n",
    "\n",
    "# Connect to vectorstore where no chunking was done only full PDF pages\n",
    "vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=index_name,\n",
    "                        embedding_function=query_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "\n",
    "# generator_model=\"google/gemma-7b-it\"\n",
    "# synthetic_generator_llm = ChatOpenAI(base_url='https://api-inference.huggingface.co/v1',\n",
    "#                             model=generator_model,\n",
    "#                             api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "#                             tags=generator_model)\n",
    "# synthetic_generator_llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "critic_model=\"gpt-3.5-turbo-0125\"\n",
    "# critic_model=\"gpt-4\"\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "synthetic_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "sample_size=500 # This is the max size before you'll hit rate limits\n",
    "percent_total=sample_size/len(lcdocs)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO run this in batches according to the rate limits\n",
    "\n",
    "# Check if testset.csv exists\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=5\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=True,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=True)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv('testset.csv', index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['question_by'] = generator_model\n",
    "\n",
    "# questions_all = [\n",
    "#     {\n",
    "#         \"question\": qa.question,\n",
    "#         \"ground_truth\": qa.ground_truth,\n",
    "#         \"question_by\": generator_model,\n",
    "#     }\n",
    "#     for qa in df_testset\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions\n",
    "\n",
    "# df_questions = pd.DataFrame(\n",
    "#     {\n",
    "#         \"id\": [f\"Question {i}\" for i, _ in enumerate(questions_all)],\n",
    "#         \"question\": [qa[\"question\"] for qa in questions_all],\n",
    "#         \"ground_truth\": [qa[\"ground_truth\"] for qa in questions_all],\n",
    "#         \"question_by\": [qa[\"question_by\"] for qa in questions_all],\n",
    "#     }\n",
    "# )\n",
    "# # keep only the first question if questions are duplicated\n",
    "# df_questions = df_questions.drop_duplicates(subset=[\"question\"])\n",
    "# df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_questions_answers = df_questions.copy().assign(answer=\"\", source_documents=\"\")\n",
    "# df_questions_answers = df_questions_answers.reindex(columns=( [col for col in df_questions_answers.columns if col not in ['answer', 'source_documents']] \n",
    "#                          + ['answer', 'source_documents'] ))\n",
    "# df_questions_answers\n",
    "\n",
    "# load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, \"rag_response_cache.txt\", \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, \"rag_response_cache.txt\", \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below demonstrates using aerospace_chatbot in batch mode. It requires some basic parameter setup and a QA_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO turn this into a function/standalone notebook to show how to use the QA model in batch mode\n",
    "\n",
    "index_type='ChromaDB'\n",
    "index_name=index_name\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n",
    "\n",
    "for i, row in df_questions_answers.iterrows():\n",
    "    if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "        print(f\"Processing question {i+1}/{len(df_questions_answers)}\")\n",
    "\n",
    "        # Use the QA model to query the documents\n",
    "        qa_obj=queries.QA_Model(index_type,\n",
    "                        index_name,\n",
    "                        query_model,\n",
    "                        llm,\n",
    "                        **QA_model_params)\n",
    "        qa_obj.query_docs(row['question'])\n",
    "        response=qa_obj.result\n",
    "\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response['answer'].content\n",
    "        # print(response['references'])\n",
    "\n",
    "        ids=[_stable_hash_meta(source_document.metadata)\n",
    "            for source_document in response['references']]\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "        # Save the response to cache file\n",
    "        response_dict = {\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": response['answer'].content,\n",
    "            \"source_documents\": ids,\n",
    "        }\n",
    "        write_dict_to_file(response_dict, \"rag_response_cache.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the context documents content for each question\n",
    "source_documents_list = []\n",
    "for cell in df_questions_answers['source_documents']:\n",
    "    cell_list = cell.strip('[]').split(', ')\n",
    "    context=[]\n",
    "    for cell in cell_list:\n",
    "        context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "    source_documents_list.append(context)\n",
    "df_questions_answers[\"contexts\"]=source_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtionaly get embeddings for questions\n",
    "\n",
    "if not Path(\"question_embeddings.pickle\").exists():\n",
    "    question_embeddings = [\n",
    "        synthetic_embeddings.embed_query(question)\n",
    "        for question in df_questions_answers[\"question\"]\n",
    "    ]\n",
    "    with open(\"question_embeddings.pickle\", \"wb\") as f:\n",
    "        pickle.dump(question_embeddings, f)\n",
    "\n",
    "question_embeddings = pickle.load(open(\"question_embeddings.pickle\", \"rb\"))\n",
    "# answer_embeddings = pickle.load(open(\"answer_embeddings_2040214_1111.pickle\", \"rb\"))\n",
    "df_questions_answers[\"embedding\"] = question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
