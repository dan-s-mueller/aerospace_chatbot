{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix openai version to version 0.28. They did a major overhaul on the python code base and it breaks a lot of stuff. Will update once langchain and others catch up. See https://github.com/openai/openai-python for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all of the required packages with the requirements.txt file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Langsmith is cool. It automatically logs and tags every single code run you do. \n",
    "* Link to Langsmith, code being logged under the Aerospace AI project. https://smith.langchain.com/o/45eb8917-7353-4296-978d-bb461fc45c65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page setup and execution for the aerospace mechanism chatbot\n",
    "Example :        \n",
    "-What can you tell me about latch mechanism design failures which have occurred        \n",
    "-Follow up: Which one of the sources discussed volatile spherical joint interfaces           \n",
    "\"\"\"\n",
    "# import databutton as db\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import data_import\n",
    "import queries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import streamlit as st\n",
    "import openai\n",
    "import secrets\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check api keys\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# print(os.getenv('PINECONE_ENVIRONMENT'))\n",
    "# print(os.getenv('PINECONE_API_KEY'))\n",
    "# print(os.getenv('HUGGING_FACE_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_TRACING_V2'))\n",
    "# print(os.getenv('LANGCHAIN_ENDPOINT'))\n",
    "# print(os.getenv('LANGCHAIN_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_PROJECT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeable fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_level=None   # Max token limit, see section below\n",
    "k=4 # Number of queries to return\n",
    "search_type='similarity'    #  'mmr' or 'similarity'\n",
    "temperature=0\n",
    "verbose=True\n",
    "chain_type='stuff'  # 'stuff' or  'map_reduce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some items\n",
    "if output_level==\"Concise\":\n",
    "    max_generated_tokens=50\n",
    "elif output_level==\"Detailed\":\n",
    "    max_generated_tokens=516\n",
    "else:\n",
    "    max_generated_tokens = -1\n",
    "    # max_generated_tokens=None # Openai>0.28 requires this, not -1.\n",
    "\n",
    "# Track filtering, set history to be blank\n",
    "message_id=0\n",
    "filter_toggle=False # Filter sources on last answer\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT')\n",
    ")\n",
    "index_name = 'canopy--ams'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openai Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voyage embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the cells below to get an llm that plugs into the prompts section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprietary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models: https://platform.openai.com/docs/models/gpt-3-5, https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "* This code will break if openai>0.28. See https://github.com/openai/openai-python for details. Langchain yells at you but it still works for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/llms/openai.py:244: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/llms/openai.py:1043: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# model_name='gpt-3.5-turbo-instruct'\n",
    "model_name='gpt-3.5-turbo-1106' # 16,385 tokens\n",
    "# model_name='gpt-4-0613'\n",
    "\n",
    "llm = OpenAI(temperature=temperature,\n",
    "             model_name=model_name,\n",
    "             max_tokens=max_generated_tokens,\n",
    "             tags=[model_name+'-'+str(temperature)+'-'+str(max_generated_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Albert Einstein was born on March 14, 1879.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What year was albert einstein born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Anthropic, Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hugging face models (via API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose a model_name from the leaderboard here: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard \n",
    "* When testing these, many of the models will time out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_name='google/flan-t5-xxl'   # Fast but with uninteresting answers\n",
    "# model_name='tiiuae/falcon-40b'  # Timed out\n",
    "# model_name='WizardLM/WizardLM-70B-V1.0'   # Timed out\n",
    "# model_name='HuggingFaceH4/zephyr-7b-beta' # Timed out\n",
    "# model_name='meta-llama/Llama-2-70b-chat-hf'   # Gated repo access, request open\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name,\n",
    "                     model_kwargs={\"temperature\": 0.1, \"max_length\": 250}\n",
    ")\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "Generate a question that a human would ask to get the following response from a chatbot who is knowledgable about aerospace mechanisms:\n",
    "Dynamic Behavior of Ball Bearings under Axial Vibration  Virgil Hinque* and René Seiler* Abstract  The paper addresses the dynamics of ball bearings when exposed to vibration loads along their axis of  rotation. Following common practice in space mechanisms design, the bearings are mounted in either hard  preloaded or soft preloaded pairs. A computer-based model has been developed for the analysis and  prediction of the load-deflection characteristics in bearing systems. Furthermore, the model may be used  to quantify the maximum loads applied on the bearings and the resulting stresses during a vibration test or  a spacecraft launch.  In parallel to the model development, an experimental test program has been carried out in order to get  sufficient data for model correlation. In this context, the paper also elaborates on the post-processing of the  acquired test signals and discusses specific effects, for instance nonlinearities due to the use of snubbers,  in the time domain as well as in the frequency domain.  Introduction  Many space mechanisms use ball bearings for rotation functions. Therefore, assessing the bearing  performance for the relevant environmental conditions is one of the typical challenges faced during the  equipment design process. In this frame, it is common engineering practice to reduce the effect of a sine  and random vibration environment to quasi-static equivalent loads and stresses. The relevant ball bearing  systems often comprise two identical deep-groove or angular-contact bearings in an axially preloaded  configuration. Several studies on the influence of the preload and other parameters on the structural  behavior of such bearing assemblies have been done by the European Space Tribology Laboratory (ESTL).  In a recent investigation, 25 ball bearing cartridges (“test units” or “bearing housings”) with different preload  and snubber configurations were submitted to a series of sine and random vibration tests. The discussion  of findings was mainly based on the analysis of frequency-domain data and bearing damage assessment  via visual inspection [1].  The ESTL investigation inspired a number of ideas for continuation of the research, among others the  development of a computer-based model that would be able to simulate the behavior of the bearing  cartridges, especially those showing nonlinear features in their response. An adequate model should be  able to predict the load transmission across the bearings in static and dynamic load situations. As the main  sizing criterion for ball bearings is based on the allowable peak Hertzian contact pressure between the balls  and the races [2], accurate knowledge of the maximum bearing loads is a key aspect for successful bearing  selection and implementation in a space mechanism.  During the current investigation at the European Space Research and Technology Centre (ESTEC), a  model was built using MATLAB®/Simulink®, with only the axial degree of freedom in a bearing taken into  consideration. Because model correlation with real test results is of importance, a test program  complementary to that reported in [1] has been conducted, with specific focus on the acquisition and  interpretation of time-domain data. The following chapters describe the computer-based model, the design  of the test units, as well as the details of the test campaign and corresponding results. The last part of the  paper is dedicated to the comparison between the model output and the experimental test data.  *European Space Agency (ESA/ESTEC), Noordwijk, The Netherlands Proceedings of the 44th Aerospace Mechanisms Symposium, NASA Glenn Research Center, May 16-18, 2018 NASA/CP—2018-219887 83\n",
    "\"\"\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ollama (local, works on mac only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to work, download ollama and install on your mac. This tended to crash my computer (defaults to running on GPU, no option to modify). Use at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import Ollama\n",
    "\n",
    "# model_name='llama2:latest'\n",
    "# llm = Ollama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(\"the first man on the moon was...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local model hosted with LM Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome. Install https://lmstudio.ai/\n",
    "* Download a model. I installed Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf and mounted it on my macbook pro. 16gb of RAM, can get around 6000 word input responses in around 10 seconds.\n",
    "* Start a server locally and run the code below. This will not work unless you have the openai version 0.28 installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# model_name='local-model'    # This can be anything. I recommend using the name of the loaded model from LM Studio.\n",
    "# model_name='TheBloke/Mistral-7B-Instruct-v0.1-GGUF'\n",
    "model_name='TheBloke/Llama-2-7B-Chat-GGUF'\n",
    "\n",
    "base_url='http://localhost:1234/v1' # point to the local server\n",
    "max_tokens=256  # Set this differently because these models have different restrictions than OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=model_name,\n",
    "             base_url=base_url,\n",
    "             temperature=temperature,\n",
    "             max_tokens=256,\n",
    "             tags=[model_name+'-'+str(temperature)+'-'+str(max_generated_tokens)]\n",
    "            )\n",
    "             # temperature=temperature,\n",
    "             # max_tokens=max_generated_tokens,\n",
    "             # tags=[model_name+'-'+str(temperature)+'-'+str(max_generated_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Albert Einstein was born on March 14, 1879, in Ulm, Kingdom of Württemberg, German Empire.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What year was albert einstein born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['Prompt 1: What types of lubricants are to be avoided when designing space mechanisms?',\n",
    "        'Prompt 2: Can you speak to what failures have occurred when using mineral oil lubricants?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt 1: What types of lubricants are to be avoided when designing space mechanisms?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  Based on the provided sources, it appears that the president did not mention Michael Jackson in any of the documents. The documents are focused on various topics related to space exploration and lubrication, including the use of different types of lubricants in space mechanisms, the challenges of lubricating complex mechanisms in robotic space exploration missions, and the development of new hydrocarbon-based lubricants. Therefore, there is no mention of Michael Jackson in any of these documents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document_id': 'AMS_1998.pdf_31107727471-051f-4979-8b7b-ff5aafb8233d', 'page': '311', 'source': 'AMS_1998.pdf'}, {'document_id': 'AMS_2020.pdf_2684dd10ab7-7371-4b76-9347-8e22cb98ea41', 'page': '268', 'source': 'AMS_2020.pdf'}, {'document_id': 'AMS_2020.pdf_2655d737dec-85ef-4c92-a748-5606ab9cf2bc', 'page': '265', 'source': 'AMS_2020.pdf'}, {'document_id': 'AMS_2020.pdf_308b4ceecb7-7783-48c3-9975-62fd28ebe114', 'page': '308', 'source': 'AMS_2020.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# Set up object\n",
    "tags=['prompt1']\n",
    "qa_model_obj=queries.QA_Model(index_name,\n",
    "                    embeddings_model,\n",
    "                    llm,\n",
    "                    k,\n",
    "                    search_type,\n",
    "                    verbose,\n",
    "                    filter_arg=filter_toggle)\n",
    "\n",
    "# Generate a response using your chat model\n",
    "qa_model_obj.query_docs(prompt[0],tags=tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[0]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt 2: Can you speak to what failures have occurred when using mineral oil lubricants?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  Based on the provided documents, here are some key takeaways related to lubrication in space mechanisms:\n",
       "1. Reliability is critical: With the increasing number of space missions and longer mission times, it is essential to have a robust design with high reliability.\n",
       "2. Long life for components: To improve reliability, it is necessary to have long lives for all components in the design, which requires the use of lubricants that can provide consistent performance over time.\n",
       "3. Managing thin films: Spacecraft component life often depends on managing thin films in an evolving environment. Oil mobility decreases with film thickness, and new techniques can detect subtle changes in lubricant degradation.\n",
       "4. Heritage lubricants: PFPE and MAC are heritage lubricants used in space applications due to their outstanding resistance to outgassing. However, they both have benefits and drawbacks, including the formation of iron fluorides in tribocontacts.\n",
       "5. New techniques for detection: Experimental techniques such as atomic force microscopy (AFM) and X-ray microtomography (XMT) can detect subtle changes in lubric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document_id': 'AMS_2020.pdf_314f564551c-6e39-4ac2-b21c-af1d540e1150', 'page': '314', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2004.pdf_3714ae8b2f2-e157-47ce-8497-e5f92628c864', 'page': '371', 'source': 'AMS_2004.pdf'},\n",
      " {'document_id': 'AMS_1997.pdf_289ab65038c-470e-4a9f-b81f-bc837a90ff37', 'page': '289', 'source': 'AMS_1997.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_311a523abb5-56f7-4a2b-974a-8675cab12b01', 'page': '311', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_3113d25b091-9d8f-4d1b-8390-bd85444d3db0', 'page': '311', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_2684dd10ab7-7371-4b76-9347-8e22cb98ea41', 'page': '268', 'source': 'AMS_2020.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# Generate a response using your chat model\n",
    "tags=['prompt2_nofilter']\n",
    "qa_model_obj.update_model(llm=llm,\n",
    "                          filter_arg=False)\n",
    "\n",
    "qa_model_obj.query_docs(prompt[1],\n",
    "                        tags=tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[1]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
