{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import markdown\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import eval\n",
    "import admin\n",
    "from data_processing import _stable_hash_meta, archive_db\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-large-0merge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them. Uncomment if desired, takes a while.\n",
    "export=False\n",
    "\n",
    "if export:\n",
    "    for collection in collections:\n",
    "        df_temp_chroma=archive_db('ChromaDB',collection.name,collection._embedding_function,export_pickle=True)\n",
    "\n",
    "    df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-2merge-0',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)},\n",
    "     {'index_name':'voyage-large-2-instruct-0merge-400',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "db=dbs[0]\n",
    "index = pinecone_client.Index(db['index_name'])\n",
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)\n",
    "\n",
    "docs=[]\n",
    "chunk_size=200  # Tune to whatever doesn't error out, 200 won't for serverless\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)\n",
    "\n",
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))\n",
    "    \n",
    "print(len(lcdocs_pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "# for db in dbs:\n",
    "#     df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "idx=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "\n",
    "docs_vectorstore=collections[idx]\n",
    "query_model=query_models[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format docs into dataframe\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "n_questions=10   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}.csv\")\n",
    "lcdocs=lcdocs_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset=eval.generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-large-2merge-0'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "df_testset=df_testset.head(4)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How much power does the four-element piezo mot...</td>\n",
       "      <td>The four-element piezo motor itself consumes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How did statistical analysis determine the num...</td>\n",
       "      <td>Statistical analysis determined that 2 scars o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How is the stress field in the Si3N4 ball and ...</td>\n",
       "      <td>The stress field in the Si3N4 ball and raceway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How important is testing in ensuring the relia...</td>\n",
       "      <td>Testing is crucial in ensuring the reliability...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                           question  \\\n",
       "0            0  How much power does the four-element piezo mot...   \n",
       "1            1  How did statistical analysis determine the num...   \n",
       "2            2  How is the stress field in the Si3N4 ball and ...   \n",
       "3            3  How important is testing in ensuring the relia...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  The four-element piezo motor itself consumes a...  \n",
       "1  Statistical analysis determined that 2 scars o...  \n",
       "2  The stress field in the Si3N4 ball and raceway...  \n",
       "3  Testing is crucial in ensuring the reliability...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_type': 'ChromaDB',\n",
       " 'index_name': 'text-embedding-3-large-2merge-0',\n",
       " 'query_model': {'query_model': 'OpenAI',\n",
       "  'embedding_name': 'text-embedding-3-large'},\n",
       " 'llm': {'llm_source': 'OpenAI',\n",
       "  'llm_model': 'gpt-4',\n",
       "  'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       " 'qa_model_params': {'rag_type': 'Standard',\n",
       "  'k': 4,\n",
       "  'search_type': 'similarity',\n",
       "  'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "# TODO add pinecone+voyage\n",
    "# TODO add parent-child\n",
    "# TODO add ragatouille\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df_qa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m setup_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_models\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Database\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     index_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChromaDB\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_markdown'"
     ]
    }
   ],
   "source": [
    "df_qa = pd.DataFrame()\n",
    "\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type='ChromaDB'\n",
    "    sb['index_type']=index_type\n",
    "    index_name=testset_name\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{index_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa = eval.eval_rag(index_name, df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=False\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
