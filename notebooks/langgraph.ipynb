{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test partitioning update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.processing import DocumentProcessor\n",
    "from aerospace_chatbot.services import EmbeddingService, RerankService, LLMService, DatabaseService\n",
    "from aerospace_chatbot.processing import QAModel\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "# Set LOCAL_DB_PATH environment variable\n",
    "# os.environ['LOCAL_DB_PATH'] = os.path.abspath('.')\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type='Pinecone'\n",
    "\n",
    "embedding_service='OpenAI'\n",
    "embedding_model='text-embedding-3-large'\n",
    "\n",
    "rerank_service='Cohere'\n",
    "rerank_model='rerank-v3.5'\n",
    "\n",
    "llm_service='OpenAI'\n",
    "llm_model='gpt-4o'\n",
    "# llm_service='Anthropic'\n",
    "# llm_model='claude-3-5-sonnet-latest'\n",
    "\n",
    "chunk_size=400\n",
    "chunk_overlap=0\n",
    "batch_size=50\n",
    "index_name = 'text-embedding-3-large-test'\n",
    "\n",
    "test_prompt='How does a thermal knife function in a cable based hold down release mechanism?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize services\n",
    "embedding_service = EmbeddingService(\n",
    "    model_service=embedding_service,\n",
    "    model=embedding_model\n",
    ")\n",
    "\n",
    "rerank_service = RerankService(\n",
    "    model_service=rerank_service,\n",
    "    model=rerank_model\n",
    ")\n",
    "\n",
    "llm_service = LLMService(\n",
    "    model_service=llm_service,\n",
    "    model=llm_model,\n",
    ")\n",
    "\n",
    "doc_processor = DocumentProcessor(\n",
    "    embedding_service=embedding_service,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Initialize database service\n",
    "db_service = DatabaseService(\n",
    "    db_type=db_type,\n",
    "    index_name=index_name,\n",
    "    embedding_service=embedding_service,\n",
    "    rerank_service=rerank_service,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'processing-pdfs'\n",
    "docs = DocumentProcessor.list_bucket_pdfs(bucket_name)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_docs = doc_processor.load_and_partition_documents(docs,partition_by_api=False, upload_bucket=bucket_name)\n",
    "partitioned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_obj, output_paths = doc_processor.chunk_documents(partitioned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     db_service.initialize_database(clear=False)\n",
    "# except ValueError as e:\n",
    "#     print(f\"Database initialization failed: {str(e)}\")\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_service.index_data(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_retrieve=20\n",
    "# k_rerank=5\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# qa_model = QAModel(\n",
    "#     db_service=db_service,\n",
    "#     llm_service=llm_service,\n",
    "#     k_retrieve=k_retrieve,\n",
    "#     k_rerank=k_rerank,\n",
    "#     memory_config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_model.query(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.result[-1]['references'])\n",
    "# print(qa_model.sources[-1])\n",
    "# print(qa_model.scores[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above section first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, RemoveMessage\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing_extensions import List\n",
    "from typing import List, Literal, Tuple\n",
    "\n",
    "# import cohere\n",
    "# import os\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from aerospace_chatbot.services.prompts import InLineCitationsResponse, AltQuestionsResponse, style_mode, CHATBOT_SYSTEM_PROMPT, QA_PROMPT, SUMMARIZE_TEXT, GENERATE_SIMILAR_QUESTIONS_W_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_service.retriever\n",
    "llm = llm_service.get_llm()\n",
    "# memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs with standard retriever\n",
    "retrieved_docs = retriever.invoke(test_prompt)\n",
    "\n",
    "# retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "# co = cohere.ClientV2(COHERE_API_KEY)\n",
    "# rerank_model = \"rerank-v3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = co.models.list()\n",
    "# rerank_model_info = next((model for model in model_list.models if model.name == rerank_model), None)\n",
    "# print(rerank_model_info)\n",
    "\n",
    "# print(model_list.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token count for each doc\n",
    "# max_context_length = 4096\n",
    "\n",
    "# token_count_list = []\n",
    "# for doc in retrieved_docs[0]:\n",
    "#     tokens = co.tokenize(\n",
    "#         model=rerank_model,\n",
    "#         text=doc.page_content,\n",
    "#     )\n",
    "#     token_count_list.append(len(tokens.tokens))\n",
    "    # print(token_count)\n",
    "\n",
    "# print(token_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cohere_rerank(query: str, retrieved_docs: List[Tuple[Document, float]], top_n: int = None):\n",
    "#     # retrieved_docs contains a list of tuples, where the first element is the document and the second is the score\n",
    "#     # Cohere's rerank expects a list of strings; we'll supply the page_content\n",
    "\n",
    "#     if top_n is None:\n",
    "#         top_n = 3\n",
    "#     elif top_n < 3:\n",
    "#         raise ValueError(\"top_n must be at least 3\")\n",
    "#     elif top_n > len(retrieved_docs):\n",
    "#         raise ValueError(\"top_n must be less than or equal to the number of retrieved documents\")\n",
    "    \n",
    "#     inputs = [doc.page_content for doc, _ in retrieved_docs]\n",
    "\n",
    "#     # Call Cohere's Rerank endpoint\n",
    "#     response = co.rerank(\n",
    "#         model=rerank_model,\n",
    "#         query=query,\n",
    "#         documents=inputs,\n",
    "#         top_n=top_n\n",
    "#     )\n",
    "\n",
    "#     # Create a dictionary to map document IDs to rerank scores\n",
    "#     rerank_scores = {retrieved_docs[i][0].id: item.relevance_score for i, item in enumerate(response.results)}\n",
    "\n",
    "#     # Create list of (doc, original_score, rerank_score) tuples\n",
    "#     doc_scores = []\n",
    "#     for doc, original_score in retrieved_docs:\n",
    "#         rerank_score = rerank_scores.get(doc.id, None)  # Get the rerank score or None if not available\n",
    "#         doc_scores.append((doc, original_score, rerank_score))\n",
    "\n",
    "#     # Sort docs by rerank score in descending order, placing those without a rerank score at the end\n",
    "#     doc_scores_sorted = sorted(doc_scores, key=lambda x: (x[2] is not None, x[2]), reverse=True)\n",
    "\n",
    "#     return doc_scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = db_service.rerank(test_prompt, retrieved_docs, top_n=k_rerank)\n",
    "# reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context to the prompt\n",
    "# TODO update this to use the doc.id. Do some testing to check this works.\n",
    "docs_content=\"\"\n",
    "for i, (doc, original_score, rerank_score) in enumerate(reranked_docs):\n",
    "    # Source IDs in the order they show in in the array. Indexed from 0.\n",
    "    if rerank_score is not None:    # Only include docs with a rerank score\n",
    "        docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "# print(docs_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InLineCitationsResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should validate\n",
    "valid_response = InLineCitationsResponse(content=\"\"\"\n",
    "The actuator was tested under high pressure <source id=\"1\">. \n",
    "Material properties were measured over 50 cycles <source id=\"2\">.\n",
    "Thermal resistance improved by 30% <source id=\"3\">.\n",
    "\"\"\",\n",
    "citations=[\"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't validate\n",
    "try:\n",
    "    invalid_response = InLineCitationsResponse(content=\"\"\"\n",
    "    The actuator was tested under high pressure [1]. \n",
    "    Material properties were measured under load <source id=\"x\">.\n",
    "    \"\"\",\n",
    "    citations=[\"1\", \"x\"]\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT_TEST=PromptTemplate(\n",
    "    template=\n",
    "\"\"\"\n",
    "# **System Prompt**\n",
    "\n",
    "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering. You will function as a knowledgeable replacement for an expert in aerospace flight hardware design, testing, analysis, and certification.\n",
    "\n",
    "> **Important Note:** The **Sources and Context** you are provided are ranked from most relevant to least relevant by a state-of-the-art retrieval and ranking tool. Please take this ranking into consideration when determining which sources to cite.\n",
    "\n",
    "Use only the **Sources and Context** provided to answer the **User Question**. **Do not use outside knowledge**, and strictly follow these rules:\n",
    "\n",
    "---\n",
    "\n",
    "## **Rules**:\n",
    "\n",
    "1. **Answer only based on the provided Sources and Context.**  \n",
    "   - If the information is not available in the Sources and Context, respond with:  \n",
    "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*  \n",
    "   - **If no sources are truly relevant to the answer, do not cite any sources.**\n",
    "\n",
    "2. **Do not make up or infer answers.**  \n",
    "   - Stay accurate and factual at all times.\n",
    "\n",
    "3. **Provide highly detailed, explanatory answers.**  \n",
    "   - Include **as many specific details from the original context** as possible to thoroughly address the user’s question.\n",
    "\n",
    "4. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
    "\n",
    "5. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
    "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
    "   - **The `source` tag must be present for every source referenced in the response.**  \n",
    "   - **Do not add, omit, or modify any part of the citation format.**  \n",
    "   \n",
    "   **Examples (Correct):**  \n",
    "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
    "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
    "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
    "\n",
    "   **Examples (Incorrect – Must Be Rejected):**  \n",
    "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
    "   > <source id=\"1\" > (Extra space after `id`)  \n",
    "   > <source id=\"a\"> (Non-numeric ID)  \n",
    "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
    "\n",
    "6. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
    "   - **Do not group multiple sources into a single tag.**  \n",
    "   - Each source must have its own, clearly separated citation.  \n",
    "   - For example:  \n",
    "     > The actuator uses a reinforced composite structure <source id=\"1\">.  \n",
    "     > This design was validated through multiple tests <source id=\"2\">.\n",
    "\n",
    "7. **Validation Requirement:**  \n",
    "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
    "   - Every source used must have a corresponding citation in the response.  \n",
    "   - **No source should be referenced without explicit citation.**\n",
    "\n",
    "8. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
    "\n",
    "9. **Always cite Source ID: 1**  \n",
    "   - Always provide a citation for Source ID: 1 unless it is entirely irrelevant to the user’s question (in which case explicitly omit it).  \n",
    "   - Follow Rule #10 for the other sources.\n",
    "\n",
    "10. **Give preference to citing top-ranked sources, provided in order of highest to lowest relevance.**  \n",
    "   - If the first sources in the list (i.e., the most relevant or highest-ranked) contain information that addresses the user’s question, cite them first.  \n",
    "   - Then cite additional sources only if they contain new or non-redundant details.  \n",
    "   - If the top-ranked sources are not relevant, skip them.\n",
    "\n",
    "---\n",
    "**Sources and Context**:\n",
    "{context}\n",
    "---\n",
    "\n",
    "---\n",
    "**User Question**:\n",
    "{question}\n",
    "---\n",
    "\n",
    "---\n",
    "{format_instructions}\n",
    "---\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": PydanticOutputParser(pydantic_object=InLineCitationsResponse).get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = llm.invoke(QA_PROMPT_TEST.format(context=docs_content, question=test_prompt))\n",
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_response = PydanticOutputParser(pydantic_object=InLineCitationsResponse).parse(raw_output.content)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AltQuestionsResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should validate\n",
    "valid_response = AltQuestionsResponse(\n",
    "    questions=[\"What is the purpose of the actuator?\", \"What are the key components of the mechanism?\", \"How does the mechanism work?\"],\n",
    ")\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't validate\n",
    "try:\n",
    "    invalid_response = AltQuestionsResponse(\n",
    "        questions=[\"What is the purpose of the actuator?\", \"What are the key components of the mechanism?\"],\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = llm.invoke(GENERATE_SIMILAR_QUESTIONS_W_CONTEXT.format(context=docs_content, question=test_prompt))\n",
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_response = PydanticOutputParser(pydantic_object=AltQuestionsResponse).parse(raw_output.content)\n",
    "print(parsed_response.questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class State(MessagesState):\n",
    "#     context: List[Tuple[Document, float, float]]\n",
    "#     cited_sources: List[Tuple[Document, float, float]]\n",
    "#     summary: str\n",
    "\n",
    "# # Define application steps\n",
    "# def retrieve(state: State):\n",
    "#     \"\"\"\n",
    "#     Retrieve the documents from the database.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: retrieve\")\n",
    "\n",
    "#     # Retrieve docs\n",
    "#     retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n",
    "#     logger.info(f\"Retrieved docs\")\n",
    "#     # Rerank docs\n",
    "#     # reranked_docs = cohere_rerank(\n",
    "#     #     state[\"messages\"][-1].content, \n",
    "#     #     retrieved_docs, \n",
    "#     #     top_n=k_rerank\n",
    "#     # )\n",
    "#     reranked_docs = db_service.rerank(\n",
    "#         state[\"messages\"][-1].content, \n",
    "#         retrieved_docs, \n",
    "#         top_n=k_rerank\n",
    "#     )\n",
    "#     logger.info(f\"Reranked docs\")\n",
    "\n",
    "#     return {\"context\": reranked_docs}\n",
    "\n",
    "# def generate_w_context(state: State):\n",
    "#     \"\"\"\n",
    "#     Call the model with the prompt with context.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: generate_w_context\")\n",
    "\n",
    "#     # Get the summary, add system prompt\n",
    "#     summary = state.get(\"summary\", \"\")\n",
    "#     system_prompt = CHATBOT_SYSTEM_PROMPT.format(style_mode=style_mode(style))\n",
    "#     logger.info(f\"generate_w_context system prompt: {system_prompt.content}\")\n",
    "#     if summary:\n",
    "#         system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "#         messages = [system_prompt] + [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "#     else:\n",
    "#         messages = [system_prompt] + state[\"messages\"]\n",
    "\n",
    "#     # Add context to the prompt\n",
    "#     docs_content=\"\"\n",
    "#     for i, (doc, retrieved_score, rerank_score) in enumerate(state[\"context\"]):\n",
    "#         # Source IDs in the order they show in in the array. Indexed from 1, retrieve with 0 index.\n",
    "#         if rerank_score is not None:    # Only include docs with a rerank score\n",
    "#             docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "#     # Prompt with context and pydantic output parser\n",
    "#     prompt_with_context = QA_PROMPT.format(\n",
    "#         context=docs_content,\n",
    "#         question=state[\"messages\"][-1].content, \n",
    "#     )\n",
    "#     # Replace the last message (user question) with the prompt with context, return LLM response\n",
    "#     messages[-1] = prompt_with_context \n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     # Parse the response. This will return a InLineCitationsResponse object. \n",
    "#     # This object has two fields: content and citations.\n",
    "#     # Replace the last message with the content of the parsed and validated response. \n",
    "#     # AIMessage metadata will be incorrect.\n",
    "#     parsed_response = PydanticOutputParser(pydantic_object=InLineCitationsResponse).parse(response.content)\n",
    "#     response.content = parsed_response.content\n",
    "\n",
    "#     # Return cited_sources as the list of tuples that matched the citations.\n",
    "#     existing_cited_sources = state.get(\"cited_sources\", [])  # Grab whatever might already be in cited_sources\n",
    "#     cited_sources = [state[\"context\"][int(citation)-1] for citation in parsed_response.citations]\n",
    "#     existing_cited_sources.append(cited_sources)  # Append the new list as a sublist\n",
    "#     state[\"cited_sources\"] = existing_cited_sources\n",
    "\n",
    "#     # Update the state messages with the messages updated in this node.\n",
    "#     state[\"messages\"] = messages\n",
    "#     return {\"messages\": [response], \n",
    "#             \"cited_sources\": state[\"cited_sources\"]}\n",
    "\n",
    "# def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "#     \"\"\"\n",
    "#     Define the logic for determining whether to end or summarize the conversation\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: should_continue\")\n",
    "\n",
    "#     # If there are more than six messages, then we summarize the conversation\n",
    "#     messages = state[\"messages\"]\n",
    "#     if len(messages) > 6:\n",
    "#         logger.info(f\"Summarizing conversation\")\n",
    "#         return \"summarize_conversation\"\n",
    "    \n",
    "#     # Otherwise just end\n",
    "#     logger.info(f\"Ending conversation\")\n",
    "#     # logger.info(f\"Messages before ending: {messages}\")\n",
    "#     return END\n",
    "\n",
    "# def summarize_conversation(state: State):\n",
    "#     \"\"\"\n",
    "#     Summarize the conversation\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: summarize_conversation\")\n",
    "\n",
    "#     summary = state.get(\"summary\", \"\")\n",
    "#     if summary:\n",
    "#         # If a summary already exists, extend it\n",
    "#         summary_message = SUMMARIZE_TEXT.format(\n",
    "#             summary=summary,\n",
    "#             augment=\"Extend the summary provided by taking into account the new messages above.\"\n",
    "#         )\n",
    "#     else:\n",
    "#         # If no summary exists, create one\n",
    "#         summary_text=\"\"\"---\\n**Conversation Summary to Date**:\\n{summary}\\n---\"\"\"\n",
    "#         summary_message = SUMMARIZE_TEXT.format(\n",
    "#             summary=summary_text,\n",
    "#             augment=\"Create a summary of the conversation above.\"\n",
    "#         )\n",
    "\n",
    "#     messages = state[\"messages\"] + [summary_message]\n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     # Prune messages. This deletes all but the last two messages\n",
    "#     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "#     return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile application and test\n",
    "# workflow = StateGraph(State)\n",
    "\n",
    "# # Define nodes\n",
    "# workflow.add_node(\"retrieve\", retrieve) \n",
    "# workflow.add_node(\"generate_w_context\", generate_w_context)\n",
    "# workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "# # Define edges\n",
    "# workflow.add_edge(START, \"retrieve\")\n",
    "# workflow.add_edge(\"retrieve\", \"generate_w_context\")\n",
    "\n",
    "# # We now add a conditional edge\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"generate_w_context\",   # Define the start node. We use `generate_w_context`. This means these are the edges taken after the `conversation` node is called.\n",
    "#     should_continue,    # Next, pass in the function that will determine which node is called next.\n",
    "# )\n",
    "\n",
    "# # Add a normal edge from `summarize_conversation` to END. This means that after `summarize_conversation` is called, we end.\n",
    "# workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# # Compile the workflow\n",
    "# app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model.workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"197\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'My name is Dan. Please tell me about some interesting mecanism designs.'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "qa_model.query(prompt)\n",
    "for message in qa_model.result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model.result['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How have these mecahnisms been tested?'\n",
    "prompt = 'How is release indicated from the functional response of the mechanism?'\n",
    "result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['context'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PDF annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, base64, zlib\n",
    "\n",
    "def extract_orig_elements(orig_elements):\n",
    "    decoded_orig_elements = base64.b64decode(orig_elements)\n",
    "    decompressed_orig_elements = zlib.decompress(decoded_orig_elements)\n",
    "    return decompressed_orig_elements.decode('utf-8')\n",
    "\n",
    "orig_elements = extract_orig_elements(result['context'][2][0].metadata['orig_elements'])\n",
    "orig_elements = json.loads(orig_elements)\n",
    "print(orig_elements)\n",
    "print(len(orig_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_range = [orig_elements[0]['metadata']['page_number'], orig_elements[-1]['metadata']['page_number']]\n",
    "print(page_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.ui.utils import display_source_highlights\n",
    "\n",
    "annotated_pdfs = display_source_highlights(result['context'][:k_rerank])\n",
    "print(len(annotated_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from IPython.display import display, Image\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "pdf_bytes = annotated_pdfs[1].read()\n",
    "pdf_document = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "\n",
    "# Display each page\n",
    "for page in pdf_document:\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(1, 1))  # 2x zoom for better resolution\n",
    "    with NamedTemporaryFile(suffix='.png') as tmp:\n",
    "        pix.save(tmp.name)\n",
    "        display(Image(filename=tmp.name))\n",
    "\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary memory test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How old are you?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some lessons learned about these mechanisms?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some problems that have occurred?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGatouille retrieval testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.processing import DocumentProcessor\n",
    "from aerospace_chatbot.services import EmbeddingService, RerankService, LLMService, DatabaseService\n",
    "from aerospace_chatbot.processing import QAModel\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "# Set LOCAL_DB_PATH environment variable\n",
    "# os.environ['LOCAL_DB_PATH'] = os.path.abspath('.')\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "db_type='RAGatouille'\n",
    "\n",
    "embedding_service='RAGatouille'\n",
    "embedding_model='colbert-ir/colbertv2.0'\n",
    "\n",
    "rerank_service='Cohere'\n",
    "rerank_model='rerank-v3.5'\n",
    "\n",
    "llm_service='OpenAI'\n",
    "llm_model='gpt-4o'\n",
    "# llm_service='Anthropic'\n",
    "# llm_model='claude-3-5-sonnet-latest'\n",
    "\n",
    "chunk_size=400\n",
    "chunk_overlap=0\n",
    "batch_size=50\n",
    "index_name = 'ragatouille-test'\n",
    "\n",
    "test_prompt='How does a thermal knife function in a cable based hold down release mechanism?'\n",
    "# Initialize services\n",
    "embedding_service = EmbeddingService(\n",
    "    model_service=embedding_service,\n",
    "    model=embedding_model\n",
    ")\n",
    "\n",
    "# rerank_service = RerankService(\n",
    "#     model_service=rerank_service,\n",
    "#     model=rerank_model\n",
    "# )\n",
    "rerank_service = None\n",
    "\n",
    "llm_service = LLMService(\n",
    "    model_service=llm_service,\n",
    "    model=llm_model,\n",
    ")\n",
    "\n",
    "doc_processor = DocumentProcessor(\n",
    "    embedding_service=embedding_service,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Initialize database service\n",
    "db_service = DatabaseService(\n",
    "    db_type=db_type,\n",
    "    index_name=index_name,\n",
    "    embedding_service=embedding_service,\n",
    "    rerank_service=rerank_service,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Number of PDFs found: 2\n",
      "INFO:aerospace_chatbot.processing.documents:PDFs found: ['gs://processing-pdfs/1999_christiansen_reocr.pdf', 'gs://processing-pdfs/1999_cremers_reocr.pdf']\n",
      "INFO:aerospace_chatbot.processing.documents:Loading 2 documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Checking document 1 of 2: gs://processing-pdfs/1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Downloading PDF from GCS: gs://processing-pdfs/1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Bucket name: processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Blob name: 1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Checking document 2 of 2: gs://processing-pdfs/1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Downloading PDF from GCS: gs://processing-pdfs/1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Bucket name: processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Blob name: 1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning 2 documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning ./document_processing/1999_christiansen_reocr.pdf Locally...\n",
      "INFO:pikepdf._core:pikepdf C++ to Python logger bridge initialized\n",
      "INFO:unstructured_inference:Reading PDF for file: ./document_processing/1999_christiansen_reocr.pdf ...\n",
      "INFO:unstructured_inference:Loading the Table agent ...\n",
      "INFO:unstructured_inference:Loading the table structure model ...\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioned data saved at ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json\n",
      "INFO:aerospace_chatbot.processing.documents:Uploaded ./document_processing/1999_christiansen_reocr.pdf and ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json to processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning ./document_processing/1999_cremers_reocr.pdf Locally...\n",
      "INFO:unstructured_inference:Reading PDF for file: ./document_processing/1999_cremers_reocr.pdf ...\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioned data saved at ./document_processing/partitioned/1999_cremers_reocr-partitioned.json\n",
      "INFO:aerospace_chatbot.processing.documents:Uploaded ./document_processing/1999_cremers_reocr.pdf and ./document_processing/partitioned/1999_cremers_reocr-partitioned.json to processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Chunking documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Total number of chunks: 91\n",
      "INFO:aerospace_chatbot.processing.documents:Output paths: ['./document_processing/chunked/1999_christiansen_reocr-chunked.json', './document_processing/chunked/1999_cremers_reocr-chunked.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json...\n",
      "Chunked data saved at ./document_processing/chunked/1999_christiansen_reocr-chunked.json\n",
      "Chunking ./document_processing/partitioned/1999_cremers_reocr-partitioned.json...\n",
      "Chunked data saved at ./document_processing/chunked/1999_cremers_reocr-chunked.json\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'processing-pdfs'\n",
    "docs = DocumentProcessor.list_bucket_pdfs(bucket_name)\n",
    "\n",
    "partitioned_docs = doc_processor.load_and_partition_documents(docs,partition_by_api=False, upload_bucket=bucket_name)\n",
    "chunk_obj, output_paths = doc_processor.chunk_documents(partitioned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.services.database:Validating index ragatouille-test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:49] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "INFO:aerospace_chatbot.services.database:RAGatouille index ragatouille-test initialized\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_service.initialize_database(clear=True)\n",
    "except ValueError as e:\n",
    "    print(f\"Database initialization failed: {str(e)}\")\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aerospace_chatbot.services.database:Metadata storage is not yet supported for RAGatouille indexes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jan 21, 09:32:49] #> Creating directory /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/ragatouille-test \n",
      "\n",
      "\n",
      "[Jan 21, 09:32:50] [0] \t\t #> Encoding 91 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:53] [0] \t\t avg_doclen_est = 55.593406677246094 \t len(local_sample) = 91\n",
      "[Jan 21, 09:32:53] [0] \t\t Creating 1,024 partitions.\n",
      "[Jan 21, 09:32:53] [0] \t\t *Estimated* 5,059 embeddings.\n",
      "[Jan 21, 09:32:53] [0] \t\t #> Saving the indexing plan to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/ragatouille-test/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sub_sample = torch.load(sub_sample_path)\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  centroids = torch.load(centroids_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (4807) is less than the minimum recommended (10240)\n",
      "used 10 iterations (0.0862s) to cluster 4807 items into 1024 clusters\n",
      "[0.033, 0.039, 0.032, 0.035, 0.029, 0.036, 0.037, 0.034, 0.037, 0.033, 0.035, 0.037, 0.035, 0.037, 0.036, 0.036, 0.031, 0.035, 0.032, 0.036, 0.035, 0.036, 0.036, 0.039, 0.032, 0.034, 0.034, 0.036, 0.031, 0.039, 0.04, 0.04, 0.033, 0.033, 0.036, 0.032, 0.035, 0.033, 0.035, 0.042, 0.034, 0.033, 0.035, 0.034, 0.032, 0.03, 0.037, 0.036, 0.036, 0.032, 0.038, 0.039, 0.037, 0.043, 0.034, 0.036, 0.038, 0.033, 0.037, 0.038, 0.032, 0.038, 0.036, 0.035, 0.036, 0.037, 0.041, 0.036, 0.032, 0.037, 0.038, 0.036, 0.034, 0.037, 0.034, 0.039, 0.042, 0.037, 0.039, 0.035, 0.043, 0.032, 0.036, 0.04, 0.034, 0.039, 0.037, 0.032, 0.031, 0.041, 0.034, 0.035, 0.031, 0.039, 0.037, 0.039, 0.046, 0.034, 0.035, 0.037, 0.034, 0.037, 0.04, 0.038, 0.044, 0.031, 0.034, 0.03, 0.038, 0.033, 0.036, 0.037, 0.038, 0.036, 0.034, 0.036, 0.039, 0.031, 0.038, 0.033, 0.031, 0.037, 0.038, 0.037, 0.035, 0.035, 0.036, 0.034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:53] [0] \t\t #> Encoding 91 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n",
      "1it [00:01,  1.95s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(codes_path, map_location='cpu')\n",
      "100%|██████████| 1/1 [00:00<00:00, 667.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:55] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jan 21, 09:32:55] #> Building the emb2pid mapping..\n",
      "[Jan 21, 09:32:55] len(emb2pid) = 5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 117105.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:55] #> Saved optimized IVF to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/ragatouille-test/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db_service.index_data(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_retrieve=20\n",
    "k_rerank=5\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "qa_model = QAModel(\n",
    "    db_service=db_service,\n",
    "    llm_service=llm_service,\n",
    "    k_retrieve=k_retrieve,\n",
    "    k_rerank=k_rerank,\n",
    "    memory_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x35983a710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model.workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.queries:Running query through workflow: How does a thermal knife function in a cable based hold down release mechanism?\n",
      "INFO:aerospace_chatbot.processing.queries:Node: retrieve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index ragatouille-test for the first time... This may take a few seconds\n",
      "[Jan 21, 09:32:57] #> Loading codec...\n",
      "[Jan 21, 09:32:57] #> Loading IVF...\n",
      "[Jan 21, 09:32:57] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  centroids = torch.load(centroids_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/search/index_loader.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ivf, ivf_lengths = torch.load(os.path.join(self.index_path, \"ivf.pid.pt\"), map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:57] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:57] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(codes_path, map_location='cpu')\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(residuals_path, map_location='cpu')\n",
      "100%|██████████| 1/1 [00:00<00:00, 490.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:57] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 21, 09:32:57] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . How does a thermal knife function in a cable based hold down release mechanism?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2129, 2515, 1037, 9829, 5442, 3853, 1999, 1037, 5830, 2241,\n",
      "        2907, 2091, 2713, 7337, 1029,  102,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "INFO:aerospace_chatbot.processing.queries:Retrieved docs\n",
      "WARNING:aerospace_chatbot.processing.queries:Rerank service is not set, but k_rerank is set to 5. Reranking will not be performed.\n",
      "INFO:aerospace_chatbot.processing.queries:Reranked docs\n",
      "INFO:aerospace_chatbot.processing.queries:Node: generate_w_context\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqa_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/processing/queries.py:68\u001b[0m, in \u001b[0;36mQAModel.query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Add answer to response, create an array as more prompts come in\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning query through workflow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_config\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1936\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/processing/queries.py:145\u001b[0m, in \u001b[0;36mQAModel._generate_w_context\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    142\u001b[0m docs_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Handle both 2-tuple (doc, retrieval_score) and 3-tuple (doc, retrieval_score, rerank_score) cases\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    146\u001b[0m     retrieval_score \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;66;03m# If reranked, only include docs with a rerank score\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Document' object is not subscriptable",
      "\u001b[0mDuring task with name 'generate_w_context' and id '6dff2c3c-07ee-2cac-c457-15069d584352'"
     ]
    }
   ],
   "source": [
    "qa_model.query(test_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
