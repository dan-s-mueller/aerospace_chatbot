{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from datasets import load_dataset\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset('dsmueller/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic_100',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: What is the title of the symposium hosted by NASA Langley Research Center and Lockheed Martin Space Systems Company, as mentioned in the context?### Assistant:NASNCP-2006-2 14290  3 sth Aerospace Mechanisms Symposium  Compiled by  Edward A. Boesiger  Lockheed Martin Space Systems Company, Sunnyvale, California  Proceedings of a symposium hosted by  the NASA Langley Research Center and  Lockheed Martin Space Systems Company and  organized by the Mechanisms Education Association  held at the Williamsburg Maniott Hotel  Williamsburg, Virginia  May 17- 19,2006  May 2006 {'source': 'AMS_2006.pdf', 'page': 1}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken this dataset and renamed into train.csv in the training folder. I can't figure out how to use autotrain with a different filename..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotrain\n",
    "https://github.com/huggingface/autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='./llms/'+'ams_data_train_TinyLlama_1.1B_Chat_v0.1_100'\n",
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v0.1'\n",
    "repo_id='ai-aerospace/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic-100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m⚠️ WARNING\u001b[0m | \u001b[32m2023-12-10 19:06:19\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[33m\u001b[1m❌ Some DreamBooth components are missing! Please run `autotrain setup` to install it. Ignore this warning if you are not using DreamBooth or running `autotrain setup` already.\u001b[0m\n",
      "usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy]\n",
      "                                        [--inference] [--data_path DATA_PATH]\n",
      "                                        [--train_split TRAIN_SPLIT]\n",
      "                                        [--valid_split VALID_SPLIT]\n",
      "                                        [--text_column TEXT_COLUMN]\n",
      "                                        [--rejected_text_column REJECTED_TEXT_COLUMN]\n",
      "                                        [--prompt-text-column PROMPT_TEXT_COLUMN]\n",
      "                                        [--model MODEL]\n",
      "                                        [--model-ref MODEL_REF]\n",
      "                                        [--learning_rate LEARNING_RATE]\n",
      "                                        [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                                        [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                                        [--warmup_ratio WARMUP_RATIO]\n",
      "                                        [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                        [--optimizer OPTIMIZER]\n",
      "                                        [--scheduler SCHEDULER]\n",
      "                                        [--weight_decay WEIGHT_DECAY]\n",
      "                                        [--max_grad_norm MAX_GRAD_NORM]\n",
      "                                        [--seed SEED] [--add_eos_token]\n",
      "                                        [--block_size BLOCK_SIZE] [--use_peft]\n",
      "                                        [--lora_r LORA_R]\n",
      "                                        [--lora_alpha LORA_ALPHA]\n",
      "                                        [--lora_dropout LORA_DROPOUT]\n",
      "                                        [--logging_steps LOGGING_STEPS]\n",
      "                                        [--project_name PROJECT_NAME]\n",
      "                                        [--evaluation_strategy EVALUATION_STRATEGY]\n",
      "                                        [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                                        [--save_strategy SAVE_STRATEGY]\n",
      "                                        [--auto_find_batch_size] [--fp16]\n",
      "                                        [--push_to_hub] [--use_int8]\n",
      "                                        [--model_max_length MODEL_MAX_LENGTH]\n",
      "                                        [--repo_id REPO_ID] [--use_int4]\n",
      "                                        [--trainer TRAINER]\n",
      "                                        [--target_modules TARGET_MODULES]\n",
      "                                        [--merge_adapter] [--token TOKEN]\n",
      "                                        [--backend BACKEND]\n",
      "                                        [--username USERNAME]\n",
      "                                        [--use_flash_attention_2] [--log LOG]\n",
      "                                        [--disable_gradient_checkpointing]\n",
      "                                        [--dpo-beta DPO_BETA]\n",
      "autotrain <command> [<args>] llm: error: argument --project_name/--project-name: expected one argument\n"
     ]
    }
   ],
   "source": [
    "!source ../.venv/bin/activate\n",
    "# !autotrain llm --train --project_name my-llm --model TinyLlama/TinyLlama-1.1B-Chat-v0.1 --data_path . --use-peft --use_int4 --learning_rate 2e-4 --train_batch_size 6 --num_train_epochs 3 --trainer sft\n",
    "\n",
    "# The training dataset to be used must be called training.csv and be located in the data_path folder.\n",
    "!autotrain llm --train \\\n",
    "    --project_name ./llms/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic-100 \\\n",
    "    --model TinyLlama/TinyLlama-1.1B-Chat-v0.1 \\\n",
    "    --data_path . \\\n",
    "    --use-peft \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --train_batch_size 6 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --trainer sft \\\n",
    "    --push_to_hub \\\n",
    "    --repo_id ai-aerospace/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic-100 \\\n",
    "    --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
