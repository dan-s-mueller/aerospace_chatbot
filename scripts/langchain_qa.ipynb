{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page setup and execution for the aerospace mechanism chatbot\n",
    "Example :        \n",
    "-What can you tell me about latch mechanism design failures which have occurred        \n",
    "-Follow up: Which one of the sources discussed volatile spherical joint interfaces           \n",
    "\"\"\"\n",
    "# import databutton as db\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import data_import\n",
    "import queries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import streamlit as st\n",
    "import openai\n",
    "import secrets\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check api keys\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# print(os.getenv('PINECONE_ENVIRONMENT'))\n",
    "# print(os.getenv('PINECONE_API_KEY'))\n",
    "# print(os.getenv('HUGGING_FACE_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_TRACING_V2'))\n",
    "# print(os.getenv('LANGCHAIN_ENDPOINT'))\n",
    "# print(os.getenv('LANGCHAIN_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_PROJECT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeable fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_level=None   # Max token limit, see section below\n",
    "k=10 # Number of queries to return\n",
    "search_type='similarity'    #  'mmr' or 'similarity'\n",
    "temperature=0\n",
    "verbose=True\n",
    "chain_type='stuff'  # 'stuff' or  'map_reduce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some items\n",
    "if output_level==\"Concise\":\n",
    "    max_generated_tokens=50\n",
    "elif output_level==\"Detailed\":\n",
    "    max_generated_tokens=516\n",
    "else:\n",
    "    # max_generated_tokens = -1\n",
    "    max_generated_tokens=None\n",
    "\n",
    "# Track filtering, set history to be blank\n",
    "message_id=0\n",
    "filter_toggle=False # Filter sources on last answer\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT')\n",
    ")\n",
    "index_name = 'canopy--ams'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openai Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voyage embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the cells below to get an llm that plugs into the prompts section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprietary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openai\n",
    "Models: https://platform.openai.com/docs/models/gpt-3-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# model_name='gpt-3.5-turbo-instruct'\n",
    "model_name='gpt-3.5-turbo-1106' # 16,385 tokens\n",
    "# model_name='gpt-4-0613'\n",
    "\n",
    "llm = ChatOpenAI(temperature=temperature,\n",
    "             model_name=model_name,\n",
    "             max_tokens=max_generated_tokens,\n",
    "             tags=[model_name+'-'+str(temperature)+'-'+str(max_generated_tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama (local, works on mac only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "model_name='llama2:latest'\n",
    "llm = Ollama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(\"the first man on the moon was...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llama.cpp (local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but is really limited and difficult to scale. Recommend using the run via API in the next section.\n",
    "* Try this: https://replicate.com/blog/run-llama-locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_id='google/flan-t5-small'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n",
    "# # model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False, device_map='auto')\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# pipeline = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=model, \n",
    "    \n",
    "#     tokenizer=tokenizer, \n",
    "#     max_length=128\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    }
   ],
   "source": [
    "# print(llm('What is the capital of England?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging face models (via API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# model_id='google/flan-t5-xxl'\n",
    "model_id='WizardLM/WizardLM-70B-V1.0'\n",
    "# model_id='meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_id, \n",
    "                     model_kwargs={\"temperature\": temperature}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['Prompt 1: What types of lubricants are to be avoided when designing space mechanisms?',\n",
    "        'Prompt 2: Can you speak to what failures have occurred when using mineral oil lubricants?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt 1: What types of lubricants are to be avoided when designing space mechanisms?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Perfluoropolyethers (PFPEs) and multiply alkylated cyclopentanes (MAC) are heritage lubricants used in space applications. They both have benefits and drawbacks; the main benefit being outstanding resistance to outgassing, but their tribofilm forming properties are problematic. PFPE forms iron fluourides in tribocontacts, which prevents seizure but eventually degrades the system autocatalytically. MAC on the other hand is a neat hydrocarbon, and is not generally tribochemically active. Additives are possible, but finding effective additives that are miscible and non-volatile is challenging, and few options are currently available. Therefore, these types of lubricants are to be avoided when designing space mechanisms.\n",
       "SOURCES: AMS_2020.pdf, AMS_2020.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document_id': 'AMS_1998.pdf_31107727471-051f-4979-8b7b-ff5aafb8233d', 'page': '311', 'source': 'AMS_1998.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_2684dd10ab7-7371-4b76-9347-8e22cb98ea41', 'page': '268', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_2655d737dec-85ef-4c92-a748-5606ab9cf2bc', 'page': '265', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_308b4ceecb7-7783-48c3-9975-62fd28ebe114', 'page': '308', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_297f3e14198-e3a0-4689-989e-a3dee241f0ae', 'page': '297', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_1997.pdf_289ab65038c-470e-4a9f-b81f-bc837a90ff37', 'page': '289', 'source': 'AMS_1997.pdf'},\n",
      " {'document_id': 'AMS_2004.pdf_288186c6849-fdc1-4229-b4af-9017670e8110', 'page': '288', 'source': 'AMS_2004.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_52786cdb6fb-8d6a-444e-a0db-b1ed3e0f2602', 'page': '527', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2018.pdf_90422f64ac-3adc-48fa-adf3-44bbf647e037', 'page': '90', 'source': 'AMS_2018.pdf'},\n",
      " {'document_id': 'AMS_2018.pdf_544b6b60330-1c55-4026-a420-cd41a5c8aa5a', 'page': '544', 'source': 'AMS_2018.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# Set up object\n",
    "tags=['prompt1']\n",
    "qa_model_obj=queries.QA_Model(index_name,\n",
    "                    embeddings_model,\n",
    "                    llm,\n",
    "                    k,\n",
    "                    search_type,\n",
    "                    verbose,\n",
    "                    filter_arg=filter_toggle)\n",
    "\n",
    "# Generate a response using your chat model\n",
    "qa_model_obj.query_docs(prompt[0],tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[0]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt 2: Can you speak to what failures have occurred when using mineral oil lubricants?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Failures that have occurred when using mineral oil lubricants include dewetting problems, volatility issues, and degradation of the anti-wear additives. There have also been concerns about bearing lifetimes due to the use of new, unqualified ODC-free cleaning techniques. Additionally, mineral oils have wide molecular weight distributions and are susceptible to autocatalytic degradation. The lead additives have also performed poorly in vacuum bearing tests, and there have been issues with lubricant breakdown due to a chemical reaction with moisture. Finally, there have been instances of conservative temperature prediction leading to a premature loss of lubrication.\n",
       "SOURCES: AMS_1998.pdf, AMS_2020.pdf, AMS_2016.pdf, AMS_1999.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document_id': 'AMS_1998.pdf_31107727471-051f-4979-8b7b-ff5aafb8233d', 'page': '311', 'source': 'AMS_1998.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_320e5ad8809-aac0-43e1-81c0-38b1d743aab4', 'page': '320', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_4159fe497be-d2d6-4adf-a041-0e7aed806f63', 'page': '415', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_2020.pdf_3257ab5ab61-246b-483a-9b8e-79c29ca1220c', 'page': '325', 'source': 'AMS_2020.pdf'},\n",
      " {'document_id': 'AMS_1999.pdf_174d5974ce7-270b-4189-95c5-4de163d6c894', 'page': '174', 'source': 'AMS_1999.pdf'},\n",
      " {'document_id': 'AMS_2016.pdf_9396788347-0781-4cb9-8062-b2ec3847bb4d', 'page': '93', 'source': 'AMS_2016.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# Generate a response using your chat model\n",
    "tags=['prompt2_nofilter']\n",
    "qa_model_obj.update_model(llm=llm,\n",
    "                          filter_arg=False)\n",
    "\n",
    "qa_model_obj.query_docs(prompt[1],\n",
    "                        tags=tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[1]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm=ChatOpenAI()\n",
    "# llm.invoke(\"Hello world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
