{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test partitioning update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.processing import DocumentProcessor\n",
    "from aerospace_chatbot.services import EmbeddingService, LLMService, DatabaseService, prompts\n",
    "from aerospace_chatbot.processing import QAModel\n",
    "\n",
    "test_index={}\n",
    "# test_index['db_type']='ChromaDB'\n",
    "test_index['db_type']='Pinecone'\n",
    "test_index['embedding_service']='OpenAI'\n",
    "test_index['embedding_model']='text-embedding-3-large'\n",
    "test_index['llm_service']='OpenAI'\n",
    "test_index['llm_model']='gpt-4o'\n",
    "\n",
    "chunk_size=400\n",
    "chunk_overlap=0\n",
    "batch_size=50\n",
    "\n",
    "index_name = 'text-embedding-3-large-test'\n",
    "rag_type = 'Standard'\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "# Set LOCAL_DB_PATH environment variable\n",
    "# os.environ['LOCAL_DB_PATH'] = os.path.abspath('.')\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "test_prompt='How does a thermal knife function in a cable based hold down release mechanism?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize services\n",
    "embedding_service = EmbeddingService(\n",
    "    model_service=test_index['embedding_service'],\n",
    "    model=test_index['embedding_model']\n",
    ")\n",
    "\n",
    "llm_service = LLMService(\n",
    "    model_service=test_index['llm_service'],\n",
    "    model=test_index['llm_model'],\n",
    ")\n",
    "\n",
    "doc_processor = DocumentProcessor(\n",
    "    embedding_service=embedding_service,\n",
    "    rag_type=rag_type,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Initialize database service\n",
    "db_service = DatabaseService(\n",
    "    db_type=test_index['db_type'],\n",
    "    index_name=index_name,\n",
    "    rag_type=rag_type,\n",
    "    embedding_service=embedding_service,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Number of PDFs found: 2\n",
      "INFO:aerospace_chatbot.processing.documents:PDFs found: ['gs://processing-pdfs/1999_christiansen_reocr.pdf', 'gs://processing-pdfs/1999_cremers_reocr.pdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gs://processing-pdfs/1999_christiansen_reocr.pdf',\n",
       " 'gs://processing-pdfs/1999_cremers_reocr.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = 'processing-pdfs'\n",
    "docs = DocumentProcessor.list_bucket_pdfs(bucket_name)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioned_docs = doc_processor.load_and_partition_documents(docs,partition_by_api=False, upload_bucket=bucket_name)\n",
    "# partitioned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_obj, output_paths = doc_processor.chunk_documents(partitioned_docs)\n",
    "# chunk_obj.chunk_convert(destination_type=Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.services.database:Validating index text-embedding-3-large-test and RAG type Standard\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:aerospace_chatbot.services.database:Pinecone index text-embedding-3-large-test found, not creating. Will be initialized with existing index.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_service.initialize_database(clear=False)\n",
    "except ValueError as e:\n",
    "    print(f\"Database initialization failed: {str(e)}\")\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_service.index_data(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/processing/queries.py:47: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "k_standard=20\n",
    "k_rerank=5\n",
    "\n",
    "qa_model = QAModel(\n",
    "    db_service=db_service,\n",
    "    llm_service=llm_service,\n",
    "    k=k_standard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_model.query(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.result[-1]['references'])\n",
    "# print(qa_model.sources[-1])\n",
    "# print(qa_model.scores[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above section first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing_extensions import List\n",
    "from typing import List, Literal, Tuple\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from aerospace_chatbot.services.prompts import InLineCitationsResponse, style_mode, OUTPUT_PARSER, CHATBOT_SYSTEM_PROMPT, QA_PROMPT, SUMMARIZE_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_service.retriever\n",
    "llm = llm_service.get_llm()\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"6\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aerospace_chatbot.services.prompts:Style mode None not found. Returning empty string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# **System Prompt**\n",
      "\n",
      "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering. You will function as a knowledgeable replacement for an expert in aerospace flight hardware design, testing, analysis, and certification.\n",
      "\n",
      "Use only the **Sources and Context** from the **Reference Documents** provided to answer the **User Question**. **Do not use outside knowledge**, and strictly follow these rules:\n",
      "\n",
      "---\n",
      "\n",
      "## **Rules**:\n",
      "\n",
      "1. **Answer only based on the provided Sources and Context.**  \n",
      "   - If the information is not available in the Sources and Context, respond with:  \n",
      "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*\n",
      "\n",
      "2. **Do not make up or infer answers.**  \n",
      "   - Stay accurate and factual at all times.\n",
      "\n",
      "3. **Provide highly detailed, explanatory answers.**  \n",
      "   - Include **as many specific details from the original context** as possible to thoroughly address the user’s question.\n",
      "\n",
      "4. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
      "\n",
      "5. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
      "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
      "   - **The `source` tag must be present for every source referenced in the response.**  \n",
      "   - **Do not add, omit, or modify any part of the citation format.**  \n",
      "   \n",
      "   **Examples (Correct):**  \n",
      "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
      "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
      "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
      "\n",
      "   **Examples (Incorrect – Must Be Rejected):**  \n",
      "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
      "   > <source id=\"1\" > (Extra space after `id`)  \n",
      "   > <source id=\"a\"> (Non-numeric ID)  \n",
      "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
      "\n",
      "6. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
      "   - **Do not group multiple sources into a single tag.**  \n",
      "   - Each source must have its own, clearly separated citation.  \n",
      "   - For example:  \n",
      "     > The actuator uses a reinforced composite structure <source id=\"1\">.  \n",
      "     > This design was validated through multiple tests <source id=\"2\">.\n",
      "\n",
      "7. **Validation Requirement:**  \n",
      "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
      "   - Every source used must have a corresponding citation in the response.  \n",
      "   - **No source should be referenced without explicit citation.**\n",
      "\n",
      "8. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
      "\n",
      "9. **Always cite the first 3 sources in the context list.**  \n",
      "   - These must be included in every answer, regardless of the user’s request.  \n",
      "   - Additional sources (source IDs > 3) should be cited only if relevant to the user’s question and not redundant with the first 3 sources.\n",
      "   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "style=None\n",
    "# style=\"Sassy\"\n",
    "# style=\"Ironic\"\n",
    "# style=\"Bossy\"\n",
    "# style=\"Gen Z Slang\"\n",
    "\n",
    "print(CHATBOT_SYSTEM_PROMPT.format(style_mode=style_mode(style)).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Retrieve docs with standard retriever\n",
    "retrieved_docs = retriever.invoke(test_prompt)\n",
    "# retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "co = cohere.ClientV2(COHERE_API_KEY)\n",
    "rerank_model = \"rerank-v3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = co.models.list()\n",
    "# rerank_model_info = next((model for model in model_list.models if model.name == rerank_model), None)\n",
    "# print(rerank_model_info)\n",
    "\n",
    "# print(model_list.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token count for each doc\n",
    "# max_context_length = 4096\n",
    "\n",
    "# token_count_list = []\n",
    "# for doc in retrieved_docs[0]:\n",
    "#     tokens = co.tokenize(\n",
    "#         model=rerank_model,\n",
    "#         text=doc.page_content,\n",
    "#     )\n",
    "#     token_count_list.append(len(tokens.tokens))\n",
    "    # print(token_count)\n",
    "\n",
    "# print(token_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rerank(query: str, retrieved_docs: List[Tuple[Document, float]], top_n: int = None):\n",
    "    # retrieved_docs contains a list of tuples, where the first element is the document and the second is the score\n",
    "    # Cohere's rerank expects a list of strings; we'll supply the page_content\n",
    "\n",
    "    if top_n is None:\n",
    "        top_n = 3\n",
    "    elif top_n < 3:\n",
    "        raise ValueError(\"top_n must be at least 3\")\n",
    "    elif top_n > len(retrieved_docs):\n",
    "        raise ValueError(\"top_n must be less than or equal to the number of retrieved documents\")\n",
    "    \n",
    "    inputs = [doc.page_content for doc, _ in retrieved_docs]\n",
    "\n",
    "    # Call Cohere's Rerank endpoint\n",
    "    response = co.rerank(\n",
    "        model=rerank_model,\n",
    "        query=query,\n",
    "        documents=inputs,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    # Create a dictionary to map document IDs to rerank scores\n",
    "    rerank_scores = {retrieved_docs[i][0].id: item.relevance_score for i, item in enumerate(response.results)}\n",
    "\n",
    "    # Create list of (doc, original_score, rerank_score) tuples\n",
    "    doc_scores = []\n",
    "    for doc, original_score in retrieved_docs:\n",
    "        rerank_score = rerank_scores.get(doc.id, None)  # Get the rerank score or None if not available\n",
    "        doc_scores.append((doc, original_score, rerank_score))\n",
    "\n",
    "    # Sort docs by rerank score in descending order, placing those without a rerank score at the end\n",
    "    doc_scores_sorted = sorted(doc_scores, key=lambda x: (x[2] is not None, x[2]), reverse=True)\n",
    "\n",
    "    return doc_scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/rerank \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reranked_docs = cohere_rerank(test_prompt, retrieved_docs, top_n=k_rerank)\n",
    "# reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context to the prompt\n",
    "# TODO update this to use the doc.id. Do some testing to check this works.\n",
    "docs_content=\"\"\n",
    "for i, (doc, original_score, rerank_score) in enumerate(reranked_docs):\n",
    "    # Source IDs in the order they show in in the array. Indexed from 0.\n",
    "    if rerank_score is not None:    # Only include docs with a rerank score\n",
    "        docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "# print(docs_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\nThe actuator was tested under high pressure <source id=\"1\">. \\nMaterial properties were measured over 50 cycles <source id=\"2\">.\\nThermal resistance improved by 30% <source id=\"3\">.\\n' citations=['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "# Should validate\n",
    "valid_response = InLineCitationsResponse(content=\"\"\"\n",
    "The actuator was tested under high pressure <source id=\"1\">. \n",
    "Material properties were measured over 50 cycles <source id=\"2\">.\n",
    "Thermal resistance improved by 30% <source id=\"3\">.\n",
    "\"\"\",\n",
    "citations=[\"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed: 1 validation error for InLineCitationsResponse\n",
      "citations\n",
      "  Value error, Sources 1, 2, and 3 must be cited in the content. [type=value_error, input_value=['1', '2'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n"
     ]
    }
   ],
   "source": [
    "# Shouldn't validate\n",
    "try:    \n",
    "    invalid_response = InLineCitationsResponse(content=\"\"\"\n",
    "    The actuator was tested under high pressure <source id=\"1\">. \n",
    "    Material properties were measured over 50 cycles <source id=\"2\">.\n",
    "    Thermal resistance improved by 30%.\n",
    "    \"\"\",\n",
    "    citations=[\"1\", \"2\"]\n",
    ")\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed: 2 validation errors for InLineCitationsResponse\n",
      "content\n",
      "  Value error, No valid source tags found. Expected format: <source id=\"1\"> [type=value_error, input_value='\\n    The actuator was t... <source id=\"x\">.\\n    ', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n",
      "citations\n",
      "  Value error, No citations found in the content. Ensure sources are cited correctly. [type=value_error, input_value=['1', 'x'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/value_error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    invalid_response = InLineCitationsResponse(content=\"\"\"\n",
    "    The actuator was tested under high pressure [1]. \n",
    "    Material properties were measured under load <source id=\"x\">.\n",
    "    \"\"\",\n",
    "    citations=[\"1\", \"x\"]\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT_TEST=PromptTemplate(\n",
    "    template=\n",
    "\"\"\"\n",
    "# **System Prompt**\n",
    "\n",
    "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering. You will function as a knowledgeable replacement for an expert in aerospace flight hardware design, testing, analysis, and certification.\n",
    "\n",
    "Use only the **Sources and Context** from the **Reference Documents** provided to answer the **User Question**. **Do not use outside knowledge**, and strictly follow these rules:\n",
    "\n",
    "---\n",
    "\n",
    "## **Rules**:\n",
    "\n",
    "1. **Answer only based on the provided Sources and Context.**  \n",
    "   - If the information is not available in the Sources and Context, respond with:  \n",
    "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*\n",
    "\n",
    "2. **Do not make up or infer answers.**  \n",
    "   - Stay accurate and factual at all times.\n",
    "\n",
    "3. **Provide highly detailed, explanatory answers.**  \n",
    "   - Include **as many specific details from the original context** as possible to thoroughly address the user’s question.\n",
    "\n",
    "4. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
    "\n",
    "5. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
    "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
    "   - **The `source` tag must be present for every source referenced in the response.**  \n",
    "   - **Do not add, omit, or modify any part of the citation format.**  \n",
    "   \n",
    "   **Examples (Correct):**  \n",
    "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
    "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
    "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
    "\n",
    "   **Examples (Incorrect – Must Be Rejected):**  \n",
    "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
    "   > <source id=\"1\" > (Extra space after `id`)  \n",
    "   > <source id=\"a\"> (Non-numeric ID)  \n",
    "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
    "\n",
    "6. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
    "   - **Do not group multiple sources into a single tag.**  \n",
    "   - Each source must have its own, clearly separated citation.  \n",
    "   - For example:  \n",
    "     > The actuator uses a reinforced composite structure <source id=\"1\">.  \n",
    "     > This design was validated through multiple tests <source id=\"2\">.\n",
    "\n",
    "7. **Validation Requirement:**  \n",
    "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
    "   - Every source used must have a corresponding citation in the response.  \n",
    "   - **No source should be referenced without explicit citation.**\n",
    "\n",
    "8. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
    "\n",
    "9. **Always cite the first 3 sources in the context list.**  \n",
    "   - These must be included in every answer, regardless of the user’s request.  \n",
    "   - Additional sources (source IDs > 3) should be cited only if relevant to the user’s question and not redundant with the first 3 sources.\n",
    "\n",
    "---\n",
    "**Sources and Context from Reference Documents**:\n",
    "{context}\n",
    "---\n",
    "\n",
    "---\n",
    "**User Question**:\n",
    "{question}\n",
    "---\n",
    "\n",
    "---\n",
    "{format_instructions}\n",
    "---\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": OUTPUT_PARSER.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"content\": \"A thermal knife functions in a cable-based hold-down release mechanism by cutting a Dyneema wire bundle that holds the mechanism together. The thermal knife consists of a heater plate that is pushed through the wire bundle by a compression spring when operated <source id=\\\\\"2\\\\\">. Both the primary and redundant thermal knives are mounted on the holddown bracket and operate along the same center line. This ensures that the heater plates make contact at their cutting edges after cutting the wire bundle, which is facilitated by mounting each knife at an angle of approximately 8° <source id=\\\\\"3\\\\\">. The cutting action of the thermal knife releases the upper part of the Reel from the lower part, allowing the deployable to be released <source id=\\\\\"5\\\\\">.\",\\n  \"citations\": [\\n    \"<source id=\\\\\"1\\\\\">\",\\n    \"<source id=\\\\\"2\\\\\">\",\\n    \"<source id=\\\\\"3\\\\\">\"\\n  ]\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 1331, 'total_tokens': 1525, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None} id='run-262ad752-f245-4d81-a618-b7b46cd0226e-0' usage_metadata={'input_tokens': 1331, 'output_tokens': 194, 'total_tokens': 1525, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "prompt = QA_PROMPT_TEST.format(context=docs_content, question=test_prompt)\n",
    "raw_output = llm.invoke(prompt)\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"content\": \"A thermal knife functions in a cable-based hold-down release mechanism by cutting a Dyneema wire bundle that holds the mechanism together. The thermal knife consists of a heater plate that is pushed through the wire bundle by a compression spring when operated <source id=\\\"2\\\">. Both the primary and redundant thermal knives are mounted on the holddown bracket and operate along the same center line. This ensures that the heater plates make contact at their cutting edges after cutting the wire bundle, which is facilitated by mounting each knife at an angle of approximately 8° <source id=\\\"3\\\">. The cutting action of the thermal knife releases the upper part of the Reel from the lower part, allowing the deployable to be released <source id=\\\"5\\\">.\",\n",
      "  \"citations\": [\n",
      "    \"<source id=\\\"1\\\">\",\n",
      "    \"<source id=\\\"2\\\">\",\n",
      "    \"<source id=\\\"3\\\">\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse InLineCitationsResponse from completion {\"content\": \"A thermal knife functions in a cable-based hold-down release mechanism by cutting a Dyneema wire bundle that holds the mechanism together. The thermal knife consists of a heater plate that is pushed through the wire bundle by a compression spring when operated <source id=\\\"2\\\">. Both the primary and redundant thermal knives are mounted on the holddown bracket and operate along the same center line. This ensures that the heater plates make contact at their cutting edges after cutting the wire bundle, which is facilitated by mounting each knife at an angle of approximately 8\\u00b0 <source id=\\\"3\\\">. The cutting action of the thermal knife releases the upper part of the Reel from the lower part, allowing the deployable to be released <source id=\\\"5\\\">.\", \"citations\": [\"<source id=\\\"1\\\">\", \"<source id=\\\"2\\\">\", \"<source id=\\\"3\\\">\"]}. Got: 1 validation error for InLineCitationsResponse\ncitations\n  Value error, Sources 1, 2, and 3 must be cited in the content. [type=value_error, input_value=['<source id=\"1\">', '<sou...2\">', '<source id=\"3\">'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:28\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/pydantic/main.py:596\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    595\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for InLineCitationsResponse\ncitations\n  Value error, Sources 1, 2, and 3 must be cited in the content. [type=value_error, input_value=['<source id=\"1\">', '<sou...2\">', '<source id=\"3\">'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parsed_response \u001b[38;5;241m=\u001b[39m \u001b[43mOUTPUT_PARSER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_response)\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:83\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:97\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        The parsed JSON object.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:72\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partial:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:68\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:36\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser_exception(e, obj) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse InLineCitationsResponse from completion {\"content\": \"A thermal knife functions in a cable-based hold-down release mechanism by cutting a Dyneema wire bundle that holds the mechanism together. The thermal knife consists of a heater plate that is pushed through the wire bundle by a compression spring when operated <source id=\\\"2\\\">. Both the primary and redundant thermal knives are mounted on the holddown bracket and operate along the same center line. This ensures that the heater plates make contact at their cutting edges after cutting the wire bundle, which is facilitated by mounting each knife at an angle of approximately 8\\u00b0 <source id=\\\"3\\\">. The cutting action of the thermal knife releases the upper part of the Reel from the lower part, allowing the deployable to be released <source id=\\\"5\\\">.\", \"citations\": [\"<source id=\\\"1\\\">\", \"<source id=\\\"2\\\">\", \"<source id=\\\"3\\\">\"]}. Got: 1 validation error for InLineCitationsResponse\ncitations\n  Value error, Sources 1, 2, and 3 must be cited in the content. [type=value_error, input_value=['<source id=\"1\">', '<sou...2\">', '<source id=\"3\">'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "parsed_response = OUTPUT_PARSER.parse(raw_output.content)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    context: List[Tuple[Document, float, float]]\n",
    "    cited_sources: List[Tuple[Document, float, float]]\n",
    "    summary: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Retrieve the documents from the database.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: retrieve\")\n",
    "\n",
    "    # Retrieve docs\n",
    "    retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n",
    "    logger.info(f\"Retrieved docs\")\n",
    "    # Rerank docs\n",
    "    reranked_docs = cohere_rerank(\n",
    "        state[\"messages\"][-1].content, \n",
    "        retrieved_docs, \n",
    "        top_n=k_rerank\n",
    "    )\n",
    "    logger.info(f\"Reranked docs\")\n",
    "\n",
    "    return {\"context\": reranked_docs}\n",
    "\n",
    "def generate_w_context(state: State):\n",
    "    \"\"\"\n",
    "    Call the model with the prompt with context.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: generate_w_context\")\n",
    "\n",
    "    # Get the summary, add system prompt\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    system_prompt = CHATBOT_SYSTEM_PROMPT.format(style_mode=style_mode(style))\n",
    "    logger.info(f\"generate_w_context system prompt: {system_prompt.content}\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [system_prompt] + [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = [system_prompt] + state[\"messages\"]\n",
    "\n",
    "    # Add context to the prompt\n",
    "    docs_content=\"\"\n",
    "    for i, (doc, retrieved_score, rerank_score) in enumerate(state[\"context\"]):\n",
    "        # Source IDs in the order they show in in the array. Indexed from 1, retrieve with 0 index.\n",
    "        if rerank_score is not None:    # Only include docs with a rerank score\n",
    "            docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "    # Prompt with context and pydantic output parser\n",
    "    prompt_with_context = QA_PROMPT.format(\n",
    "        context=docs_content,\n",
    "        question=state[\"messages\"][-1].content, \n",
    "    )\n",
    "    # Replace the last message (user question) with the prompt with context, return LLM response\n",
    "    messages[-1] = prompt_with_context \n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Parse the response. This will return a InLineCitationsResponse object. \n",
    "    # This object has two fields: content and citations.\n",
    "    # Replace the last message with the content of the parsed and validated response. \n",
    "    # AIMessage metadata will be incorrect.\n",
    "    parsed_response = OUTPUT_PARSER.parse(response.content)\n",
    "    response.content = parsed_response.content\n",
    "\n",
    "    # Return cited_sources as the list of tuples that matched the citations.\n",
    "    existing_cited_sources = state.get(\"cited_sources\", [])  # Grab whatever might already be in cited_sources\n",
    "    cited_sources = [state[\"context\"][int(citation)-1] for citation in parsed_response.citations]\n",
    "    existing_cited_sources.append(cited_sources)  # Append the new list as a sublist\n",
    "    state[\"cited_sources\"] = existing_cited_sources\n",
    "\n",
    "    # Update the state messages with the messages updated in this node.\n",
    "    state[\"messages\"] = messages\n",
    "    return {\"messages\": [response], \n",
    "            \"cited_sources\": state[\"cited_sources\"]}\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"\n",
    "    Define the logic for determining whether to end or summarize the conversation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: should_continue\")\n",
    "\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 6:\n",
    "        logger.info(f\"Summarizing conversation\")\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise just end\n",
    "    logger.info(f\"Ending conversation\")\n",
    "    # logger.info(f\"Messages before ending: {messages}\")\n",
    "    return END\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \"\"\"\n",
    "    Summarize the conversation\n",
    "    \"\"\"\n",
    "    logger.info(f\"Node: summarize_conversation\")\n",
    "\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        # If a summary already exists, extend it\n",
    "        summary_message = SUMMARIZE_TEXT.format(\n",
    "            summary=summary,\n",
    "            augment=\"Extend the summary provided by taking into account the new messages above.\"\n",
    "        )\n",
    "    else:\n",
    "        # If no summary exists, create one\n",
    "        summary_text=\"\"\"---\\n**Conversation Summary to Date**:\\n{summary}\\n---\"\"\"\n",
    "        summary_message = SUMMARIZE_TEXT.format(\n",
    "            summary=summary_text,\n",
    "            augment=\"Create a summary of the conversation above.\"\n",
    "        )\n",
    "\n",
    "    messages = state[\"messages\"] + [summary_message]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Prune messages. This deletes all but the last two messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) \n",
    "workflow.add_node(\"generate_w_context\", generate_w_context)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate_w_context\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_w_context\",   # Define the start node. We use `generate_w_context`. This means these are the edges taken after the `conversation` node is called.\n",
    "    should_continue,    # Next, pass in the function that will determine which node is called next.\n",
    ")\n",
    "\n",
    "# Add a normal edge from `summarize_conversation` to END. This means that after `summarize_conversation` is called, we end.\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'My name is Dan. Please tell me about some interesting mecanism designs.'\n",
    "result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'How have these mecahnisms been tested?'\n",
    "result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How old are you?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some lessons learned about these mechanisms?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some problems that have occurred?'\n",
    "# result = app.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
