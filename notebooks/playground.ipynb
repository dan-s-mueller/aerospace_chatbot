{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test bug on RAGxplorer on query\n",
    "Without brackets around the query in the openai client with chroma, embeddings are created for each word, not each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ragxplorer import RAGxplorer\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "import json\n",
    "import chromadb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What are examples of lubricants which should be avoided for space mechanism applications?\"\n",
    "\n",
    "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
    "embedding_model='text-embedding-ada-002'\n",
    "\n",
    "data_path='../data/AMS/'\n",
    "pdf='AMS_2022.pdf'\n",
    "\n",
    "index_path='../db/chromadb/'\n",
    "index_name='chromadb-openai-ams'\n",
    "\n",
    "viz_data='../data/AMS/ams_data-400-0-50.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(viz_data, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "index_name=data['visualization_index_name']\n",
    "umap_params=data['umap_params']\n",
    "viz_data=pd.read_json(data['viz_data'], orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client = RAGxplorer(embedding_model=embedding_model)\n",
    "chroma_client = chromadb.PersistentClient(path='../db'+'/chromadb/')\n",
    "collection=chroma_client.get_collection(name=index_name,embedding_function=rx_client._chosen_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client._query.original_query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_client.load_chroma(collection,\n",
    "                     umap_params=umap_params,\n",
    "                     initialize_projector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of these do the same thing, tested\n",
    "\n",
    "query_embedding_chroma=rx_client._chosen_embedding_model([rx_client._query.original_query])\n",
    "# query_embedding=rx_client._vectordb._embedding_function(rx_client._query.original_query)\n",
    "# query_embedding=embedding_function(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rx_client._query.original_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = rx_client.visualize_query(query,\n",
    "                                import_projection_data=viz_data,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Done correctly, see here: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "openai_embedding=client.embeddings.create(input = [query], model=embedding_model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_file = '../config/config.json'\n",
    "with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        databases = {db['name']: db for db in config['databases']}\n",
    "        llms  = {m['name']: m for m in config['llms']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_models=llms['Hugging Face']['models']\n",
    "model_names= [item['model'] for item in hf_models]\n",
    "model_endpoints= [item['endpoint'] for item in hf_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "endpoint = next(item['endpoint'] for item in hf_models if item['model'] == model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "# from langchain_community.llms import HuggingFaceTextGenInference\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#                     model_name=\"tgi\",\n",
    "#                     openai_api_key=api_key,\n",
    "#                     openai_api_base=endpoint + \"/v1/\",\n",
    "#                 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = HuggingFaceTextGenInference(\n",
    "#     inference_server_url=endpoint,\n",
    "#     # max_new_tokens=512,\n",
    "#     # top_k=10,\n",
    "#     # top_p=0.95,\n",
    "#     # typical_p=0.95,\n",
    "#     temperature=0.01,\n",
    "#     # repetition_penalty=1.03,\n",
    "# )\n",
    "# llm(\"What did foo say about bar?\")\n",
    "\n",
    "# model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name,\n",
    "                     model_kwargs={\"temperature\": 0.1, \"max_length\": 516})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "---\n",
    "Your name is Aerospace Chatbot. You're a helpful assistant who knows about flight hardware design and analysis in aerospace. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Include sources from the chat history in the standalone question created.\n",
    "---\n",
    "\n",
    "Chat History:\n",
    "\n",
    "User Question: What are examples of lubricants which should be avoided for space mechanism applications?\n",
    "Standalone Question:\n",
    "\"\"\"\n",
    "\n",
    "response=llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import itertools\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import VoyageEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Assuming `/home/user/projects/my_modules` is the path to the folder with your modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "\n",
    "\n",
    "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
    "VOYAGE_API_KEY=os.getenv('VOYAGE_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "PINECONE_API_KEY=os.getenv('PINECONE_API_KEY')\n",
    "LOCAL_DB_PATH='.'   # Default to the test path for easy cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm={'OpenAI':ChatOpenAI(model_name='gpt-3.5-turbo-1106', # Openai\n",
    "                                openai_api_key=OPENAI_API_KEY,\n",
    "                                max_tokens=500), \n",
    "            'Hugging Face':ChatOpenAI(base_url='https://api-inference.huggingface.co/v1',  # Hugging face\n",
    "                                    model='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "                                    api_key=HUGGINGFACEHUB_API_TOKEN,\n",
    "                                    max_tokens=500)}\n",
    "query_model={'OpenAI':OpenAIEmbeddings(model='text-embedding-ada-002',openai_api_key=OPENAI_API_KEY),\n",
    "                    'Voyage':VoyageEmbeddings(voyage_api_key=VOYAGE_API_KEY),\n",
    "                    'RAGatouille':'colbert-ir/colbertv2.0'}\n",
    "index_type = {index: index for index in ['ChromaDB', 'Pinecone', 'RAGatouille']}\n",
    "rag_type = {rag: rag for rag in ['Standard','Parent-Child','Summary']}\n",
    "\n",
    "def permute_tests(test_data):\n",
    "    \"\"\"\n",
    "    Permute test data to generate all possible combinations.\n",
    "    Data is in a list of dicts, where each has keys to be iterated. \n",
    "    Example: [{'index_type': ['ChromaDB'], 'rag_type': ['Standard', 'Parent-Child', 'Summary']}, \n",
    "              {'index_type': [Pinecone], 'rag_type': ['Standard', 'Parent-Child']}]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for row_data in test_data:\n",
    "        keys = row_data.keys()\n",
    "        values = row_data.values()\n",
    "        permutations = list(itertools.product(*values))\n",
    "        for perm in permutations:\n",
    "            row = dict(zip(keys, perm))\n",
    "            rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the set of cases to screen\n",
    "test_cases=[{\n",
    "    # Tests ChromaDB setups, advanced RAG (standard/parent-child)\n",
    "    'index_type': [index_type['ChromaDB']],\n",
    "    'query_model': [query_model['OpenAI']],\n",
    "    'rag_type': [rag_type['Standard'],rag_type['Parent-Child']],\n",
    "    'llm': [llm['Hugging Face']]\n",
    "},\n",
    "{\n",
    "    # Tests advanced RAG (summary) and LLM (openai/hugging face)\n",
    "    'index_type': [index_type['ChromaDB']],\n",
    "    'query_model': [query_model['OpenAI']],\n",
    "    'rag_type': [rag_type['Summary']],\n",
    "    'llm': [llm['OpenAI'],llm['Hugging Face']]\n",
    "},\n",
    "{\n",
    "    # Tests Pinecone setups, embedding types (openai/voyage)\n",
    "    'index_type': [index_type['Pinecone']],\n",
    "    'query_model': [query_model['OpenAI'],query_model['Voyage']],\n",
    "    'rag_type': [rag_type['Standard']],\n",
    "    'llm': [llm['Hugging Face']]\n",
    "},\n",
    "{\n",
    "    # Tests RAGatouille setup\n",
    "    'index_type': [index_type['RAGatouille']],\n",
    "    'query_model': [query_model['RAGatouille']],\n",
    "    'rag_type': [rag_type['Standard']],\n",
    "    'llm': [llm['Hugging Face']]\n",
    "}]\n",
    "tests=permute_tests(test_cases)\n",
    "\n",
    "# Export the output with keys\n",
    "keys = test_cases[0].keys()\n",
    "output = [{**test, 'id': i+1, 'keys': keys} for i, test in enumerate(tests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=output[0]['llm']\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
