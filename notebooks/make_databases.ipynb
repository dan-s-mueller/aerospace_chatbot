{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import admin\n",
    "import data_processing\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to split chunking from upserting\n",
    "def create_upsert(index_type,index_name,query_model,rag_type,chunker,summary_llm):\n",
    "    # Set index names for special databases\n",
    "    if rag_type == 'Parent-Child':\n",
    "        index_name = index_name + '-parent-child'\n",
    "    if rag_type == 'Summary':\n",
    "        index_name = index_name + '-' + summary_llm.model_name.replace('/', '-').replace(' ','-').lower() + '-summary' \n",
    "\n",
    "    try:\n",
    "        vectorstore = data_processing.initialize_database(index_type, \n",
    "                                            index_name, \n",
    "                                            query_model,\n",
    "                                            rag_type=rag_type,\n",
    "                                            clear=True, \n",
    "                                            local_db_path=os.getenv('LOCAL_DB_PATH'),\n",
    "                                            init_ragatouille=True,\n",
    "                                            show_progress=False)\n",
    "        print(f\"Database {index_name} created.\")\n",
    "        vectorstore, _ = data_processing.upsert_docs(index_type, \n",
    "                                        index_name,\n",
    "                                        vectorstore,\n",
    "                                        chunker,\n",
    "                                        batch_size=400,\n",
    "                                        show_progress=False,\n",
    "                                        local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database {index_name} upserted chunks.\")\n",
    "    except Exception as e:  # If there is an error, be sure to delete the database\n",
    "        data_processing.delete_index(index_type, \n",
    "                                    index_name,\n",
    "                                    rag_type,\n",
    "                                    local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database deleted: {index_name}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets, Models, Docs, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Read setup data, assign models\n",
    "json_file_path = \"databases.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "sb={}\n",
    "query_params=setup_data['query_models']\n",
    "query_models=[]\n",
    "for model in query_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    query_models.append(admin.get_query_model(sb, secrets))\n",
    "\n",
    "llm_params=setup_data['llms']\n",
    "llms=[]\n",
    "for model in llm_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    llms.append(admin.set_llm(sb, secrets))\n",
    "\n",
    "chunk_params=setup_data['chunk_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra query types that take a long time. Add to the databases.json file\n",
    "\n",
    "{\n",
    "    \"id\": \"3\",\n",
    "    \"index_type\": \"ChromaDB\",\n",
    "    \"query_model\": \"Hugging Face\",\n",
    "    \"embedding_name\": \"Dedicated Endpoint\",\n",
    "    \"embedding_hf_endpoint\": \"https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud\"\n",
    "},\n",
    "{\n",
    "    \"id\": \"4\",\n",
    "    \"index_type\": \"RAGatouille\",\n",
    "    \"embedding_name\": \"colbert-ir/colbertv2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs\n",
    "data_folder='../data/AMS'\n",
    "docs= glob.glob(os.path.join(data_folder,'*.pdf'))   # Only get the PDFs in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All query models, standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 chunks from 6 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-2merge-0 created.\n",
      "Database text-embedding-3-large-2merge-0 upserted with 6 chunks.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2'}\n",
      "Database voyage-large-2-2merge-0 created.\n",
      "Database voyage-large-2-2merge-0 upserted with 6 chunks.\n",
      "Creating and uploading database with these params: {'id': '3', 'index_type': 'ChromaDB', 'query_model': 'Hugging Face', 'embedding_name': 'Dedicated Endpoint', 'embedding_hf_endpoint': 'https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud'}\n",
      "Database dedicated-endpoint-2merge-0 created.\n",
      "Database deleted: dedicated-endpoint-2merge-0\n",
      "Error: RetryError[<Future at 0x3303b4610 state=finished raised KeyError>]\n",
      "Creating and uploading database with these params: {'id': '4', 'index_type': 'RAGatouille', 'embedding_name': 'colbert-ir/colbertv2.0'}\n",
      "Database colbert-ir-colbertv2.0-2merge-0 created.\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jul 02, 20:21:05] #> Creating directory /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0 \n",
      "\n",
      "\n",
      "[Jul 02, 20:21:06] [0] \t\t #> Encoding 38 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:10] [0] \t\t avg_doclen_est = 184.15789794921875 \t len(local_sample) = 38\n",
      "[Jul 02, 20:21:10] [0] \t\t Creating 1,024 partitions.\n",
      "[Jul 02, 20:21:10] [0] \t\t *Estimated* 6,998 embeddings.\n",
      "[Jul 02, 20:21:10] [0] \t\t #> Saving the indexing plan to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (6649) is less than the minimum recommended (10240)\n",
      "used 18 iterations (0.336s) to cluster 6649 items into 1024 clusters\n",
      "[0.034, 0.037, 0.032, 0.032, 0.035, 0.035, 0.033, 0.033, 0.035, 0.032, 0.034, 0.034, 0.033, 0.034, 0.032, 0.037, 0.028, 0.034, 0.032, 0.034, 0.031, 0.032, 0.032, 0.037, 0.031, 0.033, 0.034, 0.034, 0.035, 0.035, 0.035, 0.041, 0.037, 0.033, 0.036, 0.032, 0.036, 0.036, 0.034, 0.041, 0.035, 0.034, 0.031, 0.037, 0.034, 0.032, 0.033, 0.037, 0.035, 0.033, 0.031, 0.037, 0.036, 0.035, 0.03, 0.037, 0.041, 0.035, 0.043, 0.034, 0.032, 0.038, 0.035, 0.034, 0.034, 0.034, 0.037, 0.04, 0.03, 0.032, 0.036, 0.034, 0.033, 0.034, 0.034, 0.033, 0.035, 0.033, 0.038, 0.035, 0.035, 0.03, 0.034, 0.034, 0.032, 0.035, 0.036, 0.032, 0.034, 0.036, 0.032, 0.036, 0.033, 0.034, 0.033, 0.036, 0.037, 0.033, 0.035, 0.035, 0.032, 0.038, 0.038, 0.038, 0.037, 0.03, 0.034, 0.031, 0.033, 0.031, 0.035, 0.035, 0.035, 0.03, 0.035, 0.032, 0.033, 0.035, 0.033, 0.033, 0.035, 0.03, 0.036, 0.036, 0.031, 0.036, 0.034, 0.031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:10] [0] \t\t #> Encoding 38 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "1it [00:02,  2.38s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1140.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:13] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 02, 20:21:13] #> Building the emb2pid mapping..\n",
      "[Jul 02, 20:21:13] len(emb2pid) = 6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 91444.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:13] #> Saved optimized IVF to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-2merge-0/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n",
      "Database colbert-ir-colbertv2.0-2merge-0 upserted with 6 chunks.\n",
      "Created 111 chunks from 12 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-0merge-400 created.\n",
      "Database text-embedding-3-large-0merge-400 upserted with 111 chunks.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2'}\n",
      "Database voyage-large-2-0merge-400 created.\n",
      "Database voyage-large-2-0merge-400 upserted with 111 chunks.\n",
      "Creating and uploading database with these params: {'id': '3', 'index_type': 'ChromaDB', 'query_model': 'Hugging Face', 'embedding_name': 'Dedicated Endpoint', 'embedding_hf_endpoint': 'https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud'}\n",
      "Database dedicated-endpoint-0merge-400 created.\n",
      "Database dedicated-endpoint-0merge-400 upserted with 111 chunks.\n",
      "Creating and uploading database with these params: {'id': '4', 'index_type': 'RAGatouille', 'embedding_name': 'colbert-ir/colbertv2.0'}\n",
      "Database colbert-ir-colbertv2.0-0merge-400 created.\n",
      "New index_name received! Updating current index_name (colbert-ir-colbertv2.0-2merge-0) to colbert-ir-colbertv2.0-0merge-400\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jul 02, 20:21:35] #> Creating directory /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-0merge-400 \n",
      "\n",
      "\n",
      "[Jul 02, 20:21:36] [0] \t\t #> Encoding 111 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:40] [0] \t\t avg_doclen_est = 55.180179595947266 \t len(local_sample) = 111\n",
      "[Jul 02, 20:21:40] [0] \t\t Creating 1,024 partitions.\n",
      "[Jul 02, 20:21:40] [0] \t\t *Estimated* 6,124 embeddings.\n",
      "[Jul 02, 20:21:40] [0] \t\t #> Saving the indexing plan to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-0merge-400/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (5819) is less than the minimum recommended (10240)\n",
      "used 15 iterations (0.1901s) to cluster 5819 items into 1024 clusters\n",
      "[0.04, 0.041, 0.035, 0.037, 0.038, 0.042, 0.038, 0.037, 0.037, 0.039, 0.039, 0.039, 0.035, 0.041, 0.035, 0.043, 0.035, 0.039, 0.037, 0.041, 0.038, 0.037, 0.037, 0.04, 0.037, 0.041, 0.041, 0.043, 0.041, 0.04, 0.037, 0.041, 0.041, 0.037, 0.038, 0.036, 0.045, 0.04, 0.041, 0.047, 0.04, 0.04, 0.041, 0.039, 0.041, 0.035, 0.036, 0.046, 0.04, 0.04, 0.034, 0.04, 0.042, 0.041, 0.038, 0.041, 0.042, 0.038, 0.042, 0.039, 0.036, 0.042, 0.039, 0.039, 0.042, 0.039, 0.042, 0.041, 0.034, 0.041, 0.041, 0.039, 0.036, 0.042, 0.04, 0.043, 0.04, 0.04, 0.04, 0.042, 0.037, 0.04, 0.041, 0.042, 0.037, 0.043, 0.041, 0.036, 0.038, 0.04, 0.038, 0.043, 0.04, 0.039, 0.041, 0.039, 0.042, 0.044, 0.038, 0.039, 0.04, 0.044, 0.045, 0.039, 0.045, 0.037, 0.037, 0.036, 0.041, 0.036, 0.037, 0.043, 0.041, 0.036, 0.04, 0.04, 0.035, 0.038, 0.04, 0.043, 0.038, 0.038, 0.039, 0.042, 0.037, 0.041, 0.04, 0.04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:40] [0] \t\t #> Encoding 111 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n",
      "1it [00:03,  3.79s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1508.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:44] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 02, 20:21:44] #> Building the emb2pid mapping..\n",
      "[Jul 02, 20:21:44] len(emb2pid) = 6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 116562.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 02, 20:21:44] #> Saved optimized IVF to /Users/danmueller/Documents/GitHub/aerospace_chatbot/db/.ragatouille/colbert/indexes/colbert-ir-colbertv2.0-0merge-400/ivf.pid.pt\n",
      "Done indexing!\n",
      "Database colbert-ir-colbertv2.0-0merge-400 upserted with 111 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i_chunk in range(len(chunk_params)):\n",
    "    # Chunk the docs before creating and upserting into the database\n",
    "    chunker=data_processing.chunk_docs(docs,\n",
    "                rag_type=rag_type,\n",
    "                n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "                chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "                chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "                llm=summary_llm,\n",
    "                show_progress=False)\n",
    "\n",
    "    print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "    for i_run in range(len(query_params)):\n",
    "        # Create and upsert database\n",
    "        print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "        index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "        index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "        create_upsert(query_params[i_run]['index_type'],\n",
    "                        index_name,\n",
    "                        query_models[i_run],\n",
    "                        rag_type,\n",
    "                        chunker,\n",
    "                        summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, parent-child, 400 character-recursive chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Parent-Child'\n",
    "summary_llm=None\n",
    "i_chunk=1   # 400 character-recursive setting\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 113 chunks from 2 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-0merge-400-parent-child created.\n",
      "Database text-embedding-3-large-0merge-400-parent-child upserted with 113 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Summary'\n",
    "summary_llm=llms[3]\n",
    "\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 summaries from 2 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-2merge-0-dedicated-endpoint-summary created.\n",
      "Database deleted: text-embedding-3-large-2merge-0-dedicated-endpoint-summary\n",
      "Error: 'chunks'\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['summaries'])} summaries from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
