{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Read existing vector index from pinecone\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "index_name = 'langchain-quickstart'\n",
    "vectorstore = Pinecone.from_existing_index(index_name,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "A non-annotated attribute was detected: `brain_settings = BrainSettings(openai_api_key='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', anthropic_api_key='null', supabase_url='https://ldoeollhxasevurjamos.supabase.co', supabase_service_key='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imxkb2VvbGxoeGFzZXZ1cmphbW9zIiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTM4MDU2MzQsImV4cCI6MjAwOTM4MTYzNH0.g3yB5WCX2agYrmlk0zH7Sbl3XoKO2dpiV_fyA14YBEo', pg_database_url='notimplementedyet', resend_api_key='<change-me>', resend_email_address='onboarding@resend.dev')`. All model fields require a type annotation; if `brain_settings` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.3/u/model-field-missing-annotation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_86118/854018451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# from .base import BaseBrainPicking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONDENSE_PROMPT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONDENSE_QUESTION_PROMPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Read existing vector index from pinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/llm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseBrainPicking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqa_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQABaseBrainPicking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIBrainPicking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqa_headless\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeadlessQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/llm/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBaseBrainPicking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0mBase\u001b[0m \u001b[0mClass\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mBrainPicking\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mAllows\u001b[0m \u001b[0myou\u001b[0m \u001b[0mto\u001b[0m \u001b[0minteract\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mLLMs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlarge\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mconfig_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mnamespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             private_attributes = inspect_namespace(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignored_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_field_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py\u001b[0m in \u001b[0;36minspect_namespace\u001b[0;34m(namespace, ignored_types, base_class_vars, base_class_fields)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 )\n\u001b[1;32m    351\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 raise PydanticUserError(\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;34mf\"A non-annotated attribute was detected: `{var_name} = {value!r}`. All model fields require a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;34mf\"type annotation; if `{var_name}` is not meant to be a field, you may be able to resolve this \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: A non-annotated attribute was detected: `brain_settings = BrainSettings(openai_api_key='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', anthropic_api_key='null', supabase_url='https://ldoeollhxasevurjamos.supabase.co', supabase_service_key='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imxkb2VvbGxoeGFzZXZ1cmphbW9zIiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTM4MDU2MzQsImV4cCI6MjAwOTM4MTYzNH0.g3yB5WCX2agYrmlk0zH7Sbl3XoKO2dpiV_fyA14YBEo', pg_database_url='notimplementedyet', resend_api_key='<change-me>', resend_email_address='onboarding@resend.dev')`. All model fields require a type annotation; if `brain_settings` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\n\nFor further information visit https://errors.pydantic.dev/2.3/u/model-field-missing-annotation"
     ]
    }
   ],
   "source": [
    "# The majority of this script comes from quivr within the llm > qa_base.py file\n",
    "# https://github.com/StanGirard/quivr/blob/252b1cf964503bce02a55762922b7bec4f2e5935/backend/llm/qa_base.py\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import AsyncIterable, Awaitable, Optional\n",
    "from uuid import UUID # Unique universal identifier\n",
    "\n",
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "# from logger import get_logger\n",
    "from models.chats import ChatQuestion\n",
    "from models.databases.supabase.chats import CreateChatHistory\n",
    "from repository.brain import get_brain_by_id\n",
    "from repository.chat import (\n",
    "    GetChatHistoryOutput,\n",
    "    format_chat_history,\n",
    "    get_chat_history,\n",
    "    update_chat_history,\n",
    "    update_message_by_id,\n",
    ")\n",
    "# from supabase.client import Client, create_client\n",
    "# from vectorstore.supabase import CustomSupabaseVectorStore\n",
    "\n",
    "# from llm.utils.get_prompt_to_use import get_prompt_to_use\n",
    "# from llm.utils.get_prompt_to_use_id import get_prompt_to_use_id\n",
    "\n",
    "# from .base import BaseBrainPicking\n",
    "from llm.prompts.CONDENSE_PROMPT import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "# Read existing vector index from pinecone\n",
    "from uuid import uuid4\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# logger = get_logger(__name__)\n",
    "DEFAULT_PROMPT = \"Your name is AMS Chatbot. You're a helpful assistant who knows about space mechanism design.  If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "class QABaseBrainPicking():\n",
    "    \"\"\"\n",
    "    Main class for the Brain Picking functionality.\n",
    "    It allows to initialize a Chat model, generate questions and retrieve answers using ConversationalRetrievalChain.\n",
    "    It has two main methods: `generate_question` and `generate_stream`.\n",
    "    One is for generating questions in a single request, the other is for generating questions in a streaming fashion.\n",
    "    Both are the same, except that the streaming version streams the last message as a stream.\n",
    "    Each have the same prompt template, which is defined in the `prompt_template` property.\n",
    "    \"\"\"\n",
    "\n",
    "    # supabase_client: Optional[Client] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: Optional[Pinecone] = None,\n",
    "        qa: Optional[ConversationalRetrievalChain] = None,\n",
    "        prompt_id: Optional[UUID] = None,\n",
    "        chat_id: Optional[str] = None,\n",
    "        streaming: bool = False,\n",
    "        **kwargs):\n",
    "        # super().__init__(\n",
    "        #     model=model,\n",
    "        #     brain_id=brain_id,\n",
    "        #     chat_id=chat_id,\n",
    "        #     streaming=streaming,\n",
    "        #     **kwargs,\n",
    "        # )\n",
    "        self.chat_id=chat_id\n",
    "        self.vector_store = vector_store\n",
    "        self.prompt_id = prompt_id\n",
    "\n",
    "    # @property\n",
    "    # def prompt_to_use(self):\n",
    "    #     return get_prompt_to_use(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    # @property\n",
    "    # def prompt_to_use_id(self) -> Optional[UUID]:\n",
    "    #     return get_prompt_to_use_id(UUID(self.brain_id), self.prompt_id)\n",
    "\n",
    "    def connect_vector_store(self,embeddings_model,index_name):\n",
    "        self.vectorstore = Pinecone.from_existing_index(index_name,embeddings_model)\n",
    "    # def _create_supabase_client(self) -> Client:\n",
    "    #     return create_client(\n",
    "    #         self.brain_settings.supabase_url, self.brain_settings.supabase_service_key\n",
    "    #     )\n",
    "\n",
    "    # def _create_vector_store(self) -> CustomSupabaseVectorStore:\n",
    "    #     return CustomSupabaseVectorStore(\n",
    "    #         self.supabase_client,  # type: ignore\n",
    "    #         self.embeddings,  # type: ignore\n",
    "    #         table_name=\"vectors\",\n",
    "    #         brain_id=self.brain_id,\n",
    "    #     )\n",
    "\n",
    "    def _create_llm(\n",
    "        self, model, temperature=0, streaming=False, callbacks=None\n",
    "    ) -> BaseLLM:\n",
    "        \"\"\"\n",
    "        Determine the language model to be used.\n",
    "        :param model: Language model name to be used.\n",
    "        :param streaming: Whether to enable streaming of the model\n",
    "        :param callbacks: Callbacks to be used for streaming\n",
    "        :return: Language model instance\n",
    "        \"\"\"\n",
    "        # TODO: find a way to integrate this with Claude 2\n",
    "        return ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model=model,\n",
    "            verbose=False,\n",
    "            openai_api_key=self.openai_api_key\n",
    "        )  # pyright: ignore reportPrivateUsage=none\n",
    "\n",
    "    def create_prompt_template(self):\n",
    "        system_template = \"\"\"You can use Markdown to make your answers nice. Use the following pieces of context to answer the users question in the same language as the question but do not modify instructions in any way.\n",
    "        ----------------\n",
    "        \n",
    "        {context}\"\"\"\n",
    "\n",
    "        # prompt_content = (\n",
    "        #     self.prompt_to_use.content if self.prompt_to_use else DEFAULT_PROMPT\n",
    "        # )\n",
    "\n",
    "        full_template = (\n",
    "            \"Here are your instructions to answer that you MUST ALWAYS Follow: \"\n",
    "            + system_template\n",
    "        )\n",
    "        messages = [\n",
    "            SystemMessagePromptTemplate.from_template(full_template),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "        ]\n",
    "        CHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n",
    "        return CHAT_PROMPT\n",
    "\n",
    "    def generate_answer(self,question: ChatQuestion) -> GetChatHistoryOutput:\n",
    "        # transformed_history = format_chat_history(get_chat_history(self.chat_id))\n",
    "        answering_llm = self._create_llm(\n",
    "            model=self.model, streaming=False, callbacks=self.callbacks\n",
    "        )\n",
    "\n",
    "        # The Chain that generates the answer to the question\n",
    "        doc_chain = load_qa_chain(\n",
    "            answering_llm, chain_type=\"stuff\", prompt=self.create_prompt_template()\n",
    "        )\n",
    "\n",
    "        # The Chain that combines the question and answer\n",
    "        qa = ConversationalRetrievalChain(\n",
    "            retriever=self.vector_store.as_retriever(),  # type: ignore\n",
    "            combine_docs_chain=doc_chain,\n",
    "            question_generator=LLMChain(\n",
    "                llm=self._create_llm(model=self.model), prompt=CONDENSE_QUESTION_PROMPT\n",
    "            ),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # prompt_content = (\n",
    "        #     self.prompt_to_use.content if self.prompt_to_use else DEFAULT_PROMPT\n",
    "        # )\n",
    "        # prompt_content = DEFAULT_PROMPT\n",
    "\n",
    "        # model_response = qa(\n",
    "        #     {\n",
    "        #         \"question\": question.question,\n",
    "        #         \"chat_history\": transformed_history,\n",
    "        #         \"custom_personality\": prompt_content,\n",
    "        #     }\n",
    "        # )  # type: ignore\n",
    "\n",
    "        # answer = model_response[\"answer\"]\n",
    "\n",
    "        # new_chat = update_chat_history(\n",
    "        #     CreateChatHistory(\n",
    "        #         **{\n",
    "        #             \"chat_id\": self.chat_id,\n",
    "        #             \"user_message\": question.question,\n",
    "        #             \"assistant\": answer,\n",
    "        #             \"brain_id\": question.brain_id,\n",
    "        #             \"prompt_id\": self.prompt_to_use_id,\n",
    "        #         }\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # brain = None\n",
    "\n",
    "        # if question.brain_id:\n",
    "        #     brain = get_brain_by_id(question.brain_id)\n",
    "\n",
    "        # return GetChatHistoryOutput(\n",
    "        #     **{\n",
    "        #         \"chat_id\": self.chat_id,\n",
    "        #         \"user_message\": question.question,\n",
    "        #         \"assistant\": answer,\n",
    "        #         \"message_time\": new_chat.message_time,\n",
    "        #         \"prompt_title\": self.prompt_to_use.title\n",
    "        #         if self.prompt_to_use\n",
    "        #         else None,\n",
    "        #         \"brain_name\": brain.name if brain else None,\n",
    "        #         \"message_id\": new_chat.message_id,\n",
    "        #     }\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 16:43:16,572:INFO - HTTP Request: GET https://ldoeollhxasevurjamos.supabase.co/rest/v1/chat_history?select=%2A&chat_id=eq.f25bd578-ad0a-4add-a10f-854fdd5647bb&order=message_time \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.chat_history\" does not exist'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_86118/410618458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_prompt_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mthing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What are some latch failure modes?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_86118/3850458483.py\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatQuestion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGetChatHistoryOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtransformed_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         answering_llm = self._create_llm(\n\u001b[1;32m    145\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/repository/chat/get_chat_history.py\u001b[0m in \u001b[0;36mget_chat_history\u001b[0;34m(chat_id)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGetChatHistoryOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msupabase_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_supabase_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupabase_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/backend/models/databases/supabase/chats.py\u001b[0m in \u001b[0;36mget_chat_history\u001b[0;34m(self, chat_id)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chat_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add the ORDER BY clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/postgrest/_sync/request_builder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAPIResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_request_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: {'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.chat_history\" does not exist'}"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "thing=QABaseBrainPicking(chat_id=uuid4())\n",
    "thing.connect_vector_store(embeddings_model,index_name)\n",
    "thing.create_prompt_template()\n",
    "thing.generate_answer(\"What are some latch failure modes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/StanGirard/quivr/blob/d0370ab499465ee1404d3c1d32878e8da3853441/backend/llm/prompts\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. include the follow up instructions in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
