{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset('dsmueller/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic_100',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: What is the title of the symposium hosted by NASA Langley Research Center and Lockheed Martin Space Systems Company, as mentioned in the context?### Assistant:NASNCP-2006-2 14290  3 sth Aerospace Mechanisms Symposium  Compiled by  Edward A. Boesiger  Lockheed Martin Space Systems Company, Sunnyvale, California  Proceedings of a symposium hosted by  the NASA Langley Research Center and  Lockheed Martin Space Systems Company and  organized by the Mechanisms Education Association  held at the Williamsburg Maniott Hotel  Williamsburg, Virginia  May 17- 19,2006  May 2006 {'source': 'AMS_2006.pdf', 'page': 1}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken this dataset and renamed into train.csv in the training folder. I can't figure out how to use autotrain with a different filename..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotrain\n",
    "https://github.com/huggingface/autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TinyLlama/TinyLlama-1.1B-Chat-v0.1\n",
    "# project_name='./llms/'+'ams_data_train_'+'TinyLlama_1.1B_Chat_v0.1_100-2'\n",
    "# model_name='TinyLlama/TinyLlama-1.1B-Chat-v0.1'\n",
    "# repo_id='ai-aerospace/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic-100-2'\n",
    "\n",
    "# meta-llama/Llama-2-7b-chat-hf\n",
    "project_name='./llms/'+'ams_data_train_'+'Mistral-7B-v0.1'+'-100'\n",
    "model_name='mistralai/Mistral-7B-v0.1'\n",
    "repo_id='ai-aerospace/'+'ams_data_train_'+'Mistral-7B-v0.1'+'-100'\n",
    "\n",
    "os.environ[\"project_name\"] = project_name\n",
    "os.environ[\"model_name\"] = model_name\n",
    "os.environ[\"repo_id\"] = repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m⚠️ WARNING\u001b[0m | \u001b[32m2023-12-10 22:06:55\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[33m\u001b[1m❌ Some DreamBooth components are missing! Please run `autotrain setup` to install it. Ignore this warning if you are not using DreamBooth or running `autotrain setup` already.\u001b[0m\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(version=False, train=True, deploy=False, inference=False, data_path='.', train_split='train', valid_split=None, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model='meta-llama/Llama-2-7b-chat-hf', model_ref=None, learning_rate=0.0002, num_train_epochs=3, train_batch_size=6, warmup_ratio=0.1, gradient_accumulation_steps=1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, seed=42, add_eos_token=False, block_size=-1, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, project_name='./llms/ams_data_train_Llama-2-7b-chat-hf100', evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, fp16=False, push_to_hub=True, use_int8=False, model_max_length=1024, repo_id='ai-aerospace/ams_data_train_Llama-2-7b-chat-hf100', use_int4=False, trainer='sft', target_modules=None, merge_adapter=False, token='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', backend='default', username=None, use_flash_attention_2=False, log='none', disable_gradient_checkpointing=False, dpo_beta=0.1, func=<function run_llm_command_factory at 0x179fa4220>)\u001b[0m\n",
      "> \u001b[1mINFO    loading dataset from csv\u001b[0m\n",
      "> \u001b[31m\u001b[1mERROR   train has failed due to an exception:\u001b[0m\n",
      "> \u001b[31m\u001b[1mERROR   Traceback (most recent call last):\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 270, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 430, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1374, in hf_hub_download\n",
      "    raise head_call_error\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1247, in hf_hub_download\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1624, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 402, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 426, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 286, in hf_raise_for_status\n",
      "    raise GatedRepoError(message, response) from e\n",
      "huggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-6576a700-2b836b8d397c63231f6b51f0;1172eaf0-4561-4773-b966-eb7aae9e9ab9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/autotrain/utils.py\", line 280, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/autotrain/trainers/clm/__main__.py\", line 103, in train\n",
      "    tokenizer = AutoTokenizer.from_pretrained(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 718, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 550, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 445, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: You are trying to access a gated repo.\n",
      "Make sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!source ../.venv/bin/activate\n",
    "# !autotrain llm --train --project_name my-llm --model TinyLlama/TinyLlama-1.1B-Chat-v0.1 --data_path . --use-peft --use_int4 --learning_rate 2e-4 --train_batch_size 6 --num_train_epochs 3 --trainer sft\n",
    "\n",
    "# The training dataset to be used must be called training.csv and be located in the data_path folder.\n",
    "!autotrain llm --train \\\n",
    "    --project_name ${project_name} \\\n",
    "    --model ${model_name} \\\n",
    "    --data_path . \\\n",
    "    --use-peft \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --train_batch_size 6 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --trainer sft \\\n",
    "    --push_to_hub \\\n",
    "    --repo_id ${repo_id} \\\n",
    "    --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
