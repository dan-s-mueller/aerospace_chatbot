{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta, archive_db, get_docs_questions_df\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "    if Path(file_name).exists():\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_questions_answers_in, df_docs):\n",
    "    df_questions_answers_out=df_questions_answers_in.copy()\n",
    "    \n",
    "    # Generate responses using RAG with input parameters\n",
    "    for i, row in df_questions_answers_out.iterrows():\n",
    "        if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "            print(f\"Processing question {i+1}/{len(df_questions_answers_out)}\")\n",
    "\n",
    "            # Use the QA model to query the documents\n",
    "            qa_obj=queries.QA_Model(index_type,\n",
    "                            index_name,\n",
    "                            query_model,\n",
    "                            llm,\n",
    "                            **QA_model_params)\n",
    "            qa_obj.query_docs(row['question'])\n",
    "            response=qa_obj.result\n",
    "\n",
    "            df_questions_answers_out.loc[df_questions_answers_out.index[i], \"answer\"] = response['answer'].content\n",
    "\n",
    "            ids=[_stable_hash_meta(source_document.metadata)\n",
    "                for source_document in response['references']]\n",
    "            df_questions_answers_out.loc[df_questions_answers_out.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "            # Save the response to cache file\n",
    "            response_dict = {\n",
    "                \"question\": row['question'],\n",
    "                \"answer\": response['answer'].content,\n",
    "                \"source_documents\": ids,\n",
    "            }\n",
    "            write_dict_to_file(response_dict, f'rag_response_cache_{index_name}.json')\n",
    "\n",
    "    # Get the context documents content for each question\n",
    "    source_documents_list = []\n",
    "    for cell in df_questions_answers_out['source_documents']:\n",
    "        cell_list = cell.strip('[]').split(', ')\n",
    "        context=[]\n",
    "        for cell in cell_list:\n",
    "            context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "        source_documents_list.append(context)\n",
    "    df_questions_answers_out[\"contexts\"]=source_documents_list\n",
    "\n",
    "    # Addtionaly get embeddings for questions\n",
    "    if not Path(f'question_embeddings_{index_name}.pickle').exists():\n",
    "        question_embeddings = [\n",
    "            query_model.embed_query(question)\n",
    "            for question in df_questions_answers_out[\"question\"]\n",
    "        ]\n",
    "        with open(f'question_embeddings_{index_name}.pickle', \"wb\") as f:\n",
    "            pickle.dump(question_embeddings, f)\n",
    "\n",
    "    question_embeddings = pickle.load(open(f'question_embeddings_{index_name}.pickle', \"rb\"))\n",
    "    df_questions_answers_out[\"embedding\"] = question_embeddings\n",
    "    return df_questions_answers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rag(index_name, df_questions_answers_in):\n",
    "    df_questions_answers_out=df_questions_answers_in.copy()\n",
    "\n",
    "    # Add answer correctness column, fill in if it exists\n",
    "    df_questions_answers_out = add_cached_column_from_file(\n",
    "        df_questions_answers_out, f'ragas_result_cache_{index_name}.json', \"question\", \"answer_correctness\"\n",
    "    )\n",
    "\n",
    "    # Unclear why but sometimes ground_truth does not provide a response. Just filter those out.\n",
    "    df_questions_answers_out = df_questions_answers_out[df_questions_answers_out['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "    df_questions_answers_out\n",
    "\n",
    "    # Prepare the dataframe for evaluation\n",
    "    df_qa_eval = df_questions_answers_out.copy()\n",
    "\n",
    "    # Evaluate the answer correctness if not already done\n",
    "    fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "    for i, row in df_qa_eval.iterrows():\n",
    "        print(i, row[\"question\"])\n",
    "        if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "            evaluation_result = evaluate(\n",
    "                Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "                [answer_correctness],\n",
    "            )\n",
    "            df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "                \"answer_correctness\"\n",
    "            ]\n",
    "\n",
    "            # optionally save the response to cache\n",
    "            response_dict = {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "            }\n",
    "            write_dict_to_file(response_dict, f'ragas_result_cache_{index_name}.json')\n",
    "\n",
    "    # write the answer correctness to the original dataframe\n",
    "    df_questions_answers_out[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n",
    "\n",
    "    return df_qa_eval, df_questions_answers_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_viz_prep(index_name,df_qa_eval,df_docs):\n",
    "    # This section adds a column to df_documents containing the ids of the questions that used the document as source. \n",
    "\n",
    "    # add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "    # Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "    df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "    # Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "    agg = (\n",
    "        df_questions_exploded.groupby(\"source_documents\")\n",
    "        .agg(\n",
    "            num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "            question_ids=(\n",
    "                \"id\",\n",
    "                lambda x: list(x),\n",
    "            ),  # List of question IDs referencing the document\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"source_documents\": \"id\"})\n",
    "    )\n",
    "\n",
    "    # Merge the aggregated information back into df_documents\n",
    "    df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "    # Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "    df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    # Replace NaN values in 'num_questions' with 0\n",
    "    df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    df_visualize = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "    df_questions = df_visualize[~df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_questions[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_questions = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "\n",
    "\n",
    "    df_without_questions = df_visualize[df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_without_questions[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_docs = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "    df_visualize[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_visualize[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_all = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "    df_visualize[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "    # find the nearet question (by embedding) for each document\n",
    "    question_embeddings = np.array(df_visualize[df_visualize[\"question\"].notna()][\"embedding\"].tolist())\n",
    "\n",
    "    df_visualize[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "        np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "        for doc_emb in df_visualize[\"embedding\"].values\n",
    "    ]\n",
    "\n",
    "    # write the dataframe to parquet for later use\n",
    "    df_visualize.to_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "    return df_visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'text-embedding-3-large-2merge-0',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-0merge-400',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-0merge-400-parent-child',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-2merge-0-queries',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "db=dbs[0]\n",
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=db['index_name'],\n",
    "                        embedding_function=db['query_model'])  \n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "for db in dbs:\n",
    "    df_temp_chroma=archive_db('ChromaDB',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "embedding_model='text-embedding-3-large'\n",
    "synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "sample_size=50\n",
    "percent_total=sample_size/len(lcdocs_chroma)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs_chroma, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if testset.csv exists, use, or generate the synthetic dataset.\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=5\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=False,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=False)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv(f\"testset_{db['index_name']}.csv\", index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv(f\"testset_{db['index_name']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RAG to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type='ChromaDB'\n",
    "index_name=db['index_name']\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers_rag=rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_questions_answers, df_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa_eval, df_questions_answers_rag = eval_rag(index_name, df_questions_answers_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=data_viz_prep(index_name,df_qa_eval,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1024,\n",
       "              'host': 'voyage-large-2-instruct-2merge-0-dj30h8y.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'voyage-large-2-instruct-2merge-0',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'voyage-large-2-instruct-0merge-400-dj30h8y.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'voyage-large-2-instruct-0merge-400',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-2merge-0',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)},\n",
    "     {'index_name':'voyage-large-2-instruct-0merge-400',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 0 to 200\n",
      "Fetching 200 to 400\n",
      "Fetching 400 to 600\n",
      "Fetching 600 to 800\n",
      "Fetching 800 to 1000\n",
      "Fetching 1000 to 1200\n",
      "Fetching 1200 to 1400\n",
      "Fetching 1400 to 1600\n",
      "Fetching 1600 to 1800\n",
      "Fetching 1800 to 2000\n",
      "Fetching 2000 to 2200\n",
      "Fetching 2200 to 2400\n",
      "2222\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "db=dbs[0]\n",
    "index = pinecone_client.Index(db['index_name'])\n",
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)\n",
    "\n",
    "docs=[]\n",
    "chunk_size=200  # Tune to whatever doesn't error out, 200 won't for serverless\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)\n",
    "\n",
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))\n",
    "    \n",
    "print(len(lcdocs_pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "for db in dbs:\n",
    "    df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
