{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import eval\n",
    "import admin\n",
    "from data_processing import _stable_hash_meta, archive_db\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset, generate docs+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-large-0merge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them. Uncomment if desired, takes a while.\n",
    "export=False\n",
    "\n",
    "if export:\n",
    "    for collection in collections:\n",
    "        df_temp_chroma=archive_db('ChromaDB',collection.name,collection._embedding_function,export_pickle=True)\n",
    "\n",
    "    df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "idx=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "\n",
    "docs_vectorstore=collections[idx]\n",
    "query_model=query_models[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format docs into dataframe\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1024,\n",
       "              'host': 'voyage-large-2-instruct-2merge-0-dj30h8y.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'voyage-large-2-instruct-2merge-0',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'voyage-large-2-instruct-0merge-400-dj30h8y.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'voyage-large-2-instruct-0merge-400',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-2merge-0',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)},\n",
    "     {'index_name':'voyage-large-2-instruct-0merge-400',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "idx=0\n",
    "db=dbs[0]\n",
    "index = pinecone_client.Index(db['index_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 0 to 200\n",
      "Fetching 200 to 400\n",
      "Fetching 400 to 600\n",
      "Fetching 600 to 800\n",
      "Fetching 800 to 1000\n",
      "Fetching 1000 to 1200\n",
      "Fetching 1200 to 1400\n",
      "Fetching 1400 to 1600\n",
      "Fetching 1600 to 1800\n",
      "Fetching 1800 to 2000\n",
      "Fetching 2000 to 2200\n",
      "Fetching 2200 to 2400\n",
      "2222\n"
     ]
    }
   ],
   "source": [
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)\n",
    "\n",
    "docs=[]\n",
    "df_docs = pd.DataFrame()\n",
    "chunk_size=200  # Tune to whatever doesn't error out, 200 won't for serverless\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)\n",
    "\n",
    "    df_doc_temp = pd.DataFrame()\n",
    "    df_doc_temp[\"id\"]= [vector_elm[\"id\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"source\"]= [vector_elm[\"metadata\"][\"source\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"page\"]= [vector_elm[\"metadata\"][\"page\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"document\"]= [vector_elm[\"metadata\"][\"page_content\"] for vector_elm in vector_data]\n",
    "    df_doc_temp[\"embedding\"]= [vector_elm[\"values\"] for vector_elm in vector_data]\n",
    "    df_docs = pd.concat([df_docs, df_doc_temp])\n",
    "\n",
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))\n",
    "    \n",
    "print(len(lcdocs_pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "# for db in dbs:\n",
    "#     df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': True, 'message': ['colbert-ir-colbertv2.0-2merge-0']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes=admin.show_ragatouille_indexes(format=False)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "n_questions=10   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}.csv\")\n",
    "lcdocs=lcdocs_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset=eval.generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-large-2merge-0'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "df_testset=df_testset.head(4)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "# TODO add parent-child\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "setup_data['eval_models'] = setup_data['eval_models'][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_models': [{'index_type': 'Pinecone',\n",
       "   'index_name': 'voyage-large-2-instruct-2merge-0',\n",
       "   'query_model': {'query_model': 'Voyage',\n",
       "    'embedding_name': 'voyage-large-2-instruct'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'RAGatouille',\n",
       "   'index_name': 'colbert-ir-colbertv2.0-2merge-0',\n",
       "   'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'k': 4,\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_type': 'Pinecone', 'index_name': 'voyage-large-2-instruct-2merge-0', 'query_model': {'query_model': 'Voyage', 'embedding_name': 'voyage-large-2-instruct'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "   question_id                                           question  \\\n",
      "0            0  How much power does the four-element piezo mot...   \n",
      "1            1  How did statistical analysis determine the num...   \n",
      "2            2  How is the stress field in the Si3N4 ball and ...   \n",
      "3            3  How important is testing in ensuring the relia...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  The normal operating conditions on Earth for t...   \n",
      "1  The statistical method used to determine the o...   \n",
      "2  The presence of a ball scar within the contact...   \n",
      "3  Testing plays a crucial role in the reliabilit...   \n",
      "\n",
      "                                    source_documents answer_by  \\\n",
      "0  [2ef0bb694272ac906b32d2ce1017cc475004f3b3, 2ec...    gpt-4o   \n",
      "1  [5c2ecd80d0c7b0eea19bd732b25f89594d6fac5c, 05c...    gpt-4o   \n",
      "2  [0a22815cea1effbf06b31313a9a816dc0c2bf2b7, 05c...    gpt-4o   \n",
      "3  [1389286457dfbbe872949a23e2def8d09f06cb02, 40c...    gpt-4o   \n",
      "\n",
      "               query_model                                    qa_model_params  \\\n",
      "0  voyage-large-2-instruct  {'rag_type': 'Standard', 'k': 4, 'search_type'...   \n",
      "1  voyage-large-2-instruct  {'rag_type': 'Standard', 'k': 4, 'search_type'...   \n",
      "2  voyage-large-2-instruct  {'rag_type': 'Standard', 'k': 4, 'search_type'...   \n",
      "3  voyage-large-2-instruct  {'rag_type': 'Standard', 'k': 4, 'search_type'...   \n",
      "\n",
      "  index_type                        index_name  \n",
      "0   Pinecone  voyage-large-2-instruct-2merge-0  \n",
      "1   Pinecone  voyage-large-2-instruct-2merge-0  \n",
      "2   Pinecone  voyage-large-2-instruct-2merge-0  \n",
      "3   Pinecone  voyage-large-2-instruct-2merge-0  \n",
      "{'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0', 'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [question_id, question, answer, source_documents, answer_by, query_model, qa_model_params, index_type, index_name]\n",
      "Index: []\n",
      "Processing question 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index colbert-ir-colbertv2.0-2merge-0 for the first time... This may take a few seconds\n",
      "[Jul 14, 11:26:44] #> Loading codec...\n",
      "[Jul 14, 11:26:44] #> Loading IVF...\n",
      "[Jul 14, 11:26:44] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 547.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 14, 11:26:44] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What are the normal operating conditions on Earth for the four-element piezo motor, and how much power does it consume under these conditions?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2024,  1996,  3671,  4082,  3785,  2006,  3011,\n",
      "         2005,  1996,  2176,  1011,  5783, 11345,  6844,  5013,  1010,  1998,\n",
      "         2129,  2172,  2373,  2515,  2009, 16678,  2104,  2122,  3785,  1029,\n",
      "          102,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0])\n",
      "\n",
      "{'question_id': 0, 'question': 'How much power does the four-element piezo motor consume under normal operating conditions on Earth?', 'answer': 'The normal operating conditions on Earth for the four-element piezo motor involve it consuming approximately 3.25 watts of power. The drive electronics used in the prototype consume an additional 4 watts, resulting in a total power consumption of 7.25 watts for the mechanism. The piezo elements require a sinusoidal voltage of 120 volts for excitation, and the control electronics run off a 24 or 48 volt DC bus.', 'source_documents': ['099ec9c21af691b1133d034105a024e7148c71dd', '099ec9c21af691b1133d034105a024e7148c71dd', '2ef0bb694272ac906b32d2ce1017cc475004f3b3', '30ad4c3ea4afa853321ee6d8e862f5abe98b2325'], 'answer_by': 'gpt-4o', 'query_model': 'colbert-ir/colbertv2.0', 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}, 'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0'}\n",
      "Processing question 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index colbert-ir-colbertv2.0-2merge-0 for the first time... This may take a few seconds\n",
      "[Jul 14, 11:26:49] #> Loading codec...\n",
      "[Jul 14, 11:26:49] #> Loading IVF...\n",
      "[Jul 14, 11:26:49] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 885.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 14, 11:26:49] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.53it/s]\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What specific statistical methods were used to determine the number of scars needed on each ball for the bearing fatigue life test?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  3563,  7778,  4725,  2020,  2109,  2000,  5646,\n",
      "         1996,  2193,  1997, 13521,  2734,  2006,  2169,  3608,  2005,  1996,\n",
      "         7682, 16342,  2166,  3231,  1029,   102,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "{'question_id': 1, 'question': 'How did statistical analysis determine the number of scars needed on each ball for the bearing fatigue life test?', 'answer': 'The specific statistical method used to determine the number of scars needed on each ball for the bearing fatigue life test was based on ensuring a high probability (99.9%) that at least one of the 64 scars would be run over during the test. This was achieved through statistical analysis, which indicated that placing 2 scars on each 7/16-inch-diameter ball would meet this criterion, assuming the balls maintained their orientation throughout the entire duration of the test.\\n\\nFor tests studying the influence of scar frequency on bearing fatigue life, a different approach was used where only two balls (ball 1 and ball 5) in each bearing contained scars, resulting in a total of 16 scars.\\n\\nAdditionally, the test protocol employed sudden-death statistics with four bearings using the least-of-four technique to obtain a Weibull distribution. This method involved using accelerometers to shut down the testers when a bearing failure raised the vibration level above a pre-determined limit.\\n\\nIn summary, the statistical methods included:\\n- Ensuring a 99.9% probability that at least one scar would be run over by placing 2 scars on each ball.\\n- Using sudden-death statistics with the least-of-four technique to obtain a Weibull distribution for the fatigue life tests.', 'source_documents': ['05c38a1ac579c4b37cc07249f581c9ebd9d82dae', '05c38a1ac579c4b37cc07249f581c9ebd9d82dae', '5c2ecd80d0c7b0eea19bd732b25f89594d6fac5c', '5845a4c11b7f34dd976c86e7c374d86d0e31b34b'], 'answer_by': 'gpt-4o', 'query_model': 'colbert-ir/colbertv2.0', 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}, 'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0'}\n",
      "Processing question 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index colbert-ir-colbertv2.0-2merge-0 for the first time... This may take a few seconds\n",
      "[Jul 14, 11:26:57] #> Loading codec...\n",
      "[Jul 14, 11:26:57] #> Loading IVF...\n",
      "[Jul 14, 11:26:57] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 1349.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 14, 11:26:57] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.95it/s]\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . How does the presence of a ball scar within the contact patch influence the stress distribution in the Si3N4 ball and raceway?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2129,  2515,  1996,  3739,  1997,  1037,  3608, 11228,\n",
      "         2306,  1996,  3967,  8983,  3747,  1996,  6911,  4353,  1999,  1996,\n",
      "         9033,  2509,  2078,  2549,  3608,  1998, 23018,  1029,   102,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0])\n",
      "\n",
      "{'question_id': 2, 'question': 'How is the stress field in the Si3N4 ball and raceway affected by the presence of a ball scar within the contact patch?', 'answer': 'The presence of a ball scar within the contact patch influences the stress distribution in the Si₃N₄ ball and raceway in the following ways:\\n\\n1. **Si₃N₄ Ball Stress Distribution**:\\n   - Finite Element Method (FEM) results showed little change in the Si₃N₄ ball’s stress field around the contact region due to the presence of the scar. This suggests that the scar is relatively benign in terms of its effect on the life of the Si₃N₄ ball itself.\\n\\n2. **Steel Raceway Stress Distribution**:\\n   - The ball scars significantly alter the stress distribution in the steel raceway. Specifically, the high von Mises stress region, which is typically located directly under the contact patch in a pristine condition, is shifted closer to the surface when a scar is present.\\n   - The presence of the scar creates a low stress region directly under the scar and introduces a local stress concentration at the scar edge. This stress concentration connects with the high stress region, potentially leading to premature spalling in the steel raceway.\\n\\nThese changes in the stress distribution due to the ball scars are critical in understanding the premature failures observed in hybrid ball bearings during experimental testing. The modeling supports that the cause of the inner race spalls is the stress riser from rolling over the scar features on the Si₃N₄ balls.', 'source_documents': ['0a22815cea1effbf06b31313a9a816dc0c2bf2b7', '0a22815cea1effbf06b31313a9a816dc0c2bf2b7', '8a25009775d807fbaf72739c35aa1de8c4cbf46a', '05c38a1ac579c4b37cc07249f581c9ebd9d82dae'], 'answer_by': 'gpt-4o', 'query_model': 'colbert-ir/colbertv2.0', 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}, 'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0'}\n",
      "Processing question 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index colbert-ir-colbertv2.0-2merge-0 for the first time... This may take a few seconds\n",
      "[Jul 14, 11:27:05] #> Loading codec...\n",
      "[Jul 14, 11:27:05] #> Loading IVF...\n",
      "[Jul 14, 11:27:05] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 1968.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jul 14, 11:27:05] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.75it/s]\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . How does testing contribute to the reliability of aerospace mechanisms?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2129,  2515,  5604,  9002,  2000,  1996, 15258,  1997,\n",
      "        13395, 10595,  1029,   102,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "{'question_id': 3, 'question': 'How important is testing in ensuring the reliability of aerospace mechanisms?', 'answer': 'Testing plays a crucial role in ensuring the reliability of aerospace mechanisms. Here are several key points on how testing contributes to this reliability:\\n\\n1. **Early Detection of Failures**: Initial testing often reveals that many mechanisms have a reliability of zero. This early detection allows for the identification and rectification of potential issues before they lead to catastrophic failures. For example, the failure of the Inertial Upper Stage (IUS) during a Titan IVB launch in 1999 was due to an incomplete separation caused by an electrical connector plug failure, which could have been identified through rigorous testing.\\n\\n2. **Validation of Design Assumptions**: Testing helps validate the assumptions made during the design phase. Without comprehensive testing, reliability estimates remain speculative. Testing provides empirical data that can confirm or refute these assumptions, leading to more accurate reliability assessments.\\n\\n3. **Identification of Dominant Parameters**: Through sensitivity studies and testing, dominant parameters that affect the performance and reliability of mechanisms can be identified. This allows for targeted improvements and optimizations, as demonstrated by the study where setting only the four most dominant parameters to worst-case achieved a reliability of 0.999999.\\n\\n4. **Maintenance and Inspection Protocols**: Testing also highlights the importance of maintenance and inspection protocols for test equipment (STE). Regular inspection and refurbishment of STE ensure that the testing environment remains reliable and does not introduce additional variables that could affect the test results.\\n\\n5. **Misconception Correction**: Testing helps dispel common misconceptions about reliability. For instance, it is a misconception that the reliability of a mechanism can be simply calculated from the reliabilities of its components or that adding a back-up component will necessarily increase reliability. Testing provides real-world data that can correct these misconceptions and lead to better design practices.\\n\\nIn summary, testing is indispensable for verifying the reliability of aerospace mechanisms, identifying potential issues, validating design assumptions, and ensuring that maintenance protocols are followed. Without thorough testing, reliability estimates are purely speculative and can lead to catastrophic failures.', 'source_documents': ['08865a28dee2dd08c9bf3de207520646e0314af7', '1389286457dfbbe872949a23e2def8d09f06cb02', '40c0b04c018127d462baeeb1a9ab77c1dc19796f', '4e8c07802caf7624cfda51ee6a72672220ea7d49'], 'answer_by': 'gpt-4o', 'query_model': 'colbert-ir/colbertv2.0', 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}, 'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RAGPretrainedModel' object has no attribute 'embed_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# QA model params\u001b[39;00m\n\u001b[1;32m     19\u001b[0m qa_model_params\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqa_model_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m df_qa_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_model_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdf_qa_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m df_qa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_qa,df_qa_iter],ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# After each iteration, export a pickle of the dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/../src/aerospace_chatbot/eval.py:234\u001b[0m, in \u001b[0;36mrag_responses\u001b[0;34m(index_type, index_name, query_model, llm, QA_model_params, df_qa, df_docs, testset_name)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Addtionaly get embeddings for questions\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_embeddings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 234\u001b[0m     question_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_qa_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    238\u001b[0m df_qa_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m question_embeddings\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_qa_out\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/../src/aerospace_chatbot/eval.py:235\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Addtionaly get embeddings for questions\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_embeddings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    234\u001b[0m     question_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mquery_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m(question)\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m df_qa_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     ]\n\u001b[1;32m    238\u001b[0m df_qa_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m question_embeddings\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_qa_out\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RAGPretrainedModel' object has no attribute 'embed_query'"
     ]
    }
   ],
   "source": [
    "# Iterate through the evaluation models and databases, dump data as you go.\n",
    "# This will cache data along the way into rag_responses, so you can pick up where you left off.\n",
    "# A pickled dataframe is also exported at the end of each iteration, but not row-by-row.\n",
    "df_qa = pd.DataFrame()\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type=model['index_type']\n",
    "    sb['index_type']=index_type\n",
    "    index_name=model['index_name']\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{index_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0315,  0.0850, -0.0128,  ...,  0.0547, -0.0918,  0.0366],\n",
      "         [ 0.0012, -0.0008,  0.0097,  ..., -0.0881,  0.0215,  0.0804],\n",
      "         [-0.0275, -0.0470,  0.0361,  ..., -0.0999,  0.0272,  0.1023],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "query_model.model.encode([\"hello world2\"])\n",
    "print(query_model.model.in_memory_embed_docs)\n",
    "query_model.model.clear_encoded_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa = eval.eval_rag(index_name, df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=False\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
