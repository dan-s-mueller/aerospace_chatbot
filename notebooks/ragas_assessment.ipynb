{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import eval\n",
    "import admin\n",
    "import data_processing\n",
    "import queries\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset, generate docs+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-small-2merge-0),\n",
       " Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_models=[VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False),\n",
    "              VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes=admin.show_ragatouille_indexes(format=False)\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbs=['colbert-ir/colbertv2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=0\n",
    "# db=dbs[idx]\n",
    "\n",
    "# # TODO get this as a langchain retriever, pull docs\n",
    "# query_model = RAGPretrainedModel.from_pretrained(db,index_root=os.path.join(os.getenv('LOCAL_DB_PATH'),'.ragatouille'))\n",
    "# docs_vectorstore=data_processing.initialize_database('RAGatouille',\n",
    "#                                                      'colbert-ir-colbertv2.0-2merge-0',\n",
    "#                                                      query_model,\n",
    "#                                                      'Standard',\n",
    "#                                                      os.getenv('LOCAL_DB_PATH'),\n",
    "#                                                      init_ragatouille=False,\n",
    "#                                                      clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=docs_vectorstore.model.collection  # Document chunks (chunked smaller according to token size)\n",
    "# print(docs[0])\n",
    "# print(len(docs))\n",
    "\n",
    "# metadata=docs_vectorstore.model.docid_metadata_map  # Document metadata for original documents\n",
    "# print(len(metadata))\n",
    "\n",
    "# map=docs_vectorstore.model.pid_docid_map    # Map of document chunks to original document\n",
    "# print(len(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't quite figure out functionality to export the encodings from RAGatouille for each document. Won't pursue unless RAGatouille has exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type=\"ChromaDB\"\n",
    "index_type-=\"Pinecome\"\n",
    "\n",
    "# Select database for generating docs\n",
    "if index_type==\"ChromaDB\":\n",
    "    idx_chroma=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "    docs_vectorstore=collections[idx_chroma]\n",
    "    query_model=query_models[idx_chroma]  \n",
    "elif index_type==\"Pinecone\":\n",
    "    idx_pinecone=0\n",
    "    docs_vectorstore=indexes[idx_pinecone]\n",
    "    query_model=query_models[idx_pinecone]\n",
    "\n",
    "df_docs, lcdocs = eval.lcdoc_export(index_type,docs_vectorstore,query_model,export_pickle=False)\n",
    "print(len(lcdocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "# eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "eval_size=len(lcdocs)\n",
    "n_questions=30   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}_full.csv\")\n",
    "lcdocs=lcdocs_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loop: [0]\n",
      "Processing index 0 to 2222...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6ffd97628941a48e4b779c5c47efc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/7774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_testset=eval.generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-large-2merge-0'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "df_testset=df_testset.head(2)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the evaluation models and databases, dump data as you go.\n",
    "# This will cache data along the way into rag_responses, so you can pick up where you left off.\n",
    "# A pickled dataframe is also exported at the end of each iteration, but not row-by-row.\n",
    "df_qa = pd.DataFrame()\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type=model['index_type']\n",
    "    sb['index_type']=index_type\n",
    "    index_name=model['index_name']\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa = eval.eval_rag(index_name, df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=True\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
