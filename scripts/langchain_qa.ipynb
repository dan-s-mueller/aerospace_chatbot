{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page setup and execution for the aerospace mechanism chatbot\n",
    "Example :        \n",
    "-What can you tell me about latch mechanism design failures which have occurred        \n",
    "-Follow up: Which one of the sources discussed volatile spherical joint interfaces           \n",
    "\"\"\"\n",
    "# import databutton as db\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import data_import\n",
    "import queries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import streamlit as st\n",
    "import openai\n",
    "import secrets\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check api keys\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# print(os.getenv('PINECONE_ENVIRONMENT'))\n",
    "# print(os.getenv('PINECONE_API_KEY'))\n",
    "# print(os.getenv('HUGGING_FACE_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_TRACING_V2'))\n",
    "# print(os.getenv('LANGCHAIN_ENDPOINT'))\n",
    "# print(os.getenv('LANGCHAIN_API_KEY'))\n",
    "# print(os.getenv('LANGCHAIN_PROJECT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeable fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_level=None   # Max token limit, see section below\n",
    "k=4 # Number of queries to return\n",
    "search_type='similarity'    #  'mmr' or 'similarity'\n",
    "temperature=0\n",
    "verbose=True\n",
    "chain_type='stuff'  # 'stuff' or  'map_reduce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some items\n",
    "if output_level==\"Concise\":\n",
    "    max_generated_tokens=50\n",
    "elif output_level==\"Detailed\":\n",
    "    max_generated_tokens=516\n",
    "else:\n",
    "    # max_generated_tokens = -1\n",
    "    max_generated_tokens=None\n",
    "\n",
    "# Track filtering, set history to be blank\n",
    "message_id=0\n",
    "filter_toggle=False # Filter sources on last answer\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT')\n",
    ")\n",
    "index_name = 'canopy--ams'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openai Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voyage embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the cells below to get an llm that plugs into the prompts section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprietary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openai\n",
    "Models: https://platform.openai.com/docs/models/gpt-3-5, https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# model_name='gpt-3.5-turbo-instruct'\n",
    "model_name='gpt-3.5-turbo-1106' # 16,385 tokens\n",
    "# model_name='gpt-4-0613'\n",
    "\n",
    "llm = ChatOpenAI(temperature=temperature,\n",
    "             model_name=model_name,\n",
    "             max_tokens=max_generated_tokens,\n",
    "             tags=[model_name+'-'+str(temperature)+'-'+str(max_generated_tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Anthropic, Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama (local, works on mac only)\n",
    "To get to work, download ollama and install on your mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "model_name='llama2:latest'\n",
    "llm = Ollama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(\"the first man on the moon was...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging face models (via API)\n",
    "* Choose a model_name from the leaderboard here: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard \n",
    "* When testing these, many of the models will time out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_name='google/flan-t5-xxl'   # Fast but with uninteresting answers\n",
    "# model_name='tiiuae/falcon-40b'  # Slow\n",
    "# model_name='WizardLM/WizardLM-70B-V1.0'   # Slow\n",
    "# model_name='HuggingFaceH4/zephyr-7b-beta' # Slow\n",
    "# model_name='meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name,\n",
    "                     model_kwargs={\"temperature\": 0.1, \"max_length\": 250}\n",
    ")\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1994 FIFA World Cup was held in France. France won the 1994 FIFA World Cup. The answer: France.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the abstract about?\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "Generate a question that a human would ask to get the following response from a chatbot who is knowledgable about aerospace mechanisms:\n",
    "Dynamic Behavior of Ball Bearings under Axial Vibration  Virgil Hinque* and René Seiler* Abstract  The paper addresses the dynamics of ball bearings when exposed to vibration loads along their axis of  rotation. Following common practice in space mechanisms design, the bearings are mounted in either hard  preloaded or soft preloaded pairs. A computer-based model has been developed for the analysis and  prediction of the load-deflection characteristics in bearing systems. Furthermore, the model may be used  to quantify the maximum loads applied on the bearings and the resulting stresses during a vibration test or  a spacecraft launch.  In parallel to the model development, an experimental test program has been carried out in order to get  sufficient data for model correlation. In this context, the paper also elaborates on the post-processing of the  acquired test signals and discusses specific effects, for instance nonlinearities due to the use of snubbers,  in the time domain as well as in the frequency domain.  Introduction  Many space mechanisms use ball bearings for rotation functions. Therefore, assessing the bearing  performance for the relevant environmental conditions is one of the typical challenges faced during the  equipment design process. In this frame, it is common engineering practice to reduce the effect of a sine  and random vibration environment to quasi-static equivalent loads and stresses. The relevant ball bearing  systems often comprise two identical deep-groove or angular-contact bearings in an axially preloaded  configuration. Several studies on the influence of the preload and other parameters on the structural  behavior of such bearing assemblies have been done by the European Space Tribology Laboratory (ESTL).  In a recent investigation, 25 ball bearing cartridges (“test units” or “bearing housings”) with different preload  and snubber configurations were submitted to a series of sine and random vibration tests. The discussion  of findings was mainly based on the analysis of frequency-domain data and bearing damage assessment  via visual inspection [1].  The ESTL investigation inspired a number of ideas for continuation of the research, among others the  development of a computer-based model that would be able to simulate the behavior of the bearing  cartridges, especially those showing nonlinear features in their response. An adequate model should be  able to predict the load transmission across the bearings in static and dynamic load situations. As the main  sizing criterion for ball bearings is based on the allowable peak Hertzian contact pressure between the balls  and the races [2], accurate knowledge of the maximum bearing loads is a key aspect for successful bearing  selection and implementation in a space mechanism.  During the current investigation at the European Space Research and Technology Centre (ESTEC), a  model was built using MATLAB®/Simulink®, with only the axial degree of freedom in a bearing taken into  consideration. Because model correlation with real test results is of importance, a test program  complementary to that reported in [1] has been conducted, with specific focus on the acquisition and  interpretation of time-domain data. The following chapters describe the computer-based model, the design  of the test units, as well as the details of the test campaign and corresponding results. The last part of the  paper is dedicated to the comparison between the model output and the experimental test data.  *European Space Agency (ESA/ESTEC), Noordwijk, The Netherlands Proceedings of the 44th Aerospace Mechanisms Symposium, NASA Glenn Research Center, May 16-18, 2018 NASA/CP—2018-219887 83\n",
    "\"\"\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['Prompt 1: What types of lubricants are to be avoided when designing space mechanisms?',\n",
    "        'Prompt 2: Can you speak to what failures have occurred when using mineral oil lubricants?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up object\n",
    "tags=['prompt1']\n",
    "qa_model_obj=queries.QA_Model(index_name,\n",
    "                    embeddings_model,\n",
    "                    llm,\n",
    "                    k,\n",
    "                    search_type,\n",
    "                    verbose,\n",
    "                    filter_arg=filter_toggle)\n",
    "\n",
    "# Generate a response using your chat model\n",
    "qa_model_obj.query_docs(prompt[0],tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[0]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response using your chat model\n",
    "tags=['prompt2_nofilter']\n",
    "qa_model_obj.update_model(llm=llm,\n",
    "                          filter_arg=False)\n",
    "\n",
    "qa_model_obj.query_docs(prompt[1],\n",
    "                        tags=tags)\n",
    "ai_response=qa_model_obj.result['answer']\n",
    "references=qa_model_obj.sources[-1]\n",
    "\n",
    "display(Markdown(prompt[1]))\n",
    "display(Markdown(ai_response))\n",
    "pprint.pprint(references,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm=ChatOpenAI()\n",
    "# llm.invoke(\"Hello world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
