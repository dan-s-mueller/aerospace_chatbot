{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test partitioning update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.processing import DocumentProcessor\n",
    "from aerospace_chatbot.services import EmbeddingService, RerankService, LLMService, DatabaseService\n",
    "from aerospace_chatbot.processing import QAModel\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "# Set LOCAL_DB_PATH environment variable\n",
    "# os.environ['LOCAL_DB_PATH'] = os.path.abspath('.')\n",
    "\n",
    "# Initialize logger\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type='Pinecone'\n",
    "\n",
    "embedding_service='OpenAI'\n",
    "embedding_model='text-embedding-3-large'\n",
    "\n",
    "rerank_service='Cohere'\n",
    "rerank_model='rerank-v3.5'\n",
    "\n",
    "llm_service='OpenAI'\n",
    "llm_model='gpt-4o'\n",
    "# llm_service='Anthropic'\n",
    "# llm_model='claude-3-5-sonnet-latest'\n",
    "\n",
    "chunk_size=400\n",
    "chunk_overlap=0\n",
    "batch_size=50\n",
    "index_name = 'text-embedding-3-large-test'\n",
    "\n",
    "test_prompt='How does a thermal knife function in a cable based hold down release mechanism?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize services\n",
    "embedding_service = EmbeddingService(\n",
    "    model_service=embedding_service,\n",
    "    model=embedding_model\n",
    ")\n",
    "\n",
    "rerank_service = RerankService(\n",
    "    model_service=rerank_service,\n",
    "    model=rerank_model\n",
    ")\n",
    "\n",
    "llm_service = LLMService(\n",
    "    model_service=llm_service,\n",
    "    model=llm_model,\n",
    ")\n",
    "\n",
    "doc_processor = DocumentProcessor(\n",
    "    embedding_service=embedding_service,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Initialize database service\n",
    "db_service = DatabaseService(\n",
    "    db_type=db_type,\n",
    "    index_name=index_name,\n",
    "    embedding_service=embedding_service,\n",
    "    rerank_service=rerank_service,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Number of PDFs found: 2\n",
      "INFO:aerospace_chatbot.processing.documents:PDFs found: ['gs://processing-pdfs/1999_christiansen_reocr.pdf', 'gs://processing-pdfs/1999_cremers_reocr.pdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gs://processing-pdfs/1999_christiansen_reocr.pdf',\n",
       " 'gs://processing-pdfs/1999_cremers_reocr.pdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = 'processing-pdfs'\n",
    "docs = DocumentProcessor.list_bucket_pdfs(bucket_name)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Loading 2 documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Checking document 1 of 2: gs://processing-pdfs/1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Downloading PDF from GCS: gs://processing-pdfs/1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Bucket name: processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Blob name: 1999_christiansen_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Checking document 2 of 2: gs://processing-pdfs/1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Downloading PDF from GCS: gs://processing-pdfs/1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Bucket name: processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Blob name: 1999_cremers_reocr.pdf\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning 2 documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning ./document_processing/1999_christiansen_reocr.pdf Locally...\n",
      "INFO:unstructured_inference:Reading PDF for file: ./document_processing/1999_christiansen_reocr.pdf ...\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioned data saved at ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json\n",
      "INFO:aerospace_chatbot.processing.documents:Uploaded ./document_processing/1999_christiansen_reocr.pdf and ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json to processing-pdfs\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioning ./document_processing/1999_cremers_reocr.pdf Locally...\n",
      "INFO:unstructured_inference:Reading PDF for file: ./document_processing/1999_cremers_reocr.pdf ...\n",
      "INFO:aerospace_chatbot.processing.documents:Partitioned data saved at ./document_processing/partitioned/1999_cremers_reocr-partitioned.json\n",
      "INFO:aerospace_chatbot.processing.documents:Uploaded ./document_processing/1999_cremers_reocr.pdf and ./document_processing/partitioned/1999_cremers_reocr-partitioned.json to processing-pdfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./document_processing/partitioned/1999_christiansen_reocr-partitioned.json',\n",
       " './document_processing/partitioned/1999_cremers_reocr-partitioned.json']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_docs = doc_processor.load_and_partition_documents(docs,partition_by_api=False, upload_bucket=bucket_name)\n",
    "partitioned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aerospace_chatbot.processing.documents:Chunking documents...\n",
      "INFO:aerospace_chatbot.processing.documents:Total number of chunks: 91\n",
      "INFO:aerospace_chatbot.processing.documents:Output paths: ['./document_processing/chunked/1999_christiansen_reocr-chunked.json', './document_processing/chunked/1999_cremers_reocr-chunked.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking ./document_processing/partitioned/1999_christiansen_reocr-partitioned.json...\n",
      "Chunked data saved at ./document_processing/chunked/1999_christiansen_reocr-chunked.json\n",
      "Chunking ./document_processing/partitioned/1999_cremers_reocr-partitioned.json...\n",
      "Chunked data saved at ./document_processing/chunked/1999_cremers_reocr-chunked.json\n"
     ]
    }
   ],
   "source": [
    "chunk_obj, output_paths = doc_processor.chunk_documents(partitioned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unstructured.documents.elements.ElementMetadata object at 0x357693ed0>\n"
     ]
    }
   ],
   "source": [
    "chunk_obj.chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_obj.chunk_convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_source.url': 'gs://processing-pdfs/1999_christiansen_reocr.pdf', 'data_source.record_locator.protocol': 'gs', 'data_source.record_locator.remote_file_path': 'gs://processing-pdfs', 'file_directory': './document_processing', 'filename': '1999_christiansen_reocr.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-01-20T19:06:40', 'page_number': 1, 'orig_elements': 'eJztmF1v2zYUhv8KoWtb5qdI+mqem27BACeIXWBDEAgUScUCZNGQ6LZB0f8+knLWrPOK5sI3Ri7PS77iIfmIOOT9l8y2dmc7XzYmm4OM1rUymFeKW2NrKguEkRKUW1woAkmVTUC2s14Z5VXo/yXTzvWm6ZS3Q4pb9eQOvtza5nHrg4IxhMFzlD81xm+DinhS967pfPTd31Mu8iChgufiYQL+iSXKaYwRLlhOTwnJEZRseBq83cVZ3DafbbveK22zr6EhJlsO7tCHOObYWx2SLlunlXd9kva98067NrofhzjL3u6ct2XdtLbcq5R1aJnPZqGrtsPQdI/TvamHNMKhb/+vfYaklKXe9s3gG9UNtit763Sfh8YxO+ut9o3rSt2qYSiDvwofgzlnmBehQ0rBNCHrkO1THCefGacPadu+jZYdu3ZqF6eZ/WjcY1f/tE9d1X7fNmExQhKzY3OruseDeky7ep/Z8PmHpA6+3DnT1I1NvGCI2RSiKYYbJOewmFMY3fvgLLvDrrJxeVGcprefIw/Z+8V6AxbLzfXqN7C6WU1v/7q72Vwtf19dLwGCf6zA+up2cbfYXN+swOrDJn7tOc1N49u0o99TW1WsCEgyVUtDsZVSVciKQAjXjBOIzkYtC5TyCcAEjtQ+x4zlRYIUMZmLU0JyXCS1VAhxcdSutfMeLF/kNQGjtmmqqvF+mIB36mNjwDv3yXY/RS3VSlQVFbWlQgoqqGa1EnUtsMCygvps1HLIcxwYlGQ8So8xQQHeBCkkLEcnhNFxidQWqBD8IqhNSv+Ks/FfmHvVh60Fd3awqtdbsHT93vUp25dMr1QfxY92E42nTmQlIKeoMgQZqa3liiGkKh2yMbTi56sjeGA2HD8E8xwmto8xg8eyAWKa2P6PkByXyDaBWLI3tqmEEqzUwTftYQhgH3oPVq4PO/IasA21nFVMKCwxtJLRQlkulCZWEWm4OB/YmMWCmDA+VhLPscAj6AgiclpIjksEO+wzIW9g/+oOrbH9BCxvgIAMhiX5sF68imobj2aEMKYFUjrc9KQh1FYQC2pZgeD5CuhQUcTDV4bdTAX0GNOALh7rZSry4oQwOi6TagbpG9WbQOh+6zo7BwSSKSNwiiRmYAbeL/78pmE6Xu9+GnWIa6JMfOFgtgpFiuI6QG6NYUoKUxXne+EoSKxEKMY5Si8cx5iyXI4PGhzn5JSQHJeIOuG0eKtMsqvpTjXtHAzxQql/GcYiPNduF2hP4tN3oomXTPNS/ME/8PA3UAs4Zw==', 'element_id': '211a24e3-9952-4c05-af7a-97ee0979475c', 'type': 'CompositeElement', 'chunk_size': 400, 'chunk_overlap': 0}\n"
     ]
    }
   ],
   "source": [
    "print(chunk_obj.chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'data_source.url' in chunk_obj.chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     db_service.initialize_database(clear=False)\n",
    "# except ValueError as e:\n",
    "#     print(f\"Database initialization failed: {str(e)}\")\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_service.index_data(chunk_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Database not initialized. Please ensure database is initialized before getting retriever.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m k_rerank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 5\u001b[0m qa_model \u001b[38;5;241m=\u001b[39m \u001b[43mQAModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_service\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_service\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_service\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_service\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_retrieve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_retrieve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_rerank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_rerank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/processing/queries.py:53\u001b[0m, in \u001b[0;36mQAModel.__init__\u001b[0;34m(self, db_service, llm_service, k_retrieve, k_rerank, style, memory_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Get retrievers from database services\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_retrieve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Compile workflow\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Set to default memory config\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/src/aerospace_chatbot/services/database.py:100\u001b[0m, in \u001b[0;36mDatabaseService.get_retriever\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     97\u001b[0m search_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_retriever_args(k)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase not initialized. Please ensure database is initialized before getting retriever.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_standard_retriever(search_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Database not initialized. Please ensure database is initialized before getting retriever."
     ]
    }
   ],
   "source": [
    "# k_retrieve=20\n",
    "# k_rerank=5\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# qa_model = QAModel(\n",
    "#     db_service=db_service,\n",
    "#     llm_service=llm_service,\n",
    "#     k_retrieve=k_retrieve,\n",
    "#     k_rerank=k_rerank,\n",
    "#     memory_config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_model.query(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.result[-1]['references'])\n",
    "# print(qa_model.sources[-1])\n",
    "# print(qa_model.scores[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(qa_model.ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above section first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, RemoveMessage\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing_extensions import List\n",
    "from typing import List, Literal, Tuple\n",
    "\n",
    "# import cohere\n",
    "# import os\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from aerospace_chatbot.services.prompts import InLineCitationsResponse, AltQuestionsResponse, style_mode, CHATBOT_SYSTEM_PROMPT, QA_PROMPT, SUMMARIZE_TEXT, GENERATE_SIMILAR_QUESTIONS_W_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_service.retriever\n",
    "llm = llm_service.get_llm()\n",
    "# memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs with standard retriever\n",
    "retrieved_docs = retriever.invoke(test_prompt)\n",
    "\n",
    "# retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "# co = cohere.ClientV2(COHERE_API_KEY)\n",
    "# rerank_model = \"rerank-v3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = co.models.list()\n",
    "# rerank_model_info = next((model for model in model_list.models if model.name == rerank_model), None)\n",
    "# print(rerank_model_info)\n",
    "\n",
    "# print(model_list.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token count for each doc\n",
    "# max_context_length = 4096\n",
    "\n",
    "# token_count_list = []\n",
    "# for doc in retrieved_docs[0]:\n",
    "#     tokens = co.tokenize(\n",
    "#         model=rerank_model,\n",
    "#         text=doc.page_content,\n",
    "#     )\n",
    "#     token_count_list.append(len(tokens.tokens))\n",
    "    # print(token_count)\n",
    "\n",
    "# print(token_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cohere_rerank(query: str, retrieved_docs: List[Tuple[Document, float]], top_n: int = None):\n",
    "#     # retrieved_docs contains a list of tuples, where the first element is the document and the second is the score\n",
    "#     # Cohere's rerank expects a list of strings; we'll supply the page_content\n",
    "\n",
    "#     if top_n is None:\n",
    "#         top_n = 3\n",
    "#     elif top_n < 3:\n",
    "#         raise ValueError(\"top_n must be at least 3\")\n",
    "#     elif top_n > len(retrieved_docs):\n",
    "#         raise ValueError(\"top_n must be less than or equal to the number of retrieved documents\")\n",
    "    \n",
    "#     inputs = [doc.page_content for doc, _ in retrieved_docs]\n",
    "\n",
    "#     # Call Cohere's Rerank endpoint\n",
    "#     response = co.rerank(\n",
    "#         model=rerank_model,\n",
    "#         query=query,\n",
    "#         documents=inputs,\n",
    "#         top_n=top_n\n",
    "#     )\n",
    "\n",
    "#     # Create a dictionary to map document IDs to rerank scores\n",
    "#     rerank_scores = {retrieved_docs[i][0].id: item.relevance_score for i, item in enumerate(response.results)}\n",
    "\n",
    "#     # Create list of (doc, original_score, rerank_score) tuples\n",
    "#     doc_scores = []\n",
    "#     for doc, original_score in retrieved_docs:\n",
    "#         rerank_score = rerank_scores.get(doc.id, None)  # Get the rerank score or None if not available\n",
    "#         doc_scores.append((doc, original_score, rerank_score))\n",
    "\n",
    "#     # Sort docs by rerank score in descending order, placing those without a rerank score at the end\n",
    "#     doc_scores_sorted = sorted(doc_scores, key=lambda x: (x[2] is not None, x[2]), reverse=True)\n",
    "\n",
    "#     return doc_scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = db_service.rerank(test_prompt, retrieved_docs, top_n=k_rerank)\n",
    "# reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context to the prompt\n",
    "# TODO update this to use the doc.id. Do some testing to check this works.\n",
    "docs_content=\"\"\n",
    "for i, (doc, original_score, rerank_score) in enumerate(reranked_docs):\n",
    "    # Source IDs in the order they show in in the array. Indexed from 0.\n",
    "    if rerank_score is not None:    # Only include docs with a rerank score\n",
    "        docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "# print(docs_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InLineCitationsResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should validate\n",
    "valid_response = InLineCitationsResponse(content=\"\"\"\n",
    "The actuator was tested under high pressure <source id=\"1\">. \n",
    "Material properties were measured over 50 cycles <source id=\"2\">.\n",
    "Thermal resistance improved by 30% <source id=\"3\">.\n",
    "\"\"\",\n",
    "citations=[\"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't validate\n",
    "try:\n",
    "    invalid_response = InLineCitationsResponse(content=\"\"\"\n",
    "    The actuator was tested under high pressure [1]. \n",
    "    Material properties were measured under load <source id=\"x\">.\n",
    "    \"\"\",\n",
    "    citations=[\"1\", \"x\"]\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT_TEST=PromptTemplate(\n",
    "    template=\n",
    "\"\"\"\n",
    "# **System Prompt**\n",
    "\n",
    "Your name is **Aerospace Chatbot**, a specialized assistant for flight hardware design and analysis in aerospace engineering. You will function as a knowledgeable replacement for an expert in aerospace flight hardware design, testing, analysis, and certification.\n",
    "\n",
    "> **Important Note:** The **Sources and Context** you are provided are ranked from most relevant to least relevant by a state-of-the-art retrieval and ranking tool. Please take this ranking into consideration when determining which sources to cite.\n",
    "\n",
    "Use only the **Sources and Context** provided to answer the **User Question**. **Do not use outside knowledge**, and strictly follow these rules:\n",
    "\n",
    "---\n",
    "\n",
    "## **Rules**:\n",
    "\n",
    "1. **Answer only based on the provided Sources and Context.**  \n",
    "   - If the information is not available in the Sources and Context, respond with:  \n",
    "     *\"I don’t know the answer to that based on the information provided. You might consider rephrasing your question or asking about a related topic.\"*  \n",
    "   - **If no sources are truly relevant to the answer, do not cite any sources.**\n",
    "\n",
    "2. **Do not make up or infer answers.**  \n",
    "   - Stay accurate and factual at all times.\n",
    "\n",
    "3. **Provide highly detailed, explanatory answers.**  \n",
    "   - Include **as many specific details from the original context** as possible to thoroughly address the user’s question.\n",
    "\n",
    "4. **Provide responses in English only** and format them using **Markdown** for clarity.\n",
    "\n",
    "5. **Cite Sources in context** using the exact format `<source id=\"#\">`:  \n",
    "   - `#` – Represents the numerical order of the source as provided in the Sources and Context.  \n",
    "   - **The `source` tag must be present for every source referenced in the response.**  \n",
    "   - **Do not add, omit, or modify any part of the citation format.**  \n",
    "   \n",
    "   **Examples (Correct):**  \n",
    "   > The actuator was tested under extreme conditions <source id=\"1\">.  \n",
    "   > A secondary material exhibited increased yield strength <source id=\"2\">.  \n",
    "   > Additional research confirmed thermal properties <source id=\"3\">.  \n",
    "\n",
    "   **Examples (Incorrect – Must Be Rejected):**  \n",
    "   > Testing yielded higher efficiency [1] (Incorrect bracket format)  \n",
    "   > <source id=\"1\" > (Extra space after `id`)  \n",
    "   > <source id=\"a\"> (Non-numeric ID)  \n",
    "   > <source id=\"1,2\"> (Multiple IDs in one tag – invalid)  \n",
    "\n",
    "6. **Every sentence or paragraph that uses a source must cite it with the format `<source id=\"#\">`.**  \n",
    "   - **Do not group multiple sources into a single tag.**  \n",
    "   - Each source must have its own, clearly separated citation.  \n",
    "   - For example:  \n",
    "     > The actuator uses a reinforced composite structure <source id=\"1\">.  \n",
    "     > This design was validated through multiple tests <source id=\"2\">.\n",
    "\n",
    "7. **Validation Requirement:**  \n",
    "   - If the response contains references without the exact `<source id=\"#\">` format, the response must be flagged or rejected.  \n",
    "   - Every source used must have a corresponding citation in the response.  \n",
    "   - **No source should be referenced without explicit citation.**\n",
    "\n",
    "8. **Suggest related or alternative questions** if applicable, to help the user find relevant information within the corpus.\n",
    "\n",
    "9. **Always cite Source ID: 1**  \n",
    "   - Always provide a citation for Source ID: 1 unless it is entirely irrelevant to the user’s question (in which case explicitly omit it).  \n",
    "   - Follow Rule #10 for the other sources.\n",
    "\n",
    "10. **Give preference to citing top-ranked sources, provided in order of highest to lowest relevance.**  \n",
    "   - If the first sources in the list (i.e., the most relevant or highest-ranked) contain information that addresses the user’s question, cite them first.  \n",
    "   - Then cite additional sources only if they contain new or non-redundant details.  \n",
    "   - If the top-ranked sources are not relevant, skip them.\n",
    "\n",
    "---\n",
    "**Sources and Context**:\n",
    "{context}\n",
    "---\n",
    "\n",
    "---\n",
    "**User Question**:\n",
    "{question}\n",
    "---\n",
    "\n",
    "---\n",
    "{format_instructions}\n",
    "---\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": PydanticOutputParser(pydantic_object=InLineCitationsResponse).get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = llm.invoke(QA_PROMPT_TEST.format(context=docs_content, question=test_prompt))\n",
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_response = PydanticOutputParser(pydantic_object=InLineCitationsResponse).parse(raw_output.content)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AltQuestionsResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should validate\n",
    "valid_response = AltQuestionsResponse(\n",
    "    questions=[\"What is the purpose of the actuator?\", \"What are the key components of the mechanism?\", \"How does the mechanism work?\"],\n",
    ")\n",
    "print(valid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't validate\n",
    "try:\n",
    "    invalid_response = AltQuestionsResponse(\n",
    "        questions=[\"What is the purpose of the actuator?\", \"What are the key components of the mechanism?\"],\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = llm.invoke(GENERATE_SIMILAR_QUESTIONS_W_CONTEXT.format(context=docs_content, question=test_prompt))\n",
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_response = PydanticOutputParser(pydantic_object=AltQuestionsResponse).parse(raw_output.content)\n",
    "print(parsed_response.questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class State(MessagesState):\n",
    "#     context: List[Tuple[Document, float, float]]\n",
    "#     cited_sources: List[Tuple[Document, float, float]]\n",
    "#     summary: str\n",
    "\n",
    "# # Define application steps\n",
    "# def retrieve(state: State):\n",
    "#     \"\"\"\n",
    "#     Retrieve the documents from the database.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: retrieve\")\n",
    "\n",
    "#     # Retrieve docs\n",
    "#     retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n",
    "#     logger.info(f\"Retrieved docs\")\n",
    "#     # Rerank docs\n",
    "#     # reranked_docs = cohere_rerank(\n",
    "#     #     state[\"messages\"][-1].content, \n",
    "#     #     retrieved_docs, \n",
    "#     #     top_n=k_rerank\n",
    "#     # )\n",
    "#     reranked_docs = db_service.rerank(\n",
    "#         state[\"messages\"][-1].content, \n",
    "#         retrieved_docs, \n",
    "#         top_n=k_rerank\n",
    "#     )\n",
    "#     logger.info(f\"Reranked docs\")\n",
    "\n",
    "#     return {\"context\": reranked_docs}\n",
    "\n",
    "# def generate_w_context(state: State):\n",
    "#     \"\"\"\n",
    "#     Call the model with the prompt with context.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: generate_w_context\")\n",
    "\n",
    "#     # Get the summary, add system prompt\n",
    "#     summary = state.get(\"summary\", \"\")\n",
    "#     system_prompt = CHATBOT_SYSTEM_PROMPT.format(style_mode=style_mode(style))\n",
    "#     logger.info(f\"generate_w_context system prompt: {system_prompt.content}\")\n",
    "#     if summary:\n",
    "#         system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "#         messages = [system_prompt] + [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "#     else:\n",
    "#         messages = [system_prompt] + state[\"messages\"]\n",
    "\n",
    "#     # Add context to the prompt\n",
    "#     docs_content=\"\"\n",
    "#     for i, (doc, retrieved_score, rerank_score) in enumerate(state[\"context\"]):\n",
    "#         # Source IDs in the order they show in in the array. Indexed from 1, retrieve with 0 index.\n",
    "#         if rerank_score is not None:    # Only include docs with a rerank score\n",
    "#             docs_content += f\"Source ID: {i+1}\\n{doc.page_content}\\n\\n\"\n",
    "\n",
    "#     # Prompt with context and pydantic output parser\n",
    "#     prompt_with_context = QA_PROMPT.format(\n",
    "#         context=docs_content,\n",
    "#         question=state[\"messages\"][-1].content, \n",
    "#     )\n",
    "#     # Replace the last message (user question) with the prompt with context, return LLM response\n",
    "#     messages[-1] = prompt_with_context \n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     # Parse the response. This will return a InLineCitationsResponse object. \n",
    "#     # This object has two fields: content and citations.\n",
    "#     # Replace the last message with the content of the parsed and validated response. \n",
    "#     # AIMessage metadata will be incorrect.\n",
    "#     parsed_response = PydanticOutputParser(pydantic_object=InLineCitationsResponse).parse(response.content)\n",
    "#     response.content = parsed_response.content\n",
    "\n",
    "#     # Return cited_sources as the list of tuples that matched the citations.\n",
    "#     existing_cited_sources = state.get(\"cited_sources\", [])  # Grab whatever might already be in cited_sources\n",
    "#     cited_sources = [state[\"context\"][int(citation)-1] for citation in parsed_response.citations]\n",
    "#     existing_cited_sources.append(cited_sources)  # Append the new list as a sublist\n",
    "#     state[\"cited_sources\"] = existing_cited_sources\n",
    "\n",
    "#     # Update the state messages with the messages updated in this node.\n",
    "#     state[\"messages\"] = messages\n",
    "#     return {\"messages\": [response], \n",
    "#             \"cited_sources\": state[\"cited_sources\"]}\n",
    "\n",
    "# def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "#     \"\"\"\n",
    "#     Define the logic for determining whether to end or summarize the conversation\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: should_continue\")\n",
    "\n",
    "#     # If there are more than six messages, then we summarize the conversation\n",
    "#     messages = state[\"messages\"]\n",
    "#     if len(messages) > 6:\n",
    "#         logger.info(f\"Summarizing conversation\")\n",
    "#         return \"summarize_conversation\"\n",
    "    \n",
    "#     # Otherwise just end\n",
    "#     logger.info(f\"Ending conversation\")\n",
    "#     # logger.info(f\"Messages before ending: {messages}\")\n",
    "#     return END\n",
    "\n",
    "# def summarize_conversation(state: State):\n",
    "#     \"\"\"\n",
    "#     Summarize the conversation\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Node: summarize_conversation\")\n",
    "\n",
    "#     summary = state.get(\"summary\", \"\")\n",
    "#     if summary:\n",
    "#         # If a summary already exists, extend it\n",
    "#         summary_message = SUMMARIZE_TEXT.format(\n",
    "#             summary=summary,\n",
    "#             augment=\"Extend the summary provided by taking into account the new messages above.\"\n",
    "#         )\n",
    "#     else:\n",
    "#         # If no summary exists, create one\n",
    "#         summary_text=\"\"\"---\\n**Conversation Summary to Date**:\\n{summary}\\n---\"\"\"\n",
    "#         summary_message = SUMMARIZE_TEXT.format(\n",
    "#             summary=summary_text,\n",
    "#             augment=\"Create a summary of the conversation above.\"\n",
    "#         )\n",
    "\n",
    "#     messages = state[\"messages\"] + [summary_message]\n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     # Prune messages. This deletes all but the last two messages\n",
    "#     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "#     return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile application and test\n",
    "# workflow = StateGraph(State)\n",
    "\n",
    "# # Define nodes\n",
    "# workflow.add_node(\"retrieve\", retrieve) \n",
    "# workflow.add_node(\"generate_w_context\", generate_w_context)\n",
    "# workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "# # Define edges\n",
    "# workflow.add_edge(START, \"retrieve\")\n",
    "# workflow.add_edge(\"retrieve\", \"generate_w_context\")\n",
    "\n",
    "# # We now add a conditional edge\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"generate_w_context\",   # Define the start node. We use `generate_w_context`. This means these are the edges taken after the `conversation` node is called.\n",
    "#     should_continue,    # Next, pass in the function that will determine which node is called next.\n",
    "# )\n",
    "\n",
    "# # Add a normal edge from `summarize_conversation` to END. This means that after `summarize_conversation` is called, we end.\n",
    "# workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# # Compile the workflow\n",
    "# app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model.workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"197\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'My name is Dan. Please tell me about some interesting mecanism designs.'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "qa_model.query(prompt)\n",
    "for message in qa_model.result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model.result['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How have these mecahnisms been tested?'\n",
    "prompt = 'How is release indicated from the functional response of the mechanism?'\n",
    "result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "for message in result['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['context'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PDF annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, base64, zlib\n",
    "\n",
    "def extract_orig_elements(orig_elements):\n",
    "    decoded_orig_elements = base64.b64decode(orig_elements)\n",
    "    decompressed_orig_elements = zlib.decompress(decoded_orig_elements)\n",
    "    return decompressed_orig_elements.decode('utf-8')\n",
    "\n",
    "orig_elements = extract_orig_elements(result['context'][2][0].metadata['orig_elements'])\n",
    "orig_elements = json.loads(orig_elements)\n",
    "print(orig_elements)\n",
    "print(len(orig_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_range = [orig_elements[0]['metadata']['page_number'], orig_elements[-1]['metadata']['page_number']]\n",
    "print(page_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aerospace_chatbot.ui.utils import display_source_highlights\n",
    "\n",
    "annotated_pdfs = display_source_highlights(result['context'][:k_rerank])\n",
    "print(len(annotated_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from IPython.display import display, Image\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "pdf_bytes = annotated_pdfs[1].read()\n",
    "pdf_document = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "\n",
    "# Display each page\n",
    "for page in pdf_document:\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(1, 1))  # 2x zoom for better resolution\n",
    "    with NamedTemporaryFile(suffix='.png') as tmp:\n",
    "        pix.save(tmp.name)\n",
    "        display(Image(filename=tmp.name))\n",
    "\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary memory test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'How old are you?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some lessons learned about these mechanisms?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'What are some problems that have occurred?'\n",
    "# result = qa_model.workflow.invoke({\"messages\": [(\"human\", prompt)]}, config)\n",
    "# for message in result['messages']:\n",
    "#     message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
