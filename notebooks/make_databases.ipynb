{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 13:16:52.199 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-11-02 13:16:52.200 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import admin\n",
    "import data_processing\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to split chunking from upserting\n",
    "def create_upsert(index_type,index_name,query_model,rag_type,chunker,summary_llm):\n",
    "    # Set index names for special databases\n",
    "    if rag_type == 'Parent-Child':\n",
    "        index_name = index_name + '-parent-child'\n",
    "    if rag_type == 'Summary':\n",
    "        index_name = index_name + '-' + summary_llm.model_name.replace('/', '-').replace(' ','-').lower() + '-summary' \n",
    "\n",
    "    try:\n",
    "        vectorstore = data_processing.initialize_database(index_type, \n",
    "                                            index_name, \n",
    "                                            query_model,\n",
    "                                            rag_type=rag_type,\n",
    "                                            clear=True, \n",
    "                                            local_db_path=os.getenv('LOCAL_DB_PATH'),\n",
    "                                            init_ragatouille=True,\n",
    "                                            show_progress=False)\n",
    "        print(f\"Database {index_name} created.\")\n",
    "        vectorstore, _ = data_processing.upsert_docs(index_type, \n",
    "                                        index_name,\n",
    "                                        vectorstore,\n",
    "                                        chunker,\n",
    "                                        batch_size=400,\n",
    "                                        show_progress=False,\n",
    "                                        local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database {index_name} upserted chunks.\")\n",
    "    except Exception as e:  # If there is an error, be sure to delete the database\n",
    "        data_processing.delete_index(index_type, \n",
    "                                    index_name,\n",
    "                                    rag_type,\n",
    "                                    local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database deleted: {index_name}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets, Models, Docs, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, assign models\n",
    "json_file_path = \"databases_minimal.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "sb={}\n",
    "query_params=setup_data['query_models']\n",
    "query_models=[]\n",
    "for model in query_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    query_models.append(admin.get_query_model(sb, secrets))\n",
    "\n",
    "llm_params=setup_data['llms']\n",
    "llms=[]\n",
    "for model in llm_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    llms.append(admin.set_llm(sb, secrets))\n",
    "\n",
    "chunk_params=setup_data['chunk_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs\n",
    "data_folder='../data/AMS'\n",
    "docs= glob.glob(os.path.join(data_folder,'*.pdf'))   # Only get the PDFs in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tag='ams'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, standard, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=llms[0] # gpt-4o\n",
    "\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 340 chunks from 340 pages.\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-ams-2merge-0 created.\n",
      "Database text-embedding-3-large-ams-2merge-0 upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') \n",
    "                + '-' + index_tag + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
