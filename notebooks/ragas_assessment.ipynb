{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta, archive_db, get_docs_questions_df\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "    if Path(file_name).exists():\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset_loop(lcdocs,eval_size,n_questions,fname):\n",
    "    # Check if testset.csv exists, use, or generate the synthetic dataset. If it doesn't exist, just loop through everything.\n",
    "    # If a partial export exists, it will loop through the remaining indices.\n",
    "    # If a few entries are missing it will loop through that full index.\n",
    "    if os.path.exists(fname):\n",
    "        # Import testset.csv into a DataFrame\n",
    "        df_testset = pd.read_csv(fname)\n",
    "\n",
    "        full_index_list = list(range(0, len(lcdocs), eval_size))\n",
    "        index_values = df_testset['index'].unique()\n",
    "        index_loop = [] # Indices to index over\n",
    "\n",
    "        for index_value in index_values:\n",
    "            if len(df_testset[df_testset['index'] == index_value]) != n_questions:\n",
    "                index_loop.append(index_value)\n",
    "\n",
    "        index_loop += list(set([index for index in full_index_list if index not in index_values]))\n",
    "        index_loop = sorted(index_loop)\n",
    "    else:\n",
    "        df_testset = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "        index_loop = list(range(0, len(lcdocs), eval_size))\n",
    "    return df_testset, index_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config):\n",
    "    # Loop through eval_size chunks of the dataset to generate the testset. Export to csv along the way.\n",
    "\n",
    "    # Initialize the DataFrame to store the testset, determine what parts of the dataset need to be looped over\n",
    "    df_testset, index_loop = synthetic_dataset_loop(lcdocs,eval_size,n_questions,fname)\n",
    "    print(f\"Index loop: {index_loop}\")\n",
    "\n",
    "    for i in index_loop:\n",
    "        print(f\"Processing index {i} to {i+eval_size}...\")\n",
    "        lcdocs_eval = lcdocs[i:i+eval_size]\n",
    "\n",
    "        testset = generator.generate_with_langchain_docs(lcdocs_eval, \n",
    "                                                        test_size=n_questions,\n",
    "                                                        with_debugging_logs=True,\n",
    "                                                        is_async=True,\n",
    "                                                        run_config=run_config,\n",
    "                                                        raise_exceptions=False)\n",
    "\n",
    "        # df_testset_new = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "        df_testset_new = testset.to_pandas()\n",
    "        df_testset_new['index'] = i\n",
    "        df_testset_new['eval_size'] = eval_size\n",
    "\n",
    "        # Export the testset to a csv file with the index\n",
    "        if os.path.exists(fname):\n",
    "            df_testset_new.to_csv(fname, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df_testset_new.to_csv(fname, index=False)\n",
    "        df_testset = pd.concat([df_testset, df_testset_new])\n",
    "    return df_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_qa, df_docs):\n",
    "    # df_questions_answers_out=df_questions_answers_in.copy()\n",
    "    \n",
    "    # Generate responses using RAG with input parameters\n",
    "    for i, row in df_qa.iterrows():\n",
    "        if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "            print(f\"Processing question {i+1}/{len(df_qa)}\")\n",
    "\n",
    "            # Use the QA model to query the documents\n",
    "            qa_obj=queries.QA_Model(index_type,\n",
    "                            index_name,\n",
    "                            query_model,\n",
    "                            llm,\n",
    "                            **QA_model_params)\n",
    "            qa_obj.query_docs(row['question'])\n",
    "            response=qa_obj.result\n",
    "\n",
    "            df_qa.loc[df_qa.index[i], \"answer\"] = response['answer'].content\n",
    "\n",
    "            ids=[_stable_hash_meta(source_document.metadata)\n",
    "                for source_document in response['references']]\n",
    "            df_qa.loc[df_qa.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "            # Save the response to cache file\n",
    "            response_dict = {\n",
    "                \"question\": row['question'],\n",
    "                \"answer\": response['answer'].content,\n",
    "                \"source_documents\": ids,\n",
    "            }\n",
    "            write_dict_to_file(response_dict, os.path.join('output',f'rag_response_cache_{index_name}.json'))\n",
    "\n",
    "    # Get the context documents content for each question\n",
    "    source_documents_list = []\n",
    "    for cell in df_qa['source_documents']:\n",
    "        cell_list = cell.strip('[]').split(', ')\n",
    "        context=[]\n",
    "        for cell in cell_list:\n",
    "            context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "        source_documents_list.append(context)\n",
    "    df_qa[\"contexts\"]=source_documents_list\n",
    "\n",
    "    # Addtionaly get embeddings for questions\n",
    "    if not Path(os.path.join('output',f'question_embeddings_{index_name}.pickle')).exists():\n",
    "        question_embeddings = [\n",
    "            query_model.embed_query(question)\n",
    "            for question in df_qa[\"question\"]\n",
    "        ]\n",
    "        with open(os.path.join('output',f'question_embeddings_{index_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(question_embeddings, f)\n",
    "\n",
    "    question_embeddings = pickle.load(open(os.path.join('output',f'question_embeddings_{index_name}.pickle'), \"rb\"))\n",
    "    df_qa[\"embedding\"] = question_embeddings\n",
    "    return df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rag(index_name, df_qa):\n",
    "    # df_questions_answers_out=df_questions_answers_in.copy()\n",
    "\n",
    "    # Add answer correctness column, fill in if it exists\n",
    "    df_qa = add_cached_column_from_file(\n",
    "        df_qa, os.path.join('output',f'ragas_result_cache_{index_name}.json', \"question\"), \"answer_correctness\"\n",
    "    )\n",
    "\n",
    "    # Sometimes ground_truth does not provide a response. Just filter those out.\n",
    "    df_qa = df_qa[df_qa['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "    df_qa\n",
    "\n",
    "    # Prepare the dataframe for evaluation\n",
    "    df_qa_eval = df_qa.copy()\n",
    "\n",
    "    # Evaluate the answer correctness if not already done\n",
    "    fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "    for i, row in df_qa_eval.iterrows():\n",
    "        print(i, row[\"question\"])\n",
    "        # TODO add multiple eval criteria\n",
    "        if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "            evaluation_result = evaluate(\n",
    "                Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "                [answer_correctness],\n",
    "            )\n",
    "            df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "                \"answer_correctness\"\n",
    "            ]\n",
    "\n",
    "            # optionally save the response to cache\n",
    "            response_dict = {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "            }\n",
    "            write_dict_to_file(response_dict, os.path.join('output',f'ragas_result_cache_{index_name}.json'))\n",
    "\n",
    "    # write the answer correctness to the original dataframe\n",
    "    df_qa[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n",
    "\n",
    "    return df_qa_eval, df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_viz_prep(index_name,df_qa_eval,df_docs):\n",
    "    # This section adds a column to df_documents containing the ids of the questions that used the document as source. \n",
    "\n",
    "    # add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "    # Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "    df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "    # Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "    agg = (\n",
    "        df_questions_exploded.groupby(\"source_documents\")\n",
    "        .agg(\n",
    "            num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "            question_ids=(\n",
    "                \"id\",\n",
    "                lambda x: list(x),\n",
    "            ),  # List of question IDs referencing the document\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"source_documents\": \"id\"})\n",
    "    )\n",
    "\n",
    "    # Merge the aggregated information back into df_documents\n",
    "    df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "    # Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "    df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    # Replace NaN values in 'num_questions' with 0\n",
    "    df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    df_visualize = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "    df_questions = df_visualize[~df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_questions[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_questions = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "\n",
    "\n",
    "    df_without_questions = df_visualize[df_visualize[\"question\"].isna()]\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_without_questions[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_docs = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "    df_visualize[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "    umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "        df_visualize[\"embedding\"].values.tolist()\n",
    "    )\n",
    "    umap_all = umap.transform(df_visualize[\"embedding\"].values.tolist())\n",
    "    df_visualize[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "    # find the nearet question (by embedding) for each document\n",
    "    question_embeddings = np.array(df_visualize[df_visualize[\"question\"].notna()][\"embedding\"].tolist())\n",
    "\n",
    "    df_visualize[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "        np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "        for doc_emb in df_visualize[\"embedding\"].values\n",
    "    ]\n",
    "\n",
    "    # write the dataframe to parquet for later use\n",
    "    df_visualize.to_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "    return df_visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-large-0merge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'text-embedding-3-large-2merge-0',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-0merge-400',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-0merge-400-parent-child',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-2merge-0-queries',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them. Uncomment if desired, takes a while.\n",
    "# for db in dbs:\n",
    "#     df_temp_chroma=archive_db('ChromaDB',db['index_name'],db['query_model'],export_pickle=True)\n",
    "\n",
    "# df_temp_chroma.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database for determining synthetic dataset\n",
    "db=dbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=db['index_name'],\n",
    "                        embedding_function=db['query_model'])  \n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "lcdocs_chroma = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]\n",
    "\n",
    "print(len(lcdocs_chroma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format docs into dataframe\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "# synthetic_generator_llm = ChatOpenAI(base_url='https://api-inference.huggingface.co/v1',\n",
    "#                                     model='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "#                                     api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "#                                     temperature=0.1,\n",
    "#                                     max_tokens=500)\n",
    "\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "# synthetic_critic_llm = ChatOpenAI(base_url='https://api-inference.huggingface.co/v1',\n",
    "#                                     model='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "#                                     api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "#                                     temperature=0.1,\n",
    "#                                     max_tokens=500)\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=db['query_model']  # Set to be the same as the database\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "# run_config=None\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "eval_size=50    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "n_questions=5   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{db['index_name']}.csv\")\n",
    "lcdocs=lcdocs_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 629, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 588, in _check_running\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loop: [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150, 2200]\n",
      "Processing index 0 to 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1040: RuntimeWarning: coroutine 'Runner._aresults' was never awaited\n",
      "  self._invoke_excepthook(self)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_testset\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_testset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mgenerate_testset\u001b[0;34m(lcdocs, generator, eval_size, n_questions, fname, run_config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39meval_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m lcdocs_eval \u001b[38;5;241m=\u001b[39m lcdocs[i:i\u001b[38;5;241m+\u001b[39meval_size]\n\u001b[0;32m---> 12\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcdocs_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mis_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# df_testset_new = pd.DataFrame()  # Initialize an empty DataFrame\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df_testset_new \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/testset/generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/testset/docstore.py:215\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    211\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    212\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    214\u001b[0m ]\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/testset/docstore.py:254\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    252\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ipykernel/iostream.py:123: RuntimeWarning: coroutine 'as_completed.<locals>.sema_coro' was never awaited\n",
      "  await self._event_pipe_gc()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "df_testset=generate_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f\"rag_response_cache_{db['index_name']}.txt\", \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RAG to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type='ChromaDB'\n",
    "index_name=db['index_name']\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers_rag=rag_responses(index_type, index_name, query_model, llm, QA_model_params, df_questions_answers, df_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa_eval, df_questions_answers_rag = eval_rag(index_name, df_questions_answers_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=data_viz_prep(index_name,df_qa_eval,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'voyage-large-2-instruct-2merge-0',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)},\n",
    "     {'index_name':'voyage-large-2-instruct-0merge-400',\n",
    "     'query_model': VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                       voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first db, save for synthetic test dataset\n",
    "db=dbs[0]\n",
    "index = pinecone_client.Index(db['index_name'])\n",
    "ids=[]\n",
    "for id in index.list():\n",
    "    ids.extend(id)\n",
    "\n",
    "docs=[]\n",
    "chunk_size=200  # Tune to whatever doesn't error out, 200 won't for serverless\n",
    "for i in range(0, len(ids), chunk_size):\n",
    "    print(f\"Fetching {i} to {i+chunk_size}\")\n",
    "    vector=index.fetch(ids[i:i+chunk_size])['vectors']\n",
    "    vector_data = []\n",
    "    for key, value in vector.items():\n",
    "        vector_data.append(value)\n",
    "    docs.extend(vector_data)\n",
    "\n",
    "lcdocs_pinecone = []\n",
    "for data in docs:\n",
    "    data=data['metadata']\n",
    "    lcdocs_pinecone.append(Document(page_content=data['page_content'],\n",
    "                           metadata={'page':data['page'],'source':data['source']}))\n",
    "    \n",
    "print(len(lcdocs_pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all collections to pickles to store them\n",
    "for db in dbs:\n",
    "    df_temp_pinecone=archive_db('Pinecone',db['index_name'],db['query_model'],export_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
