{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta, get_docs_df\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "\n",
    "    if Path(file_name).exists():\n",
    "\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # return df.insert(0, column, None)\n",
    "\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        # df_out = df.copy().assign(column=\"\")\n",
    "        # df_out = df_out.reindex(columns=( [col for col in df_out.columns if col not in [column]] + [column] ))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-large-ams-nomerge-400),\n",
       " Collection(name=mixedbread-ai-mxbai-embed-large-v1-ams-nomerge-400),\n",
       " Collection(name=text-embedding-3-small-ams-2merge-none),\n",
       " Collection(name=text-embedding-3-large-ams-nomerge-400-parent-child),\n",
       " Collection(name=text-embedding-3-large-ams-2merge-none)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs=[{'index_name':'text-embedding-3-large-ams-2merge-none',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-ams-nomerge-400',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-large-ams-nomerge-400-parent-child',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'text-embedding-3-small-ams-2merge-none',\n",
    "     'query_model':OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=os.getenv('OPENAI_API_KEY'))},\n",
    "     {'index_name':'mixedbread-ai-mxbai-embed-large-v1-ams-nomerge-400',\n",
    "     'query_model':HuggingFaceInferenceAPIEmbeddings(model_name='mixedbread-ai-mxbai-embed-large-v1',api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'))}]\n",
    "\n",
    "db=dbs[0]\n",
    "# TODO: add Voyage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to docs_vectorstore where no chunking was done only full PDF pages\n",
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=db['index_name'],\n",
    "                        embedding_function=db['query_model'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lcdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Figure 6. Bearing Race-Ball Dent Fixture.  Results and Discussions The single ball versus race static load capacity of a hybrid Si 3N4-60NiTi bearing was evaluated and the  results are shown in Table 2. In the table, the dent depth, dent depth normalized by the ball diameter, the  peak stress, and the mean stress are listed. The normalized dent depth as a function of mean stress is plotted in Figure 7. The dent depth data for the ball-race contact agrees reasonably well with the more generalized ball-flat  plate dent depth data collected previously (Ref. 8). This agreement, despite differences in contact geometry, further support the argument that the Hertz stress relations for contacting bodies can be  confidently applied to bearing design. Using the generally accepted dent depth criteria (dp/D~0.00003 to  0.00010), the data indicate that the damage threshold for 60NiTi is around 2.8 to 3.3 GPa. This is slightly lower than the value estimated from flat plate dent studies but within data scatter and is to be considered in good agreement. Further experiments with additional race sizes are warranted to narrow the stress limit more precisely. For the present, a reasonable stress limit, when designing bearings to withstand high  static load, is 3.1 GPa. In the section that follows, a notional spacecraft mechanism, a small reaction wheel assembly (RWA), will  be assessed to determine how the substitution of 60NiTi for steel in the bearings can affect the ability to withstand the rigorous launch vibration load environment. Through such an assessment it is hoped that  pathways to performance improvement, weight reduction or enhanced robustness will be revealed. 79 Table 2.  Indentation Depth Data Summary [{Si 3N4Ball (8.74-mm dia.) loaded against hardened (HRC 60.5) polished 60NiTi inner race} Inner Race Curvature Radii: ball-path, 4.25 mm; cross-race, 1.27 mm.] Indent load, kgf(lb)Mean stress,  GPa (ksi)Peak stress, GPa (ksi)Dent depth,a /g80/g80m (/g80/g80in.)Dent depth/Ball  dia.,/g117/g11710â€“4  493 (1000) 2.05 (298) 3. 07 (446) None detected ----- 741 (1500) 2.40 (348) 3.5 2 (511) None detected ----- 988 (2000) 2.59 (375) 3.88 (562) 0.19 (7.5) 0.22 1111 (2250) 2.69 (390) 4.03 (585) 0.12 (4.8) 0.14 1235 (2500) 2.79 (404) 4.18 (606) 0.59 (23.3) 0.68 1358 (2750) 2.88 (417) 4.32 (626) 0.55 (21.8) 0.63 1482 (3000) 2.96 (429) 4.44 (644) 0.80 (31.4) 0.92 1605 (3250) 3.04 (441) 4.53 (661) 1.20 (47.4) 1.37 1729 (3500) 3.12 (452) 4.68 (678) 1.84 (72.6) 2.11 1852 (3750) 3.19 (463) 4.79 (694) 1.76 (69.3) 2.01 1976 (4000) 3.26 (473) 4.89 (709) 2.53 (99.8) 2.89 2100 (4250) 3.32 (482) 4.99 (723) 3.68 (145) 4.21 2223 (4500) 3.39 (492) 5.08 (737) 4.01 (158) 4.59 2346 (4750) 3.45 (500) 5.18 (751) 5.53 (218) 6.33 2470 (5000) 3.51 (509) 5.27 (764) 4.62 (182) 5.29 aTypical data scatter range /g1145 percent.  Figure 7.  Normalized Dent Depth Ve rsus Mean Hertz Contact Stress for  Inner Race of a 50-mm 60NiTi Bearing Loaded by a Si 3N4 Indenter Ball  (8.74-mm dia.). 80', metadata={'page': '[95, 96]', 'source': \"['AMS_2014.pdf', 'AMS_2014.pdf']\"})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcdocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export all collections to pickles to store them before doing anything heavy duty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in dbs:\n",
    "    df_temp=get_docs_df(os.getenv('LOCAL_DB_PATH'), db['index_name'], db['query_model'])\n",
    "    with open(os.path.join(os.getenv('LOCAL_DB_PATH'),\n",
    "                      f\"archive_chromadb_{db['index_name']}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(df_temp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "embedding_model=\"text-embedding-ada-002\"\n",
    "synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "# sample_size=min(len(lcdocs),20) # 500 is the max size before you'll hit rate limits with a tier 3 openai account\n",
    "sample_size=100\n",
    "percent_total=sample_size/len(lcdocs)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if testset.csv exists\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=50\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=False,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=False)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv(f'testset_{index_name}.csv', index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv(f'testset_{index_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, f'rag_response_cache_{index_name}.txt', \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f'rag_response_cache_{index_name}.txt', \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below demonstrates using aerospace_chatbot in batch mode. It requires some basic parameter setup and a QA_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO turn this into a function/standalone notebook to show how to use the QA model in batch mode\n",
    "\n",
    "index_type='ChromaDB'\n",
    "index_name=index_name\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n",
    "\n",
    "for i, row in df_questions_answers.iterrows():\n",
    "    if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "        print(f\"Processing question {i+1}/{len(df_questions_answers)}\")\n",
    "\n",
    "        # Use the QA model to query the documents\n",
    "        qa_obj=queries.QA_Model(index_type,\n",
    "                        index_name,\n",
    "                        query_model,\n",
    "                        embedding_model,\n",
    "                        llm,\n",
    "                        **QA_model_params)\n",
    "        qa_obj.query_docs(row['question'])\n",
    "        response=qa_obj.result\n",
    "\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response['answer'].content\n",
    "\n",
    "        ids=[_stable_hash_meta(source_document.metadata)\n",
    "            for source_document in response['references']]\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "        # Save the response to cache file\n",
    "        response_dict = {\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": response['answer'].content,\n",
    "            \"source_documents\": ids,\n",
    "        }\n",
    "        write_dict_to_file(response_dict, f'rag_response_cache_{index_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the context documents content for each question\n",
    "source_documents_list = []\n",
    "for cell in df_questions_answers['source_documents']:\n",
    "    cell_list = cell.strip('[]').split(', ')\n",
    "    context=[]\n",
    "    for cell in cell_list:\n",
    "        context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "    source_documents_list.append(context)\n",
    "df_questions_answers[\"contexts\"]=source_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtionaly get embeddings for questions\n",
    "\n",
    "if not Path(f'question_embeddings_{index_name}.pickle').exists():\n",
    "    question_embeddings = [\n",
    "        synthetic_embeddings.embed_query(question)\n",
    "        for question in df_questions_answers[\"question\"]\n",
    "    ]\n",
    "    with open(f'question_embeddings_{index_name}.pickle', \"wb\") as f:\n",
    "        pickle.dump(question_embeddings, f)\n",
    "\n",
    "question_embeddings = pickle.load(open(f'question_embeddings_{index_name}.pickle', \"rb\"))\n",
    "df_questions_answers[\"embedding\"] = question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, f'ragas_result_cache_{index_name}.txt', \"question\", \"answer_correctness\"\n",
    ")\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unclear why but sometimes ground_truth does not provide a response. Just filter those out.\n",
    "df_questions_answers = df_questions_answers[df_questions_answers['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for evaluation\n",
    "df_qa_eval = df_questions_answers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the answer correctness if not already done\n",
    "fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "for i, row in df_qa_eval.iterrows():\n",
    "    print(i, row[\"question\"])\n",
    "    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "        evaluation_result = evaluate(\n",
    "            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "            [answer_correctness],\n",
    "        )\n",
    "        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "            \"answer_correctness\"\n",
    "        ]\n",
    "\n",
    "        # optionally save the response to cache\n",
    "        response_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "        }\n",
    "        write_dict_to_file(response_dict, f'ragas_result_cache_{index_name}.txt')\n",
    "\n",
    "# write the answer correctness to the original dataframe\n",
    "df_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link from documents to questions, that used the document as source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section adds a column to df_documents containing the ids of the questions that used the document as source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "\n",
    "# Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "# Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "agg = (\n",
    "    df_questions_exploded.groupby(\"source_documents\")\n",
    "    .agg(\n",
    "        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "        question_ids=(\n",
    "            \"id\",\n",
    "            lambda x: list(x),\n",
    "        ),  # List of question IDs referencing the document\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"source_documents\": \"id\"})\n",
    ")\n",
    "\n",
    "# Merge the aggregated information back into df_documents\n",
    "df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "# Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "# Replace NaN values in 'num_questions' with 0\n",
    "df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_qa_eval, df_documents_agg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UMAP only using documents and apply it to the documents and questions\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "df_questions = df[~df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_questions = umap.transform(df[\"embedding\"].values.tolist())\n",
    "\n",
    "\n",
    "df_without_questions = df[df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_without_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_docs = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_all = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "# find the nearet question (by embedding) for each document\n",
    "question_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\n",
    "\n",
    "df[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "    for doc_emb in df[\"embedding\"].values\n",
    "]\n",
    "\n",
    "# write the dataframe to parquet for later use\n",
    "df.to_parquet(f'df_{index_name}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the first cell to use the downloaded dataframes if you skipped the preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "# OR Load the data from downloaded file https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/df_f1_rag_docs_and_questions_umaps_v3.parquet\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe with the question and answer in spotlight\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
