{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from autotrain.dataset import AutoTrainDataset\n",
    "from autotrain.project import AutoTrainProject\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ai-aerospace/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic_100'\n",
    "dataset=load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: What is the title of the symposium hosted by NASA Langley Research Center and Lockheed Martin Space Systems Company, as mentioned in the context?### Assistant:NASNCP-2006-2 14290  3 sth Aerospace Mechanisms Symposium  Compiled by  Edward A. Boesiger  Lockheed Martin Space Systems Company, Sunnyvale, California  Proceedings of a symposium hosted by  the NASA Langley Research Center and  Lockheed Martin Space Systems Company and  organized by the Mechanisms Education Association  held at the Williamsburg Maniott Hotel  Williamsburg, Virginia  May 17- 19,2006  May 2006 {'source': 'AMS_2006.pdf', 'page': 1}\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: What is the aerospace mechanisms symposia?### Assistant: An annual meeting of space mechanism experts. {'source': 'DM', 'page': 0}\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotrain\n",
    "https://github.com/huggingface/autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "username='ai-aerospace'\n",
    "project_name='./llms/'+'ams_data_train-100_'+str(uuid4())\n",
    "repo_name='ams_data_train-100_'+str(uuid4())\n",
    "\n",
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v0.1'\n",
    "model_name='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cli (more well documented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken this dataset and renamed into train.csv in the training folder. I can't figure out how to use autotrain with a different filename when using cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"project_name\"] = project_name\n",
    "os.environ[\"model_name\"] = model_name\n",
    "os.environ[\"repo_id\"] = username+'/'+repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../.venv/bin/activate\n",
    "# !autotrain llm --train --project_name my-llm --model TinyLlama/TinyLlama-1.1B-Chat-v0.1 --data_path . --use-peft --use_int4 --learning_rate 2e-4 --train_batch_size 6 --num_train_epochs 3 --trainer sft\n",
    "\n",
    "# The training dataset to be used must be called training.csv and be located in the data_path folder.\n",
    "!autotrain llm --train \\\n",
    "    --project_name ${project_name} \\\n",
    "    --model ${model_name} \\\n",
    "    --data_path . \\\n",
    "    --use-peft \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --train_batch_size 6 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --trainer sft \\\n",
    "    --push_to_hub \\\n",
    "    --repo_id ${repo_id} \\\n",
    "    --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "dataset_temp = dataset.copy()\n",
    "train = dataset_temp['train']\n",
    "\n",
    "# FILEPATH: /Users/danmueller/Documents/GitHub/aerospace_chatbot/training/train_llm_autotrain.ipynb\n",
    "train_df = train.to_pandas()\n",
    "\n",
    "# Validation. If empty, just creates an empty dataset. Needed to run autotrain.\n",
    "try:\n",
    "    validation = dataset_temp['validation']\n",
    "    validation_df = validation.to_pandas()\n",
    "except:\n",
    "    validation_df = pd.DataFrame()\n",
    "    validation_df['text'] = ''\n",
    "    # Add validation item to the dataset\n",
    "    dataset_temp['validation'] = validation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Dataset: ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52 (lm_training)\n",
      "Train data: [                                                  text\n",
      "0    ### Human: What is the title of the symposium ...\n",
      "1    ### Human: What is the purpose of the NASA Sci...\n",
      "2    ### Human: What is the name of the symposium h...\n",
      "3    ### Human: What is the contact information for...\n",
      "4    ### Human: What is the name of the organizatio...\n",
      "..                                                 ...\n",
      "96   ### Human: What is the cause of the failure me...\n",
      "97   ### Human: What was identified as a root cause...\n",
      "98   ### Human: What was the primary challenge of a...\n",
      "99   ### Human: What is the cause of the failure en...\n",
      "100  ### Human: What is the aerospace mechanisms sy...\n",
      "\n",
      "[101 rows x 1 columns]]\n",
      "Valid data: [                                                text\n",
      "0  ### Human: What is the aerospace mechanisms sy...]\n",
      "Column mapping: {'text': 'text'}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                text\n",
      "0  ### Human: What is the aerospace mechanisms sy...]\n"
     ]
    }
   ],
   "source": [
    "# # prepare dataset for AutoTrain\n",
    "# dset = AutoTrainDataset(\n",
    "#     train_data=[train_df],\n",
    "#     valid_data=[validation_df],\n",
    "#     task=TASK,\n",
    "#     token=HUGGINGFACE_TOKEN,\n",
    "#     project_name=project_name,\n",
    "#     username=USERNAME,\n",
    "#     column_mapping={\"text\": \"text\", \"label\": \"label\"},\n",
    "#     percent_valid=None,\n",
    "# )\n",
    "\n",
    "# prepare dataset for AutoTrain\n",
    "task = \"lm_training\"\n",
    "dset = AutoTrainDataset(\n",
    "    train_data=[train_df],\n",
    "    valid_data=[validation_df],\n",
    "    task=task,\n",
    "    username=username,\n",
    "    project_name=repo_name,\n",
    "    token=os.environ['HUGGINGFACE_TOKEN'],\n",
    "    percent_valid=None,\n",
    "    column_mapping={\"text\": \"text\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd25f571cf9a4c7298f4bf82daa2ff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540cdc2aed394d4d952c1fe9b4305466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5ba2875a4d4ae193da33c1fd3ff0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df8c03ed8dd462fa5ee01ac2bf6f6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef0efef95304c1f972deaae767f8824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_accumulation_steps': <class 'autotrain.params.GradientAccumulationSteps'>,\n",
      " 'hub_model': <class 'autotrain.params.HubModel'>,\n",
      " 'learning_rate': <class 'autotrain.params.LMLearningRate'>,\n",
      " 'lora_alpha': <class 'autotrain.params.LoraAlpha'>,\n",
      " 'lora_dropout': <class 'autotrain.params.LoraDropout'>,\n",
      " 'lora_r': <class 'autotrain.params.LoraR'>,\n",
      " 'num_train_epochs': <class 'autotrain.params.LMEpochs'>,\n",
      " 'optimizer': <class 'autotrain.params.Optimizer'>,\n",
      " 'percentage_warmup': <class 'autotrain.params.PercentageWarmup'>,\n",
      " 'scheduler': <class 'autotrain.params.Scheduler'>,\n",
      " 'train_batch_size': <class 'autotrain.params.LMTrainBatchSize'>,\n",
      " 'training_type': <class 'autotrain.params.LMTrainingType'>,\n",
      " 'weight_decay': <class 'autotrain.params.WeightDecay'>}\n"
     ]
    }
   ],
   "source": [
    "from autotrain.params import Params\n",
    "import pprint\n",
    "params = Params(task=task, param_choice='manual',model_choice='hub_model').get()\n",
    "pprint.pprint(params) # to get full list of params for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# How to get params for a task:\n",
    "#\n",
    "# from autotrain.params import Params\n",
    "# params = Params(task=TASK, training_type=\"hub_model\").get()\n",
    "# print(params) to get full list of params for the task\n",
    "\n",
    "# define params in proper format\n",
    "job1 = {\n",
    "    \"hub_model\": model_name,\n",
    "    \"model_choice\":\"\",\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"scheduler\": \"linear\",\n",
    "    \"epochs\": 5,\n",
    "    \"backend\": 'CPU (Free)'\n",
    "}\n",
    "\n",
    "job2 = {\n",
    "    \"hub_model\": model_name,\n",
    "    \"model_choice\":\"\",\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"epochs\": 5,\n",
    "    \"backend\": 'CPU (Free)'\n",
    "}\n",
    "\n",
    "job3 = {\n",
    "    \"hub_model\": model_name,\n",
    "    \"model_choice\":\"\",\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"epochs\": 5,\n",
    "    \"backend\": 'CPU (Free)'\n",
    "}\n",
    "\n",
    "jobs = pd.DataFrame([job1, job2, job3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    [{\"hub_model\":\"mistralai\\/Mistral-7B-v0.1\",\"model_choice\":\"\",\"task\":\"lm_training\",\"learning_rate\":0.00001,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"epochs\":5,\"backend\":\"CPU (Free)\"},{\"hub_model\":\"mistralai\\/Mistral-7B-v0.1\",\"model_choice\":\"\",\"task\":\"lm_training\",\"learning_rate\":0.00003,\"optimizer\":\"adamw_torch\",\"scheduler\":\"cosine\",\"epochs\":5,\"backend\":\"CPU (Free)\"},{\"hub_model\":\"mistralai\\/Mistral-7B-v0.1\",\"model_choice\":\"\",\"task\":\"lm_training\",\"learning_rate\":0.00005,\"optimizer\":\"sgd\",\"scheduler\":\"cosine\",\"epochs\":5,\"backend\":\"CPU (Free)\"}]\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters not supplied by user and set to default: use_int8, model_ref, train_split, trainer, add_eos_token, target_modules, lora_alpha, merge_adapter, use_peft, warmup_ratio, valid_split, seed, disable_gradient_checkpointing, evaluation_strategy, lora_r, auto_find_batch_size, use_flash_attention_2, dpo_beta, weight_decay, log, block_size, save_total_limit, gradient_accumulation, lora_dropout, batch_size, lr, fp16, save_strategy, model_max_length, use_int4, logging_steps, max_grad_norm\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters supplied but not used: hub_model, learning_rate, task, model_choice, backend\u001b[0m\n",
      "> \u001b[1mINFO    Creating Space for job: 0\u001b[0m\n",
      "> \u001b[1mINFO    Using params: {'model': '', 'data_path': 'ai-aerospace/autotrain-data-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52', 'project_name': 'ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-0', 'train_split': 'train', 'valid_split': None, 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'token': '*****', 'lr': 3e-05, 'epochs': 5, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'add_eos_token': True, 'block_size': -1, 'use_peft': False, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'save_strategy': 'epoch', 'auto_find_batch_size': False, 'fp16': False, 'push_to_hub': True, 'use_int8': False, 'model_max_length': 2048, 'repo_id': 'ai-aerospace/ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-0', 'use_int4': False, 'trainer': 'default', 'target_modules': None, 'merge_adapter': False, 'username': 'ai-aerospace', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'model_ref': None, 'dpo_beta': 0.1, 'prompt_text_column': 'autotrain_prompt'}\u001b[0m\n",
      "> \u001b[1mINFO    Space created with id: ai-aerospace/autotrain-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-0\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters not supplied by user and set to default: use_int8, model_ref, train_split, trainer, add_eos_token, target_modules, lora_alpha, merge_adapter, use_peft, warmup_ratio, valid_split, seed, disable_gradient_checkpointing, evaluation_strategy, lora_r, auto_find_batch_size, use_flash_attention_2, dpo_beta, weight_decay, log, block_size, save_total_limit, gradient_accumulation, lora_dropout, batch_size, lr, fp16, save_strategy, model_max_length, use_int4, logging_steps, max_grad_norm\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters supplied but not used: hub_model, learning_rate, task, model_choice, backend\u001b[0m\n",
      "> \u001b[1mINFO    Creating Space for job: 1\u001b[0m\n",
      "> \u001b[1mINFO    Using params: {'model': '', 'data_path': 'ai-aerospace/autotrain-data-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52', 'project_name': 'ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-1', 'train_split': 'train', 'valid_split': None, 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'token': '*****', 'lr': 3e-05, 'epochs': 5, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'cosine', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'add_eos_token': True, 'block_size': -1, 'use_peft': False, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'save_strategy': 'epoch', 'auto_find_batch_size': False, 'fp16': False, 'push_to_hub': True, 'use_int8': False, 'model_max_length': 2048, 'repo_id': 'ai-aerospace/ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-1', 'use_int4': False, 'trainer': 'default', 'target_modules': None, 'merge_adapter': False, 'username': 'ai-aerospace', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'model_ref': None, 'dpo_beta': 0.1, 'prompt_text_column': 'autotrain_prompt'}\u001b[0m\n",
      "> \u001b[1mINFO    Space created with id: ai-aerospace/autotrain-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-1\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters not supplied by user and set to default: use_int8, model_ref, train_split, trainer, add_eos_token, target_modules, lora_alpha, merge_adapter, use_peft, warmup_ratio, valid_split, seed, disable_gradient_checkpointing, evaluation_strategy, lora_r, auto_find_batch_size, use_flash_attention_2, dpo_beta, weight_decay, log, block_size, save_total_limit, gradient_accumulation, lora_dropout, batch_size, lr, fp16, save_strategy, model_max_length, use_int4, logging_steps, max_grad_norm\u001b[0m\n",
      "> \u001b[33m\u001b[1mWARNING Parameters supplied but not used: hub_model, learning_rate, task, model_choice, backend\u001b[0m\n",
      "> \u001b[1mINFO    Creating Space for job: 2\u001b[0m\n",
      "> \u001b[1mINFO    Using params: {'model': '', 'data_path': 'ai-aerospace/autotrain-data-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52', 'project_name': 'ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-2', 'train_split': 'train', 'valid_split': None, 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'token': '*****', 'lr': 3e-05, 'epochs': 5, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'sgd', 'scheduler': 'cosine', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'add_eos_token': True, 'block_size': -1, 'use_peft': False, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'save_strategy': 'epoch', 'auto_find_batch_size': False, 'fp16': False, 'push_to_hub': True, 'use_int8': False, 'model_max_length': 2048, 'repo_id': 'ai-aerospace/ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-2', 'use_int4': False, 'trainer': 'default', 'target_modules': None, 'merge_adapter': False, 'username': 'ai-aerospace', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'model_ref': None, 'dpo_beta': 0.1, 'prompt_text_column': 'autotrain_prompt'}\u001b[0m\n",
      "> \u001b[1mINFO    Space created with id: ai-aerospace/autotrain-ams_data_train-100_db50c43f-2753-4dfa-9c0b-dc151dc83a52-2\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AutoTrainProject' object has no attribute 'approve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m project \u001b[38;5;241m=\u001b[39m AutoTrainProject(dataset\u001b[38;5;241m=\u001b[39mdset, job_params\u001b[38;5;241m=\u001b[39mjobs)\n\u001b[1;32m      2\u001b[0m project_id \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapprove\u001b[49m(project_id)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoTrainProject' object has no attribute 'approve'"
     ]
    }
   ],
   "source": [
    "project = AutoTrainProject(dataset=dset, job_params=jobs)\n",
    "project_id = project.create()\n",
    "project.approve(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
