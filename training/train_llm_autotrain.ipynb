{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from autotrain.dataset import AutoTrainDataset\n",
    "from autotrain.project import AutoTrainProject\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ai-aerospace/ams_data_train_Llama-2-7B-Chat-GGUF-LLM-generic_100'\n",
    "dataset=load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: What is the title of the symposium hosted by NASA Langley Research Center and Lockheed Martin Space Systems Company, as mentioned in the context?### Assistant:NASNCP-2006-2 14290  3 sth Aerospace Mechanisms Symposium  Compiled by  Edward A. Boesiger  Lockheed Martin Space Systems Company, Sunnyvale, California  Proceedings of a symposium hosted by  the NASA Langley Research Center and  Lockheed Martin Space Systems Company and  organized by the Mechanisms Education Association  held at the Williamsburg Maniott Hotel  Williamsburg, Virginia  May 17- 19,2006  May 2006 {'source': 'AMS_2006.pdf', 'page': 1}\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotrain\n",
    "https://github.com/huggingface/autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "username='ai-aerospace'\n",
    "project_name='./llms/'+'ams_data_train-100_'+str(uuid4())\n",
    "repo_name='ams_data_train-100_'+str(uuid4())\n",
    "\n",
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v0.1'\n",
    "model_name='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cli (more well documented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken this dataset and renamed into train.csv in the training folder. I can't figure out how to use autotrain with a different filename when using cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"project_name\"] = project_name\n",
    "os.environ[\"model_name\"] = model_name\n",
    "os.environ[\"repo_id\"] = username+'/'+repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../.venv/bin/activate\n",
    "# !autotrain llm --train --project_name my-llm --model TinyLlama/TinyLlama-1.1B-Chat-v0.1 --data_path . --use-peft --use_int4 --learning_rate 2e-4 --train_batch_size 6 --num_train_epochs 3 --trainer sft\n",
    "\n",
    "# The training dataset to be used must be called training.csv and be located in the data_path folder.\n",
    "!autotrain llm --train \\\n",
    "    --project_name ${project_name} \\\n",
    "    --model ${model_name} \\\n",
    "    --data_path . \\\n",
    "    --use-peft \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --train_batch_size 6 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --trainer sft \\\n",
    "    --push_to_hub \\\n",
    "    --repo_id ${repo_id} \\\n",
    "    --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train = dataset['train']\n",
    "train_df = train.to_pandas()\n",
    "\n",
    "# Validation. If empty, just creates an empty dataset. Needed to run autotrain.\n",
    "try:\n",
    "    validation = dataset[\"test\"]\n",
    "    validation_df = validation.to_pandas()\n",
    "except:\n",
    "    validation_df = pd.DataFrame()\n",
    "    validation_df['text'] = ''\n",
    "    # Add validation item to the dataset\n",
    "    dataset['validation']=validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Empty DataFrame\n",
       "    Columns: [text]\n",
       "    Index: []\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Dataset: ams_data_train-100_4a46db37-8c3e-48dc-8ca3-c3a7df6a2960 (lm_training)\n",
      "Train data: [                                                 text\n",
      "0   ### Human: What is the title of the symposium ...\n",
      "1   ### Human: What is the purpose of the NASA Sci...\n",
      "2   ### Human: What is the name of the symposium h...\n",
      "3   ### Human: What is the contact information for...\n",
      "4   ### Human: What is the name of the organizatio...\n",
      "..                                                ...\n",
      "95  ### Human: What is the likely cause of the vib...\n",
      "96  ### Human: What is the cause of the failure me...\n",
      "97  ### Human: What was identified as a root cause...\n",
      "98  ### Human: What was the primary challenge of a...\n",
      "99  ### Human: What is the cause of the failure en...\n",
      "\n",
      "[100 rows x 1 columns]]\n",
      "Valid data: [Empty DataFrame\n",
      "Columns: [text]\n",
      "Index: []]\n",
      "Column mapping: {'text': 'text'}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Empty DataFrame\n",
      "Columns: [text]\n",
      "Index: []]\n"
     ]
    }
   ],
   "source": [
    "# # prepare dataset for AutoTrain\n",
    "# dset = AutoTrainDataset(\n",
    "#     train_data=[train_df],\n",
    "#     valid_data=[validation_df],\n",
    "#     task=TASK,\n",
    "#     token=HUGGINGFACE_TOKEN,\n",
    "#     project_name=project_name,\n",
    "#     username=USERNAME,\n",
    "#     column_mapping={\"text\": \"text\", \"label\": \"label\"},\n",
    "#     percent_valid=None,\n",
    "# )\n",
    "\n",
    "# prepare dataset for AutoTrain\n",
    "task = \"lm_training\"\n",
    "dset = AutoTrainDataset(\n",
    "    train_data=[train_df],\n",
    "    valid_data=[validation_df],\n",
    "    task=task,\n",
    "    username=username,\n",
    "    project_name=repo_name,\n",
    "    token=os.environ['HUGGINGFACE_TOKEN'],\n",
    "    percent_valid=None,\n",
    "    column_mapping={\"text\": \"text\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2af62275e8471d9fe1c033bbfa5b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1fdf39c73f453aae29aa5044f2c6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3a3b2761e46b9b54986102f489f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148b54870a444cb6b610e23360693e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778a67467c844acfb7ba96371bb56559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Features of the new split don't match the features of the existing splits on the hub: {'autotrain_text': Value(dtype='null', id=None)} != {'autotrain_text': Value(dtype='string', id=None)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/autotrain/dataset.py:315\u001b[0m, in \u001b[0;36mAutoTrainDataset.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m     rejected_text_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_mapping\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrejected_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    303\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m LLMPreprocessor(\n\u001b[1;32m    304\u001b[0m         train_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df,\n\u001b[1;32m    305\u001b[0m         text_column\u001b[38;5;241m=\u001b[39mtext_column,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabular_binary_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    318\u001b[0m     id_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/autotrain/preprocessor/text.py:205\u001b[0m, in \u001b[0;36mLLMPreprocessor.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m valid_df \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(valid_df)\n\u001b[1;32m    199\u001b[0m train_df\u001b[38;5;241m.\u001b[39mpush_to_hub(\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/autotrain-data-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    202\u001b[0m     private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    204\u001b[0m )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mvalid_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musername\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/autotrain-data-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_df, valid_df\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:5486\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[0;34m(self, repo_id, config_name, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_info\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(repo_info\u001b[38;5;241m.\u001b[39msplits) \u001b[38;5;241m!=\u001b[39m [split]:\n\u001b[1;32m   5485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m!=\u001b[39m repo_info\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m-> 5486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5487\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures of the new split don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the features of the existing splits on the hub: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_info\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5488\u001b[0m         )\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m repo_info\u001b[38;5;241m.\u001b[39msplits:\n\u001b[1;32m   5491\u001b[0m         repo_info\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m deleted_size\n",
      "\u001b[0;31mValueError\u001b[0m: Features of the new split don't match the features of the existing splits on the hub: {'autotrain_text': Value(dtype='null', id=None)} != {'autotrain_text': Value(dtype='string', id=None)}"
     ]
    }
   ],
   "source": [
    "dset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# How to get params for a task:\n",
    "#\n",
    "# from autotrain.params import Params\n",
    "# params = Params(task=TASK, training_type=\"hub_model\").get()\n",
    "# print(params) to get full list of params for the task\n",
    "\n",
    "# define params in proper format\n",
    "job1 = {\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"scheduler\": \"linear\",\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "\n",
    "job2 = {\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "\n",
    "job3 = {\n",
    "    \"task\": task,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "\n",
    "jobs = [job1, job2, job3]\n",
    "project = AutoTrainProject(dataset=dset, hub_model=model_name, job_params=jobs)\n",
    "project_id = project.create()\n",
    "project.approve(project_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
