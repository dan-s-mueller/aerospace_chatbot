{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "from data_processing import _stable_hash_meta\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nifty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "\n",
    "    if Path(file_name).exists():\n",
    "\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        # return df.insert(0, column, None)\n",
    "\n",
    "        # Create a copy of the DataFrame\n",
    "        df_out = df.copy()\n",
    "\n",
    "        # Add the new column with the name of the variable 'column'\n",
    "        df_out[column] = None\n",
    "\n",
    "        # Reorder the columns to place the new column at the end\n",
    "        columns = list(df_out.columns)\n",
    "        columns.remove(column)\n",
    "        columns.append(column)\n",
    "        df_out = df_out[columns]\n",
    "        \n",
    "        # df_out = df.copy().assign(column=\"\")\n",
    "        # df_out = df_out.reindex(columns=( [col for col in df_out.columns if col not in [column]] + [column] ))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "query_model=OpenAIEmbeddings(model='text-embedding-ada-002',openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name='chromadb-text-embedding-ada-002-ams-nomerge-2000-2020-400-0'\n",
    "\n",
    "# Connect to docs_vectorstore where no chunking was done only full PDF pages\n",
    "docs_vectorstore = Chroma(client=persistent_client,\n",
    "                        collection_name=index_name,\n",
    "                        embedding_function=query_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs = [Document(page_content=doc, metadata=metadata) \n",
    "          for doc, metadata in zip(all_docs['documents'], all_docs['metadatas'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcdocs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openai\n",
    "generator_model=\"gpt-3.5-turbo-16k\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "\n",
    "# Hugging face LLM\n",
    "# generator_model=\"google/gemma-7b-it\"\n",
    "# synthetic_generator_llm = ChatOpenAI(base_url='https://api-inference.huggingface.co/v1',\n",
    "#                             model=generator_model,\n",
    "#                             api_key=os.getenv('HUGGINGFACEHUB_API_TOKEN'),\n",
    "#                             tags=generator_model)\n",
    "\n",
    "# Local LLM via LM studio\n",
    "# synthetic_generator_llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model=\"gpt-3.5-turbo-0125\"\n",
    "# critic_model=\"gpt-4\"\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "embedding_model=\"text-embedding-ada-002\"\n",
    "synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings\n",
    ")\n",
    "\n",
    "sample_size=min(len(lcdocs),20) # 500 is the max size before you'll hit rate limits with a tier 3 openai account\n",
    "percent_total=sample_size/len(lcdocs)\n",
    "print(percent_total)\n",
    "\n",
    "# Get a random sample of lcdocs\n",
    "lcdocs_random = random.sample(lcdocs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if testset.csv exists\n",
    "if not os.path.exists('./testset.csv'):\n",
    "    run_config=RunConfig(timeout=1000,\n",
    "                    max_retries=50,\n",
    "                    max_wait=1000,\n",
    "                    max_workers=1)\n",
    "\n",
    "    n_questions=10\n",
    "    testset = generator.generate_with_langchain_docs(lcdocs_random, \n",
    "                                                    test_size=n_questions,\n",
    "                                                    with_debugging_logs=True,\n",
    "                                                    is_async=False,\n",
    "                                                    run_config=run_config,\n",
    "                                                    raise_exceptions=False)\n",
    "    df_testset=testset.to_pandas()\n",
    "    df_testset.to_csv('testset.csv', index=False)\n",
    "else:\n",
    "    # Import testset.csv into a DataFrame\n",
    "    df_testset = pd.read_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_questions['id'] = f\"Question {i}\" for i, _ in enumerate(questions_all),\n",
    "\n",
    "df_questions = df_testset[['question', 'ground_truth']].copy()\n",
    "df_questions['id'] = 'Question ' + df_questions.index.astype(str)\n",
    "df_questions['question_by'] = generator_model\n",
    "df_questions = df_questions[['id', 'question', 'ground_truth', 'question_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [_stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, \"rag_response_cache.txt\", \"question\", \"answer\")\n",
    "\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, \"rag_response_cache.txt\", \"question\", \"source_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG questions/answers (batch mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below demonstrates using aerospace_chatbot in batch mode. It requires some basic parameter setup and a QA_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO turn this into a function/standalone notebook to show how to use the QA model in batch mode\n",
    "\n",
    "index_type='ChromaDB'\n",
    "index_name=index_name\n",
    "query_model=synthetic_embeddings\n",
    "llm=synthetic_generator_llm\n",
    "\n",
    "QA_model_params={'rag_type':'Standard',\n",
    "                 'k':4,\n",
    "                 'search_type':'similarity',\n",
    "                 'local_db_path':os.getenv('LOCAL_DB_PATH')}\n",
    "\n",
    "for i, row in df_questions_answers.iterrows():\n",
    "    if row['answer'] is None or pd.isnull(row['answer']) or row['answer']=='':\n",
    "        print(f\"Processing question {i+1}/{len(df_questions_answers)}\")\n",
    "\n",
    "        # Use the QA model to query the documents\n",
    "        qa_obj=queries.QA_Model(index_type,\n",
    "                        index_name,\n",
    "                        query_model,\n",
    "                        embedding_model,\n",
    "                        llm,\n",
    "                        **QA_model_params)\n",
    "        qa_obj.query_docs(row['question'])\n",
    "        response=qa_obj.result\n",
    "\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response['answer'].content\n",
    "        # print(response['references'])\n",
    "\n",
    "        ids=[_stable_hash_meta(source_document.metadata)\n",
    "            for source_document in response['references']]\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = ', '.join(ids)\n",
    "\n",
    "        # Save the response to cache file\n",
    "        response_dict = {\n",
    "            \"question\": row['question'],\n",
    "            \"answer\": response['answer'].content,\n",
    "            \"source_documents\": ids,\n",
    "        }\n",
    "        write_dict_to_file(response_dict, \"rag_response_cache.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the context documents content for each question\n",
    "source_documents_list = []\n",
    "for cell in df_questions_answers['source_documents']:\n",
    "    cell_list = cell.strip('[]').split(', ')\n",
    "    context=[]\n",
    "    for cell in cell_list:\n",
    "        context.append(df_docs[df_docs[\"id\"] == cell][\"document\"].values[0])\n",
    "    source_documents_list.append(context)\n",
    "df_questions_answers[\"contexts\"]=source_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addtionaly get embeddings for questions\n",
    "\n",
    "if not Path(\"question_embeddings.pickle\").exists():\n",
    "    question_embeddings = [\n",
    "        synthetic_embeddings.embed_query(question)\n",
    "        for question in df_questions_answers[\"question\"]\n",
    "    ]\n",
    "    with open(\"question_embeddings.pickle\", \"wb\") as f:\n",
    "        pickle.dump(question_embeddings, f)\n",
    "\n",
    "question_embeddings = pickle.load(open(\"question_embeddings.pickle\", \"rb\"))\n",
    "# answer_embeddings = pickle.load(open(\"answer_embeddings_2040214_1111.pickle\", \"rb\"))\n",
    "df_questions_answers[\"embedding\"] = question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, \"ragas_result_cache.txt\", \"question\", \"answer_correctness\"\n",
    ")\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unclear why but sometimes ground_truth does not provide a response. Just filter those out.\n",
    "df_questions_answers = df_questions_answers[df_questions_answers['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for evaluation\n",
    "df_qa_eval = df_questions_answers.copy()\n",
    "\n",
    "# adapt the ground truth to the ragas name and format\n",
    "# df_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\n",
    "# df_qa_eval[\"ground_truths\"] = [\n",
    "#     [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truth\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the answer correctness if not already done\n",
    "fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "for i, row in df_qa_eval.iterrows():\n",
    "    print(i, row[\"question\"])\n",
    "    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "        evaluation_result = evaluate(\n",
    "            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "            [answer_correctness],\n",
    "        )\n",
    "        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "            \"answer_correctness\"\n",
    "        ]\n",
    "\n",
    "        # optionally save the response to cache\n",
    "        response_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "        }\n",
    "        write_dict_to_file(response_dict, \"ragas_result_cache.txt\")\n",
    "\n",
    "# write the answer correctness to the original dataframe\n",
    "df_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "\n",
    "# Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "# Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "agg = (\n",
    "    df_questions_exploded.groupby(\"source_documents\")\n",
    "    .agg(\n",
    "        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "        question_ids=(\n",
    "            \"id\",\n",
    "            lambda x: list(x),\n",
    "        ),  # List of question IDs referencing the document\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"source_documents\": \"id\"})\n",
    ")\n",
    "\n",
    "# Merge the aggregated information back into df_documents\n",
    "df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "# Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "# Replace NaN values in 'num_questions' with 0\n",
    "df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
