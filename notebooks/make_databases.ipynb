{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import admin\n",
    "import data_processing\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to split chunking from upserting\n",
    "def create_upsert(index_type,index_name,query_model,rag_type,chunker,summary_llm):\n",
    "    # Set index names for special databases\n",
    "    if rag_type == 'Parent-Child':\n",
    "        index_name = index_name + '-parent-child'\n",
    "    if rag_type == 'Summary':\n",
    "        index_name = index_name + '-' + summary_llm.model_name.replace('/', '-').replace(' ','-').lower() + '-summary' \n",
    "\n",
    "    try:\n",
    "        vectorstore = data_processing.initialize_database(index_type, \n",
    "                                            index_name, \n",
    "                                            query_model,\n",
    "                                            rag_type=rag_type,\n",
    "                                            clear=True, \n",
    "                                            local_db_path=os.getenv('LOCAL_DB_PATH'),\n",
    "                                            init_ragatouille=True,\n",
    "                                            show_progress=False)\n",
    "        print(f\"Database {index_name} created.\")\n",
    "        vectorstore, _ = data_processing.upsert_docs(index_type, \n",
    "                                        index_name,\n",
    "                                        vectorstore,\n",
    "                                        chunker,\n",
    "                                        batch_size=400,\n",
    "                                        show_progress=False,\n",
    "                                        local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database {index_name} upserted chunks.\")\n",
    "    except Exception as e:  # If there is an error, be sure to delete the database\n",
    "        data_processing.delete_index(index_type, \n",
    "                                    index_name,\n",
    "                                    rag_type,\n",
    "                                    local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database deleted: {index_name}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets, Models, Docs, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, assign models\n",
    "json_file_path = \"databases.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "sb={}\n",
    "query_params=setup_data['query_models']\n",
    "query_models=[]\n",
    "for model in query_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    query_models.append(admin.get_query_model(sb, secrets))\n",
    "\n",
    "llm_params=setup_data['llms']\n",
    "llms=[]\n",
    "for model in llm_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    llms.append(admin.set_llm(sb, secrets))\n",
    "\n",
    "chunk_params=setup_data['chunk_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra query types that take a long time. Add to the databases.json file\n",
    "\n",
    "{\n",
    "    \"id\": \"3\",\n",
    "    \"index_type\": \"ChromaDB\",\n",
    "    \"query_model\": \"Hugging Face\",\n",
    "    \"embedding_name\": \"Dedicated Endpoint\",\n",
    "    \"embedding_hf_endpoint\": \"https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud\"\n",
    "},\n",
    "{\n",
    "    \"id\": \"4\",\n",
    "    \"index_type\": \"RAGatouille\",\n",
    "    \"embedding_name\": \"colbert-ir/colbertv2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs\n",
    "data_folder='../data/AMS'\n",
    "docs= glob.glob(os.path.join(data_folder,'*.pdf'))   # Only get the PDFs in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All query models, standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2222 chunks from 2222 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-2merge-0 created.\n",
      "Database text-embedding-3-large-2merge-0 upserted chunks.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2'}\n",
      "Database voyage-large-2-2merge-0 created.\n",
      "Database voyage-large-2-2merge-0 upserted chunks.\n",
      "Created 33478 chunks from 4439 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-0merge-400 created.\n",
      "Database text-embedding-3-large-0merge-400 upserted chunks.\n",
      "Creating and uploading database with these params: {'id': '2', 'index_type': 'Pinecone', 'query_model': 'Voyage', 'embedding_name': 'voyage-large-2'}\n",
      "Database voyage-large-2-0merge-400 created.\n",
      "Database voyage-large-2-0merge-400 upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "for i_chunk in range(len(chunk_params)):\n",
    "    # Chunk the docs before creating and upserting into the database\n",
    "    chunker=data_processing.chunk_docs(docs,\n",
    "                rag_type=rag_type,\n",
    "                n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "                chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "                chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "                llm=summary_llm,\n",
    "                show_progress=False)\n",
    "\n",
    "    print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "    for i_run in range(len(query_params)):\n",
    "        # Create and upsert database\n",
    "        print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "        index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "        index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "        create_upsert(query_params[i_run]['index_type'],\n",
    "                        index_name,\n",
    "                        query_models[i_run],\n",
    "                        rag_type,\n",
    "                        chunker,\n",
    "                        summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, parent-child, 400 character-recursive chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Parent-Child'\n",
    "summary_llm=None\n",
    "i_chunk=1   # 400 character-recursive setting\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 36148 chunks from 2 pages.\n",
      "Creating and uploading database with these params: {'id': '1', 'index_type': 'ChromaDB', 'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}\n",
      "Database text-embedding-3-large-0merge-400-parent-child created.\n",
      "Database text-embedding-3-large-0merge-400-parent-child upserted chunks.\n"
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Summary'\n",
    "summary_llm=llms[4] # mistralai/Mistral-7B-Instruct-v0.2 serverless\n",
    "\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chunker\u001b[38;5;241m=\u001b[39m\u001b[43mdata_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrag_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrag_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_merge_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_chunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_merge_pages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_chunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_chunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m summaries from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create and upsert database\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/notebooks/../src/aerospace_chatbot/data_processing.py:269\u001b[0m, in \u001b[0;36mchunk_docs\u001b[0;34m(docs, rag_type, chunk_method, file_out, n_merge_pages, chunk_size, chunk_overlap, k_parent, llm, show_progress)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pages), batch_size_submit):\n\u001b[1;32m    268\u001b[0m     batch_pages \u001b[38;5;241m=\u001b[39m pages[i:i\u001b[38;5;241m+\u001b[39mbatch_size_submit]  \u001b[38;5;66;03m# Get the batch of pages\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_pages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_concurrency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_submit\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(summary)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2643\u001b[0m, in \u001b[0;36mRunnableSequence.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2642\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2643\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# each step a child run of the corresponding root run\u001b[39;49;00m\n\u001b[1;32m   2647\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;66;03m# finish the root runs\u001b[39;00m\n\u001b[1;32m   2655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:633\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[Output], [invoke(inputs[\u001b[38;5;241m0\u001b[39m], configs[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(configs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[Output], \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(invoke, inputs, configs)))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['summaries'])} summaries from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
