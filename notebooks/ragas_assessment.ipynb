{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas evaluation\n",
    "Test batch and ragas capability.\n",
    "\n",
    "Uses this article as a model: https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557\n",
    "\n",
    "Ragas repository: https://github.com/explodinggradients/ragas/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas import RunConfig\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from pinecone import Pinecone as pinecone_client, ServerlessSpec\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness, faithfulness, context_recall\n",
    "from datasets import Dataset\n",
    "\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "# from umap import UMAP\n",
    "# import numpy as np\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import eval\n",
    "import admin\n",
    "import data_processing\n",
    "import queries\n",
    "\n",
    "# Set environment variables with .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test (synthetic) dataset, generate docs+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chroma with standard RAG to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=text-embedding-3-small-2merge-0),\n",
       " Collection(name=text-embedding-3-large-2merge-0-parent-child-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0),\n",
       " Collection(name=text-embedding-3-large-0merge-400),\n",
       " Collection(name=text-embedding-3-small-2merge-0-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0-queries),\n",
       " Collection(name=text-embedding-3-large-2merge-0-parent-child)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=os.path.join(os.getenv('LOCAL_DB_PATH'),'chromadb'))   \n",
    "collections=persistent_client.list_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma _embedding_function isn't compatible like embedding objects. Index by embeddings used.\n",
    "query_models=[OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY')),\n",
    "              OpenAIEmbeddings(model='text-embedding-3-large',openai_api_key=os.getenv('OPENAI_API_KEY'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone_client(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "indexes=pinecone_client.list_indexes()\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_models=[VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False),\n",
    "              VoyageAIEmbeddings(model='voyage-large-2-instruct', \n",
    "                                 voyage_api_key=os.getenv('VOYAGE_API_KEY'), truncation=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes=admin.show_ragatouille_indexes(format=False)\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbs=['colbert-ir/colbertv2.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=0\n",
    "# db=dbs[idx]\n",
    "\n",
    "# # TODO get this as a langchain retriever, pull docs\n",
    "# query_model = RAGPretrainedModel.from_pretrained(db,index_root=os.path.join(os.getenv('LOCAL_DB_PATH'),'.ragatouille'))\n",
    "# docs_vectorstore=data_processing.initialize_database('RAGatouille',\n",
    "#                                                      'colbert-ir-colbertv2.0-2merge-0',\n",
    "#                                                      query_model,\n",
    "#                                                      'Standard',\n",
    "#                                                      os.getenv('LOCAL_DB_PATH'),\n",
    "#                                                      init_ragatouille=False,\n",
    "#                                                      clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=docs_vectorstore.model.collection  # Document chunks (chunked smaller according to token size)\n",
    "# print(docs[0])\n",
    "# print(len(docs))\n",
    "\n",
    "# metadata=docs_vectorstore.model.docid_metadata_map  # Document metadata for original documents\n",
    "# print(len(metadata))\n",
    "\n",
    "# map=docs_vectorstore.model.pid_docid_map    # Map of document chunks to original document\n",
    "# print(len(map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't quite figure out functionality to export the encodings from RAGatouille for each document. Won't pursue unless RAGatouille has exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type=\"ChromaDB\"\n",
    "# index_type=\"Pinecone\"\n",
    "\n",
    "# Select database for generating docs\n",
    "if index_type==\"ChromaDB\":\n",
    "    idx_chroma=0   # Most reasonable baseline (text-embedding-3-large-2merge-0), top of the line embeddings, 2 page size good to genreate questions from.\n",
    "    docs_vectorstore=collections[idx_chroma]\n",
    "    query_model=query_models[idx_chroma]  \n",
    "elif index_type==\"Pinecone\":\n",
    "    idx_pinecone=0\n",
    "    docs_vectorstore=indexes[idx_pinecone]\n",
    "    query_model=query_models[idx_pinecone]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "df_docs, lcdocs = eval.lcdoc_export(index_type,docs_vectorstore,query_model,export_pickle=False)\n",
    "print(len(lcdocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good article on how models/embeddings are used in the `TestsetGenerator`: https://www.pondhouse-data.com/blog/evaluate-rag-performance-using-ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll get a tricky threading error. Fully close vs studio, open a new window, restart the kernel, and it'll clear. It also appears to be related to versions newer than 0.1.6 for ragas. I'll stick with that for now until I find ways to test an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generator inputs\n",
    "generator_model=\"gpt-3.5-turbo-0125\"\n",
    "synthetic_generator_llm = ChatOpenAI(model=generator_model, tags=[generator_model])\n",
    "\n",
    "critic_model='gpt-4o'\n",
    "synthetic_critic_llm = ChatOpenAI(model=critic_model,tags=[critic_model])\n",
    "\n",
    "# embedding_model='text-embedding-3-large'\n",
    "# synthetic_embeddings = OpenAIEmbeddings(model=embedding_model,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "synthetic_embeddings=query_model\n",
    "\n",
    "# Run parameters for testset generation\n",
    "run_config=RunConfig(timeout=1000,\n",
    "                max_retries=50,\n",
    "                max_wait=1000,\n",
    "                max_workers=1)\n",
    "\n",
    "# Create generator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    synthetic_generator_llm,\n",
    "    synthetic_critic_llm,\n",
    "    synthetic_embeddings,\n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "# eval_size=100    # Number of samples to evaluate at a time. Intended to circumvent OpenAI API rate limits.\n",
    "eval_size=len(lcdocs)\n",
    "n_questions=30   # Number of questions to generate for each evaluation sample.\n",
    "fname=os.path.join('output',f\"testset_{docs_vectorstore.name}_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset=eval.generat7e_testset(lcdocs,generator,eval_size,n_questions,fname,run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will use the same base data as the synthetic test dataset but apply different RAG strategies:\n",
    "* Different chunk sizes\n",
    "* Embedding models\n",
    "* LLMs\n",
    "* Advanced RAG (parent-child, RAGatouille)\n",
    "\n",
    "The database may not be the same as the synthetic test dataset but uses the same base data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset and database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test dataset. Skip this if you have generated it above.\n",
    "\n",
    "# testset_name=docs_vectorstore.name    # Uncomment if you want to use the most recent testset\n",
    "testset_name='text-embedding-3-small-2merge-0_full'\n",
    "fname=os.path.join('output',f\"testset_{testset_name}.csv\")\n",
    "\n",
    "import_csv=True\n",
    "if import_csv:\n",
    "    df_testset = pd.read_csv(fname)\n",
    "\n",
    "# In case there was missing truth, drop the question\n",
    "df_testset = df_testset.dropna(subset=['ground_truth'])\n",
    "\n",
    "# temporarily reduce the quantity to evaluate the functionality\n",
    "# df_testset=df_testset.head(2)\n",
    "\n",
    "# Create template dataframe to iterate over later\n",
    "df_qa_template = df_testset[['question', 'ground_truth']].copy()\n",
    "df_qa_template['question_id'] = df_qa_template.index\n",
    "df_qa_template = df_qa_template[['question_id', 'question', 'ground_truth']]\n",
    "# for column in [\"answer\", \"source_documents\", \"answer_by\", \"query_model\"]:\n",
    "#     df_qa_template[column] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have a blank dataframe to generate questions for an evaluate. For each model and database in setup_data below, this template dataframe will be what is evaluated with RAG responses/RAGAS criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAG to generate responses, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read setup data, determining the evaluation models and databases\n",
    "json_file_path = \"eval_models.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_models': [{'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-small-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-small'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0-parent-child',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Parent-Child',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'ChromaDB',\n",
       "   'index_name': 'text-embedding-3-large-2merge-0',\n",
       "   'query_model': {'query_model': 'OpenAI',\n",
       "    'embedding_name': 'text-embedding-3-large'},\n",
       "   'llm': {'llm_source': 'Hugging Face',\n",
       "    'llm_model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000},\n",
       "    'hf_endpoint': 'https://api-inference.huggingface.co/v1'},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'Pinecone',\n",
       "   'index_name': 'voyage-large-2-instruct-2merge-0',\n",
       "   'query_model': {'query_model': 'Voyage',\n",
       "    'embedding_name': 'voyage-large-2-instruct'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'rag_type': 'Standard',\n",
       "    'k': 4,\n",
       "    'search_type': 'similarity',\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}},\n",
       "  {'index_type': 'RAGatouille',\n",
       "   'index_name': 'colbert-ir-colbertv2.0-2merge-0',\n",
       "   'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'},\n",
       "   'llm': {'llm_source': 'OpenAI',\n",
       "    'llm_model': 'gpt-4o',\n",
       "    'model_options': {'temperature': 0.2, 'output_level': 1000}},\n",
       "   'qa_model_params': {'k': 4,\n",
       "    'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-small-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-small'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0-parent-child', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Parent-Child', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'Hugging Face', 'llm_model': 'meta-llama/Meta-Llama-3-8B-Instruct', 'model_options': {'temperature': 0.2, 'output_level': 1000}, 'hf_endpoint': 'https://api-inference.huggingface.co/v1'}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'ChromaDB', 'index_name': 'text-embedding-3-large-2merge-0', 'query_model': {'query_model': 'OpenAI', 'embedding_name': 'text-embedding-3-large'}, 'llm': {'llm_source': 'Hugging Face', 'llm_model': 'mistralai/Mistral-7B-Instruct-v0.2', 'model_options': {'temperature': 0.2, 'output_level': 1000}, 'hf_endpoint': 'https://api-inference.huggingface.co/v1'}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'Pinecone', 'index_name': 'voyage-large-2-instruct-2merge-0', 'query_model': {'query_model': 'Voyage', 'embedding_name': 'voyage-large-2-instruct'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'rag_type': 'Standard', 'k': 4, 'search_type': 'similarity', 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n",
      "{'index_type': 'RAGatouille', 'index_name': 'colbert-ir-colbertv2.0-2merge-0', 'query_model': {'embedding_name': 'colbert-ir/colbertv2.0'}, 'llm': {'llm_source': 'OpenAI', 'llm_model': 'gpt-4o', 'model_options': {'temperature': 0.2, 'output_level': 1000}}, 'qa_model_params': {'k': 4, 'local_db_path': '/Users/danmueller/Documents/GitHub/aerospace_chatbot/db'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the evaluation models and databases, dump data as you go.\n",
    "# This will cache data along the way into rag_responses, so you can pick up where you left off.\n",
    "# A pickled dataframe is also exported at the end of each iteration, but not row-by-row.\n",
    "df_qa = pd.DataFrame()\n",
    "for model in setup_data['eval_models']:\n",
    "    print(model)\n",
    "    \n",
    "    # Database\n",
    "    index_type=model['index_type']\n",
    "    sb['index_type']=index_type\n",
    "    index_name=model['index_name']\n",
    "    sb['index_name']=index_name\n",
    "    # Query model and llm\n",
    "    for key in model['query_model']:\n",
    "        sb[key] = model['query_model'][key]\n",
    "    query_model=admin.get_query_model(sb, secrets)\n",
    "    for key in model['llm']:\n",
    "        sb[key] = model['llm'][key]\n",
    "    llm=admin.set_llm(sb, secrets)\n",
    "    # QA model params\n",
    "    qa_model_params=model['qa_model_params']\n",
    "    \n",
    "    df_qa_iter=eval.rag_responses(index_type, index_name, query_model, llm, qa_model_params, \n",
    "                                  df_qa_template, df_docs, testset_name)\n",
    "    df_qa = pd.concat([df_qa,df_qa_iter],ignore_index=True)\n",
    "\n",
    "    # After each iteration, export a pickle of the dataframe\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "            pickle.dump(df_qa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 How does the motion on the PoD tribometer differ from the motion on the SOT tribometer, and how does this difference impact the assessment of lubricant behavior in hybrid lubrication?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c37ca54ae644079b292a6ac9b05410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39c2078f3b445b686af9e39971ec766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d84496d3514a5187d8d1eb1a0769c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 What testing methods were used to assess the performance of the spring strut hardware, and what were the results of these tests?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3cf894836448ba9a9b02f26900858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e741c6ad8e9243f1869abc00e8175929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691bc9dcbb3e4a279b79928776b5d47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 What challenges were faced with the surface of the L7 deployment floor and how did it impact the deployment process?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccae792af74414db23403726ba04498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd30874069ac4ab298fdd8733639f324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13221c559098487988bd559a7bae4008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 How was the position sensor selected for the Future Actuator design?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d460072c14812a94e344ec666b8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b570eb6203744a8c8809322312ac8b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf437dc459041a496fc58fbfae75e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 How does the presence of lubrication on threads affect the nut factor ranges in bolted joints?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7900cf58e0d34cf08cc387a1a02768dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe79d2b1179435e899c468ea58959a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c3aca00c9c49b7946c1075b33aee0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 What role do DC motors play in space applications and why is proper selection and specification essential for the success of future space missions?\n",
      "answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d64505707e4437a56cbfad049c7633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/metrics/base.py\", line 116, in ascore\n",
      "    raise e\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/metrics/base.py\", line 112, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/metrics/_answer_correctness.py\", line 163, in _ascore\n",
      "    assert self.answer_similarity is not None, \"AnswerSimilarity must be set\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: AnswerSimilarity must be set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         eval_obj\u001b[38;5;241m=\u001b[39mcontext_recall\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[eval_criteria] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(row[eval_criteria]):\n\u001b[0;32m---> 30\u001b[0m         evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_qa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43meval_obj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         df_qa\u001b[38;5;241m.\u001b[39mloc[i,eval_criteria] \u001b[38;5;241m=\u001b[39m evaluation_result[\n\u001b[1;32m     35\u001b[0m             eval_criteria\n\u001b[1;32m     36\u001b[0m         ]\n\u001b[1;32m     38\u001b[0m response_dict[eval_criteria]\u001b[38;5;241m=\u001b[39mevaluation_result[eval_criteria]\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/evaluation.py:211\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/ragas/executor.py:132\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m executor_job\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mexecutor_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_criterias=[\"answer_correctness\", \"faithfulness\", \"context_recall\"]\n",
    "# Add answer correctness column, fill in if it exists\n",
    "df_qa = eval.add_cached_columns_from_file(\n",
    "    df_qa, os.path.join('output',f'ragas_result_cache_{testset_name}.jsonl'), \"question\", \n",
    "    eval_criterias\n",
    ")\n",
    "\n",
    "# Sometimes ground_truth does not provide a response. Just filter those out.\n",
    "df_qa = df_qa[df_qa['ground_truth'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Evaluate the answer correctness if not already done\n",
    "fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "\n",
    "for i, row in df_qa.iterrows():\n",
    "    print(i, row[\"question\"])\n",
    "    response_dict={}\n",
    "    response_dict[\"question\"]=row[\"question\"]\n",
    "\n",
    "    if any(row[eval_criteria] is None for eval_criteria in eval_criterias):\n",
    "        for eval_criteria in eval_criterias:\n",
    "            print(eval_criteria)\n",
    "            if eval_criteria==\"answer_correctness\":\n",
    "                eval_obj=answer_correctness\n",
    "            elif eval_criteria==\"faithfulness\":\n",
    "                eval_obj=faithfulness\n",
    "            elif eval_criteria==\"context_recall\":\n",
    "                eval_obj=context_recall\n",
    "\n",
    "            if row[eval_criteria] is None or pd.isnull(row[eval_criteria]):\n",
    "                evaluation_result = evaluate(\n",
    "                    Dataset.from_pandas(df_qa.iloc[i : i + 1][fields]),\n",
    "                    [eval_obj],\n",
    "                )\n",
    "                df_qa.loc[i,eval_criteria] = evaluation_result[\n",
    "                    eval_criteria\n",
    "                ]\n",
    "                \n",
    "            response_dict[eval_criteria]=evaluation_result[eval_criteria]\n",
    "        eval.write_dict_to_file(response_dict, os.path.join('output',f'ragas_result_cache_{testset_name}.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "df_qa = eval.eval_rag(index_name, df_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write=True\n",
    "if write:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"wb\") as f:\n",
    "        pickle.dump(df_qa, f)\n",
    "else:\n",
    "    with open(os.path.join('output',f'df_qa_{testset_name}.pickle'), \"rb\") as f:\n",
    "        df_qa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas eval, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link from documents to questions, that used the document as source. Add UMAP column for visualization purposes.\n",
    "df_visualize=eval.data_viz_prep(index_name,df_qa,df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "df = pd.read_parquet(f'df_{index_name}.parquet')\n",
    "\n",
    "# show the dataframe with the question and answer in spotlight\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
