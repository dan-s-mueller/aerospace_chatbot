{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 10:05:52.237 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('../src/aerospace_chatbot')\n",
    "import queries\n",
    "import admin\n",
    "import data_processing\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to split chunking from upserting\n",
    "def create_upsert(index_type,index_name,query_model,rag_type,chunker,summary_llm):\n",
    "    # Set index names for special databases\n",
    "    if rag_type == 'Parent-Child':\n",
    "        index_name = index_name + '-parent-child'\n",
    "    if rag_type == 'Summary':\n",
    "        index_name = index_name + '-' + summary_llm.model_name.replace('/', '-').replace(' ','-').lower() + '-summary' \n",
    "\n",
    "    try:\n",
    "        vectorstore = data_processing.initialize_database(index_type, \n",
    "                                            index_name, \n",
    "                                            query_model,\n",
    "                                            rag_type=rag_type,\n",
    "                                            clear=True, \n",
    "                                            local_db_path=os.getenv('LOCAL_DB_PATH'),\n",
    "                                            init_ragatouille=True,\n",
    "                                            show_progress=False)\n",
    "        print(f\"Database {index_name} created.\")\n",
    "        vectorstore, _ = data_processing.upsert_docs(index_type, \n",
    "                                        index_name,\n",
    "                                        vectorstore,\n",
    "                                        chunker,\n",
    "                                        batch_size=400,\n",
    "                                        show_progress=False,\n",
    "                                        local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database {index_name} upserted chunks.\")\n",
    "    except Exception as e:  # If there is an error, be sure to delete the database\n",
    "        data_processing.delete_index(index_type, \n",
    "                                    index_name,\n",
    "                                    rag_type,\n",
    "                                    local_db_path=os.getenv('LOCAL_DB_PATH'))\n",
    "        print(f\"Database deleted: {index_name}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets, Models, Docs, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set secrets\n",
    "secrets={}\n",
    "sb={}\n",
    "\n",
    "secrets['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "secrets['VOYAGE_API_KEY'] = os.getenv('VOYAGE_API_KEY')\n",
    "secrets['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY')\n",
    "secrets['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size None\n",
      "[Oct 26, 10:05:53] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/danmueller/Documents/GitHub/aerospace_chatbot/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Read setup data, assign models\n",
    "json_file_path = \"databases.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    setup_data = json.load(json_file)\n",
    "\n",
    "sb={}\n",
    "query_params=setup_data['query_models']\n",
    "query_models=[]\n",
    "for model in query_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    query_models.append(admin.get_query_model(sb, secrets))\n",
    "\n",
    "llm_params=setup_data['llms']\n",
    "llms=[]\n",
    "for model in llm_params:\n",
    "    for key in model:\n",
    "        sb[key] = model[key]\n",
    "    llms.append(admin.set_llm(sb, secrets))\n",
    "\n",
    "chunk_params=setup_data['chunk_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra query types that take a long time. Add to the databases.json file\n",
    "\n",
    "{\n",
    "    \"id\": \"3\",\n",
    "    \"index_type\": \"ChromaDB\",\n",
    "    \"query_model\": \"Hugging Face\",\n",
    "    \"embedding_name\": \"Dedicated Endpoint\",\n",
    "    \"embedding_hf_endpoint\": \"https://d95tsnjp6nub114k.us-east4.gcp.endpoints.huggingface.cloud\"\n",
    "},\n",
    "{\n",
    "    \"id\": \"4\",\n",
    "    \"index_type\": \"RAGatouille\",\n",
    "    \"embedding_name\": \"colbert-ir/colbertv2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docs\n",
    "data_folder='../data/AMS'\n",
    "docs= glob.glob(os.path.join(data_folder,'*.pdf'))   # Only get the PDFs in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_appendix_name='ams'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All query models (except RAGatouille), standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_chunk in range(len(chunk_params)):\n",
    "    # Chunk the docs before creating and upserting into the database\n",
    "    chunker=data_processing.chunk_docs(docs,\n",
    "                rag_type=rag_type,\n",
    "                n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "                chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "                chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "                llm=summary_llm,\n",
    "                show_progress=False)\n",
    "\n",
    "    print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "    # for i_run in range(len(query_params)-1):\n",
    "    # TODO Uncomment above to loop through all query models. Updated below to rerun pinecone only.\n",
    "    i_run=1\n",
    "    \n",
    "    # Create and upsert database\n",
    "    print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "    index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "    index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "    create_upsert(query_params[i_run]['index_type'],\n",
    "                    index_name,\n",
    "                    query_models[i_run],\n",
    "                    rag_type,\n",
    "                    chunker,\n",
    "                    summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, parent-child, 400 character-recursive chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Parent-Child'\n",
    "summary_llm=None\n",
    "i_chunk=1   # 400 character-recursive setting\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI text-embedding-3-large, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Summary'\n",
    "summary_llm=llms[3] # meta-llama-3-8b-instruct-dsm\n",
    "\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=0     # OpenAI text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['summaries'])} summaries from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGatouille, summary, 2 page merge, no chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type='Standard'\n",
    "summary_llm=None\n",
    "i_chunk=0   # 2 page merge, no chunk\n",
    "i_run=2     # RAGatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker=data_processing.chunk_docs(docs,\n",
    "            rag_type=rag_type,\n",
    "            n_merge_pages=chunk_params[i_chunk]['n_merge_pages'],\n",
    "            chunk_method=chunk_params[i_chunk]['chunk_method'],\n",
    "            chunk_size=chunk_params[i_chunk]['chunk_size'],\n",
    "            llm=summary_llm,\n",
    "            show_progress=False)\n",
    "\n",
    "print(f\"Created {len(chunker['chunks'])} chunks from {len(chunker['pages'])} pages.\")\n",
    "\n",
    "# Create and upsert database\n",
    "print(f\"Creating and uploading database with these params: {query_params[i_run]}\")\n",
    "\n",
    "index_appendix=str(chunk_params[i_chunk]['n_merge_pages'])+'merge'+'-'+str(chunk_params[i_chunk]['chunk_size'])\n",
    "index_name = (query_params[i_run]['embedding_name'].replace('/', '-').replace(' ', '-') + '-' + index_appendix).lower()\n",
    "\n",
    "create_upsert(query_params[i_run]['index_type'],\n",
    "                index_name,\n",
    "                query_models[i_run],\n",
    "                rag_type,\n",
    "                chunker,\n",
    "                summary_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
