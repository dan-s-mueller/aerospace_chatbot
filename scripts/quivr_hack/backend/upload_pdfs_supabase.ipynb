{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the same as the other stuff I wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check api keys\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# print(os.getenv('SUPABASE_URL'))\n",
    "# print(os.getenv('SUPABASE_SERVICE_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate OpenAI embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The quivr hack stuff starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from parsers import common, pdf\n",
    "from models.files import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357cec98-7c5b-43c3-adb5-162ea09ee7d4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Brain(id=None, name='Default brain', description='This is a description', status='private', model='gpt-3.5-turbo', temperature=0.0, max_tokens=256, openai_api_key=None, files=[], max_brain_size=52428800, prompt_id=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a brain\n",
    "from models import brains\n",
    "from uuid import UUID, uuid4\n",
    "\n",
    "brain_id=uuid4()\n",
    "\n",
    "brain=brains.Brain(id=brain_id,\n",
    "                    name='ams_bot',\n",
    "                    description='ams expert',\n",
    "                    status='private',\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    temperature=0.0,\n",
    "                    max_tokens=256,\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    files=[],\n",
    "                    max_brain_size=52428800)\n",
    "\n",
    "print(brain.id)\n",
    "\n",
    "brain.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object process_file at 0x12fef5a60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to create a file object with one of the PDFs.\n",
    "# From /Users/danmueller/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/routes/crawl_routes.py\n",
    "\n",
    "# Create a SpooledTemporaryFile from the file_path\n",
    "from fastapi import APIRouter, Depends, Query, Request, UploadFile\n",
    "from tempfile import SpooledTemporaryFile\n",
    "import shutil\n",
    "\n",
    "spooled_file = SpooledTemporaryFile()\n",
    "file_path='../../../data/'\n",
    "file_name='AMS_2000.pdf'\n",
    "with open(file_path+file_name, \"rb\") as f:\n",
    "    shutil.copyfileobj(f, spooled_file)\n",
    "\n",
    "# Pass the SpooledTemporaryFile to UploadFile\n",
    "uploadFile = UploadFile(\n",
    "    file=spooled_file,  # pyright: ignore reportPrivateUsage=none\n",
    "    filename=file_name,\n",
    ")\n",
    "file = File(file=uploadFile)\n",
    "\n",
    "pdf.process_pdf(file=file,\n",
    "                    enable_summarization=False,\n",
    "                    brain_id=brain_id,\n",
    "                    user_openai_api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 00:05:29,258:INFO - HTTP Request: GET https://ldoeollhxasevurjamos.supabase.co/rest/v1/brains_vectors?select=vector_id&brain_id=eq.357cec98-7c5b-43c3-adb5-162ea09ee7d4 \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.brains_vectors\" does not exist'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/j9ms5wbn0sj9nw8k1tlzrq600000gn/T/ipykernel_78724/3456338163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unique_brain_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/models/brains.py\u001b[0m in \u001b[0;36mget_unique_brain_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mvector_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupabase_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_brain_vector_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_files_from_vector_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/aerospace_chatbot/scripts/quivr_hack/backend/models/databases/supabase/brains.py\u001b[0m in \u001b[0;36mget_brain_vector_ids\u001b[0;34m(self, brain_id)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vector_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brain_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/postgrest/_sync/request_builder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAPIResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_request_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: {'code': '42P01', 'details': None, 'hint': None, 'message': 'relation \"public.brains_vectors\" does not exist'}"
     ]
    }
   ],
   "source": [
    "brain.get_unique_brain_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is also stuff which was working before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize Pinecone client\n",
    "import pinecone\n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase.client import Client\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT') \n",
    ")\n",
    "pinecone.whoami()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the existing index, clear for new start\n",
    "index_name = \"langchain-quickstart\"\n",
    "index=pinecone.Index(index_name)\n",
    "index.delete(delete_all=True) # Clear the index first, then upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import parsers\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone database: https://app.pinecone.io/organizations/-Nam3zmbSmzuXKeH8EWl/projects/us-west1-gcp-free:32467cc/indexes/langchain-quickstart\n",
    "import glob\n",
    "\n",
    "data_folder='../data/'\n",
    "docs = glob.glob(data_folder+'*.pdf')   # Only get the PDFs in the directory\n",
    "\n",
    "for doc in docs:\n",
    "    print('Parsing: '+doc)\n",
    "    loader = PyPDFLoader(doc)\n",
    "    data = loader.load_and_split()\n",
    "    \n",
    "    # This is optional, but needed to play with the data parsing.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "\n",
    "    for text in texts:\n",
    "        text.metadata['page']=text.metadata['page']+1   # Pages are 0 based, update\n",
    "    \n",
    "    print('Uploading to pinecone index '+index_name)\n",
    "    vectorstore = Pinecone.from_documents(texts, embeddings_model, index_name=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
