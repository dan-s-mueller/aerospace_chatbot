{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster info for pinecone databases\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pinecone\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENVIRONMENT') \n",
    ")\n",
    "index_name = 'ams'\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\",openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of ids list is shorter than the number of total vectors...\n",
      "creating random vector...\n",
      "searching pinecone...\n",
      "<class 'pinecone.core.client.model.query_response.QueryResponse'>\n",
      "getting ids from a vector query...\n",
      "updating ids set...\n",
      "Collected 7546 ids out of 7546.\n"
     ]
    }
   ],
   "source": [
    "def get_ids_from_query(index,input_vector):\n",
    "  print(\"searching pinecone...\")\n",
    "  results = index.query(vector=input_vector, top_k=10000,include_values=False)\n",
    "  ids = set()\n",
    "  print(type(results))\n",
    "  for result in results['matches']:\n",
    "    ids.add(result['id'])\n",
    "  return ids\n",
    "\n",
    "def get_all_ids_from_index(index, num_dimensions, namespace=\"\"):\n",
    "  num_vectors = index.describe_index_stats()[\"namespaces\"][namespace]['vector_count']\n",
    "  all_ids = set()\n",
    "  while len(all_ids) < num_vectors:\n",
    "    print(\"Length of ids list is shorter than the number of total vectors...\")\n",
    "    input_vector = np.random.rand(num_dimensions).tolist()\n",
    "    print(\"creating random vector...\")\n",
    "    ids = get_ids_from_query(index,input_vector)\n",
    "    print(\"getting ids from a vector query...\")\n",
    "    all_ids.update(ids)\n",
    "    print(\"updating ids set...\")\n",
    "    print(f\"Collected {len(all_ids)} ids out of {num_vectors}.\")\n",
    "\n",
    "  return all_ids\n",
    "\n",
    "all_ids = get_all_ids_from_index(vectorstore, num_dimensions=1536, namespace=\"\")\n",
    "all_ids=list(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_query=1000 # The fectch function will top out around 1000 entries.\n",
    "\n",
    "def iterate_in_chunks(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i+chunk_size]\n",
    "\n",
    "# Create a list of all of the vector embeddings and text that it iterated in max_size_query chunks\n",
    "vectors=[]\n",
    "vector_text=[]\n",
    "vector_embeddings=[]\n",
    "for chunk in iterate_in_chunks(all_ids, max_size_query):\n",
    "    vector_temp=vectorstore.fetch(ids=chunk)\n",
    "    vectors.append(vector_temp)\n",
    "    for id in chunk:\n",
    "        vector_text.append(vector_temp['vectors'][id]['metadata']['text'])\n",
    "        vector_embeddings.append(vector_temp['vectors'][id]['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "silhouette_scores = []\n",
    "i_clusters=range(20,1001,20)\n",
    "for i in i_clusters:\n",
    "    # print(i)\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init='auto',random_state=42)\n",
    "    kmeans.fit(vector_embeddings)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_scores.append(silhouette_score(vector_embeddings, labels))\n",
    "\n",
    "plt.plot(i_clusters, wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "best_num_clusters = i_clusters[np.argmax(silhouette_scores)]\n",
    "\n",
    "plt.plot(i_clusters, silhouette_scores)\n",
    "plt.title('Silhouette Scores')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "print(best_num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add labeling using free open source and cheap llm. Try bloom https://lancerninja.com/open-source-models-with-langchain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the Data\n",
    "# Assuming you have a numpy array called 'pinecone_vectors' with shape (num_vectors, vector_dim)\n",
    "# Make sure your data is in the right format and normalized if necessary\n",
    "\n",
    "# Step 2: Choose a Clustering Algorithm\n",
    "# num_clusters = best_num_clusters  # Specify the desired number of clusters\n",
    "num_clusters = 50  # Specify the desired number of clusters\n",
    "\n",
    "# Step 4: Apply the Clustering Algorithm\n",
    "kmeans = KMeans(n_clusters=num_clusters, init=\"k-means++\", n_init='auto')\n",
    "cluster_labels = list(kmeans.fit_predict(vector_embeddings))\n",
    "\n",
    "# Step 5: Interpret the Clusters\n",
    "# You can analyze the cluster centroids or representative vectors to understand the cluster's properties\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Assign Automatic Descriptions using OpenAI\n",
    "cluster_descriptions = []\n",
    "\n",
    "def truncate_string_at_n_tokens(text, n):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Retrieve the first N tokens\n",
    "    truncated_tokens = tokens[:n]\n",
    "    # Join the tokens back into a string\n",
    "    truncated_string = ' '.join(truncated_tokens)\n",
    "    return truncated_string\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = [text for text, label in zip(vector_slice_text, cluster_labels) if label == i]\n",
    "    cluster_content = \", \".join([str(vec) for vec in cluster_data])  # Modify this based on your specific data representation\n",
    "    # print('Cluster content: \\n'+cluster_content+'\\n'+'---'+'\\n')\n",
    "    cluster_content=truncate_string_at_n_tokens(cluster_content,500)\n",
    "    # print('Truncated cluster content: \\n'+cluster_content+'\\n'+'---'+'\\n')\n",
    "\n",
    "    prompt = f\"Cluster {i+1} content:\\n {cluster_content}.\\n---\\n Describe the common characteristics or theme of this cluster.\"\n",
    "    # print('Prompt: \\n'+prompt+'\\n'+'---'+'\\n')\n",
    "    response=completion_with_backoff(engine='gpt-3.5-turbo-instruct',\n",
    "                                    prompt=prompt,\n",
    "                                    max_tokens=100,\n",
    "                                    temperature=0)\n",
    "    # print(response)\n",
    "    description = response.choices[0].text.strip()\n",
    "    cluster_descriptions.append(description)\n",
    "    print(f'Cluster {i+1} Description: \\n'+description+'\\n'+'---'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels == i\n",
    "vector_slice_text[cluster_labels == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Visualize the Clusters\n",
    "# Reduce the dimensionality of the vectors for visualization using PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_vectors = pca.fit_transform(pinecone_vectors)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = pca_vectors[cluster_labels == i]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=cluster_descriptions[i])\n",
    "plt.title(\"Pinecone Vector Clustering\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Evaluate and Refine (optional)\n",
    "# You can use clustering evaluation metrics to assess the quality of the clustering results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
